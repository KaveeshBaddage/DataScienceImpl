{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information Retrieval – Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO29bUSNnKeRSiUE4zADLOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaveeshBaddage/DataScienceImpl/blob/main/Information_Retrieval_%E2%80%93_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lblfiO0jPMPg"
      },
      "source": [
        "**Get Content from text files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpOC9gUH2JIm"
      },
      "source": [
        "def read_file(filename):\n",
        "    input_file = open(filename)\n",
        "    input_file_content = input_file.read()\n",
        "    input_file.close()\n",
        "    return input_file_content"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1X5KxwUPR4s"
      },
      "source": [
        "**Read contents in the text files into the runtime**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqlqjtamNRdH"
      },
      "source": [
        "student_feedback_content =  read_file(\"/content/Student Course Feedback.txt\") \n",
        "twitter_content = read_file(\"/content/Twitter Data.txt\")\n",
        "research_paper_content = read_file(\"/content/research paper.txt\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMHA-sRPffz"
      },
      "source": [
        "**Tokenize the Content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ5UGVOJOjLX",
        "outputId": "f50a0df3-2e8d-4716-bff8-1a844aa7d601"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "tokenized_student_feedback_content = word_tokenize(student_feedback_content)\n",
        "tokenized_twitter_content = word_tokenize(twitter_content)\n",
        "tokenized_research_paper_content = word_tokenize(research_paper_content)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVmGth5oRTKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4510315-5caf-4c1e-abbd-2bbd9b4d0988"
      },
      "source": [
        "print(\"-------------------  Tokkenized Student feedback content   --------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(tokenized_student_feedback_content ,\"\\n\" )\n",
        "print(\"-------------------  Tokkenized Twitter content   --------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(tokenized_twitter_content , \"\\n\")\n",
        "print(\"-------------------  Tokkenized Research paper content   --------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(tokenized_research_paper_content , \"\\n\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------  Tokkenized Student feedback content   --------------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.', '<', 'br', '/', '>', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Thanks', '!', ':', ')', '<', 'br', '/', '>', '``', 'The', 'lectures', 'are', 'good..but', 'a', 'bit', 'speed.A', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'one.So', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame.', \"''\", '<', 'br', '/', '>', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', '.', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', '.', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', '.', '<', 'br', '/', '>', \"''\", 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', '.', 'It', 'was', 'easy', 'to', 'understand', '.', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', '.', 'Lectures', 'were', 'good', '.', 'understandable', '.', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'OOP', '.', 'Motivated', 'to', 'well', '.', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', '.', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', '.', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', '.', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', '.', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'examples.lectures', 'were', 'interesting.we', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', '.', 'I', 'satisfy', 'about', 'first', '7', 'lectures', '.', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', '.', 'lectuers', 'are', 'very', 'good', '.', 'take', 'good', 'effort', 'to', 'make', 'undersatand', 'every', 'student', 'in', 'the', 'room', '.', 'very', 'helpfull', '.', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'OOP', 'and', 'its', 'concepts', '.', '``', 'lecture', 'slides', ',', 'explanations', 'were', 'very', 'clear', '.', '<', 'br', '/', '>', 'it', '&', '#', '039', ';', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', '.', '<', 'br', '/', '>', 'sometimes', ',', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', '.', '<', 'br', '/', '>', 'overall', 'very', 'good', '!', '!', '!', '<', 'br', '/', '>', \"''\", 'The', 'lectures', 'were', 'good', 'and', 'clear', '.', 'And', 'they', 'weren', '&', '#', '039', ';', 't', 'too', 'fast', '.', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', '&', '#', '039', ';', 't', 'know', 'java', 'before', '.', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'class.it', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', '!', '.', 'thankyou'] \n",
            "\n",
            "-------------------  Tokkenized Twitter content   --------------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#', 'cdnpoli', '#', 'LPC', '#', 'CPCLDR��_', 'https', ':', '//t.co/ZOZOSe1CqQ', '#', 'immigration', '#', 'integration', '#', 'canada', 'https', ':', '//t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'amp', ';', 'Canada', '.', 'https', ':', '//t.co/99mYliuOes', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', '?', 'https', ':', '//t.co/LsG7C3vLe9', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', ':', 'XKofy', 'https', ':', '//t.co/becgusY2i6', 'Canada', 'Immigration', 'Minister', 'to', '���Substantially', 'Increase', 'Immigration', 'Numbers', 'https', ':', '//t.co/nEFw30MRaa', 'https', ':', '//t.co/cyI867PZRV', 'M��me', 'les', '#', 'USA=pays', \"d'immigration\", 'par', 'excellence', 'CONTR��LE', 'RIGOUREUSEMENT', \"l'immigration\", 'et', 'acc��s', '��', 'la', '#', 'GreenCARD', '!', '��_', 'https', ':', '//t.co/IHpVhW2BaG', '@', 'Shawhelp', 'what', 'changes', 'should', 'be', 'made', 'to', 'Canada', \"'s\", 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', '?', 'L��immigration', 'irr��guli��re', 'au', 'Canada', 'd��cortiqu��e', 'en', '5', 'questions', 'https', ':', '//t.co/f4utO5A7ZF', \"L'immigration\", 'irr��guli��re', 'au', 'Canada', 'd��cortiqu��e', 'en', '5', 'questions', '-', 'https', ':', '//t.co/UiBsEZOqas', 'https', ':', '//t.co/j77dEvjoiX', 'https', ':', '//t.co/XXDeIG7Dbu', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', '_��_�_', '?', '?', 'From', 'my', 'view', '--', 'immigration', 'out', 'of', 'C��_', 'https', ':', '//t.co/YAgwmZ8ECp', 'Dan', 'Murray', 'of��Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear-mongering', 'liar', '#', 'racism', '#', 'canada', '#', 'cdnpoli', '#', 'hatecrime��_', 'https', ':', '//t.co/kwZ3csvYxM', 'Le', 'Canada', 'lance', 'une', 'vaste', 'campagne', \"d'immigration\", 'pour', 'faire', 'face', '��', 'son', 'besoin', 'de', 'main', \"d'��uvre\", 'https', ':', '//t.co/kXdfMGTZzN', 'L��', '#', 'immigration', 'irr��guli��re', 'au', '#', 'Canada', 'd��cortiqu��e', 'en', '5��questions', 'https', ':', '//t.co/s3hu1OKKIG', '@', 'Canadidly', 'I', \"'ve\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'US', 'Canada', 'Immigration', 'Website', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', '#', 'fasttraffic', ',', '#', 'sitetraffic', ',', '#', 'website', ',', '#', 'traffic', 'https', ':', '//t.co/zRlJ26jnkC', 'Mr', 'Know-all', 'of', 'Canada', 'Immigration', 'https', ':', '//t.co/wTQK4QDiKI', 'Move', 'to', 'Canada', '@', 'LadyMadonna___', 'Oh', ',', 'immigration', 'rules', ',', 'you', 'ca', \"n't\", '...', 'https', ':', '//t.co/5LIEVHO7A4', '#', 'OnThisDay', 'Annette', 'Toft', 'becomes', 'Canada', \"'s\", '2', 'millionth', 'immigrant', 'since', '1945', '.', 'Do', 'you', 'know', 'your', 'family', \"'s\", 'immigration', 'st��_', 'https', ':', '//t.co/UvRuw8eR1b', '.', '@', 'TheEconomist', 'profiles', 'Canada', \"'s\", 'open', 'immigration', 'policies', '&', 'amp', ';', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', ':', '��_', 'https', ':', '//t.co/4K84EE8Y63', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', ',', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'https', ':', '//t.co/x2IfO0EXI2', 'Immigration', 'for', 'canada', 'without', 'india', ':', 'an', 'compassionate', 'handle', ':', 'deyFy', \"''\", '#', 'Jamaican', '#', 'immigrants', '#', 'Canada', 'https', ':', '//t.co/vcmfYGadR5', '#', 'statistics', '#', 'immigration', \"''\", 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', '$', '262M', 'over', 'a', 'decade', 'https', ':', '//t.co/9i72fRhtij', 'Are', 'people', 'still', 'moving', 'to', '#', 'Canada', '?', '?', '?', 'Oh', 'that', \"'s\", 'right', ',', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's��_\", 'https', ':', '//t.co/0C5OBfmxLG', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', ',', 'B.C', '.', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'https', ':', '//t.co/YXH5W53srO', 'I', 'added', 'a', 'video', 'to', 'a', '@', 'YouTube', 'playlist', 'https', ':', '//t.co/CnEyWN40x3', 'Funny', 'Talking', 'of', 'Haryanavi', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa-Free', 'To', 'Canada', 'https', ':', '//t.co/Ec3XHORO2s', 'https', ':', '//t.co/RQRr5nebcG', 'L��immigration', 'irr��guli��re', 'au', 'Canada', 'd��cortiqu��e', 'en', '5��questions', 'https', ':', '//t.co/DkpuKyWmaK', '@', 'SweetnessShawnB', 'Hes', 'the', 'POS', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', ',', 'among', 'other', 'globalist', 'policies', '.', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', ',', '2016', '.', 'Thoughts', '?', '#', 'visa', '#', 'immigration', '@', 'HuffingtonPost', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'willl', 'boot', 'your', 'liberal', 'American', 'ass'] \n",
            "\n",
            "-------------------  Tokkenized Research paper content   --------------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework', ',', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides', ',', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off-the-shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', '.', 'Multi-task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', '.', 'Recently', ',', 'neural-based', 'models', 'for', 'multi-task', 'learning', 'have', 'become', 'very', 'popular', ',', 'ranging', 'from', 'computer', 'vision', '(', 'Misra', 'et', 'al.', ',', '2016', ';', 'Zhang', 'et', 'al.', ',', '2014', ')', 'to', 'natural', 'language', 'processing', '(', 'Collobert', 'andWeston', ',', '2008', ';', 'Luong', 'et', 'al.', ',', '2015', ')', ',', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', '.', 'However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', ',', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', '.', 'As', 'shown', 'in', 'Figure', '1-', '(', 'a', ')', ',', 'the', 'general', 'shared-private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', ':', 'one', 'is', 'used', 'to', 'store', 'task-dependent', 'features', ',', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', '.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task-specific', 'features', ',', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', ',', 'suffering', 'from', 'feature', 'redundancy', '.', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', ',', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', ':', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', '.', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', '.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', '.', 'The', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', '.', 'However', ',', 'the', 'general', 'shared-private', 'model', 'could', 'place', 'the', 'task-specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space', ',', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', '.', 'Additionally', ',', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', '.', 'To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'framework', ',', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints.Specifically', ',', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', '.'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVsbIQBDo7P"
      },
      "source": [
        "## **Context sensitive word correction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjRwzW5ADkKz",
        "outputId": "93d26e91-445c-46e9-c241-7b6b59046e5e"
      },
      "source": [
        "#required libraries for context sensitive Spell checker\n",
        "!pip install neuspell\n",
        "from neuspell import SclstmChecker\n",
        "checker = SclstmChecker()\n",
        "\n",
        "checker.from_pretrained()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neuspell in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from neuspell) (4.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neuspell) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from neuspell) (1.9.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from neuspell) (0.1.96)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from neuspell) (4.62.3)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (from neuspell) (0.6.2)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (from neuspell) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->neuspell) (3.7.4.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert->neuspell) (1.18.61)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert->neuspell) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert->neuspell) (2.23.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert->neuspell) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert->neuspell) (0.5.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.61 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert->neuspell) (1.21.61)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.61->boto3->pytorch-pretrained-bert->neuspell) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.61->boto3->pytorch-pretrained-bert->neuspell) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.61->boto3->pytorch-pretrained-bert->neuspell) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert->neuspell) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert->neuspell) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert->neuspell) (2021.5.30)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (0.0.19)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->neuspell) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->neuspell) (3.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->neuspell) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->neuspell) (7.1.2)\n",
            "loading vocab from path:/usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "SCLSTM(\n",
            "  (lstmmodule): LSTM(294, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (dense): Linear(in_features=1024, out_features=100002, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n",
            "112111266\n",
            "loading pretrained weights from path:/usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise\n",
            "Loading model params from checkpoint dir: /usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzMujc2EE017",
        "outputId": "945173cf-a712-49a6-c06b-4743d39e25b0"
      },
      "source": [
        "spell_corrected_student_feedback_content = checker.correct_strings([student_feedback_content])\n",
        "spell_corrected_twitter_content = checker.correct_strings([twitter_content])\n",
        "spell_corrected_research_paper_content = checker.correct_strings([research_paper_content])\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 5.556799 secs\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 7.386354 secs\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 5.680917 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2UBdLHcF1Wc",
        "outputId": "01ffe41d-2f18-4e18-ec0a-44c9399cd988"
      },
      "source": [
        "print(\"----------------------------------------------------------------------------------------------- \")\n",
        "print(\"----------  Student feedback content before applying context sensitive word correction  --------\")\n",
        "print(\"----------------------------------------------------------------------------------------------- \\n\")\n",
        "print(student_feedback_content + '\\n')\n",
        "print(\"------------------------------------------------------------------------------------------------\")\n",
        "print(\"----------  Student feedback content after applying context sensitive word correction  ---------\")\n",
        "print(\"------------------------------------------------------------------------------------------------ \\n\")\n",
        "print(spell_corrected_student_feedback_content[0] + '\\n')\n",
        "\n",
        "print(\"---------------------------------------------------------------------------------------- \")\n",
        "print(\"----------  Twitter content before applying context sensitive word correction  --------\")\n",
        "print(\"--------------------------------------------------------------------------------------- \\n\")\n",
        "print(twitter_content + '\\n')\n",
        "print(\"---------------------------------------------------------------------------------------\")\n",
        "print(\"----------  Twitter content after applying context sensitive word correction  ---------\")\n",
        "print(\"--------------------------------------------------------------------------------------- \\n\")\n",
        "print(spell_corrected_twitter_content[0] + '\\n')\n",
        "\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------------------- \")\n",
        "print(\"----------  Research paper content before applying context sensitive word correction  --------\")\n",
        "print(\"-------------------------------------------------------------------------------------------- \\n\")\n",
        "print(research_paper_content + '\\n')\n",
        "print(\"---------------------------------------------------------------------------------------------\")\n",
        "print(\"----------  Research paper content after applying context sensitive word correction  ---------\")\n",
        "print(\"---------------------------------------------------------------------------------------------- \\n\")\n",
        "print(spell_corrected_research_paper_content[0]+ '\\n')\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------- \n",
            "----------  Student feedback content before applying context sensitive word correction  --------\n",
            "----------------------------------------------------------------------------------------------- \n",
            "\n",
            "Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful to self-study also. The given opportunity to ask questions from the lecturer is appreciative. \n",
            "\"Good :) \n",
            "<br />please do recap at class starting it&#039;s better for us.\n",
            "<br />sometimes teaching speed is very high.\n",
            "<br />\n",
            "<br />Thanks ! :)\n",
            "<br /> \"\n",
            "The lectures are good..but a bit speed.A in class working activity is a must one.So please take another hour in thursdays madame.\n",
            "\"\n",
            "<br />We can hear your voice clearly and can understand the things you teach. Presentation slides also good source to refer . lf you can do more example questions within the classroom and it will help us to understand the principles well. \n",
            "<br />\"\n",
            "Lectures was well structured and well organized. It was easy to understand. Lecture slides and labs were also well organized. \n",
            "Lectures were good. understandable. \n",
            "The lecture slides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP. Motivated to well. Would have been better if we discussed more about the solutions of coding exercisers.   \n",
            "I think i learned a lot from the codes you write in the board. When i compare my codes with yours i can learn about my mistakes and good coding practices that i should follow. There fore i think it would be great if we can discuss more examples in the class.\n",
            "madam explained  the oop concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future.\n",
            "I satisfy about first 7 lectures. That way of teaching is really good for coming lectures too.\n",
            "lectuers are very good. take good effort to make undersatand every student in the room. very helpfull. \n",
            "I was able to obtain a clear picture about OOP and its concepts. \n",
            "\"lecture slides, explanations were very clear.\n",
            "<br />it&#039;s very good to letting ask questions and explain again with suitable examples.\n",
            "<br />sometimes, some codes on white board were unclear at the back.\n",
            "<br />overall very good!!!\n",
            "<br />\"\n",
            "The lectures were good and clear. And they weren&#039;t too fast. Writing code was somewhat confusing because I didn&#039;t know java before. \n",
            "Actually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well!. thankyou\n",
            "\n",
            "------------------------------------------------------------------------------------------------\n",
            "----------  Student feedback content after applying context sensitive word correction  ---------\n",
            "------------------------------------------------------------------------------------------------ \n",
            "\n",
            "Honestly last seven lectures are good . Lecturers are understandable . Lecture slides are very useful to self -- study also . The given opportunity to ask questions from the lecturer is appreciative . \" Good : < br />please do recap at class starting it&#039;s better for us . < br sometimes teaching speed is very high . < br / < br />Thanks ) < br \" The lectures are good . but a bit speed . A in class working activity is a must one . So please take another hour in thursdays manmade . \" < br />We can hear your voice clearly and can understand the things you teach . Presentation slides also good source to refer .. if you can do more example questions within the classroom and it will help us to understand the principles well . < br \" Lecturers was well structured and well organized . It was easy to understand . Lecture slides and labs were also well organized . Lecturers were good . understandable . The lecture slides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP . Motivated to well . Would have been better if we discussed more about the solutions of coding exercisers . I think I learned a lot from the codes you write in the board . When i compare my codes with yours I can learn about my mistakes and good coding practices that I should follow . There for I think it would be great if we can discuss more examples in the class . madam explained the pop concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future . I satisfy about first 7 lectures . That way of teaching is really good for coming lectures too . lectures are very good . take good effort to make understand every student in the room . very helpful . I was able to obtain a clear picture about OOP and its concepts . \" lecture slides , explanations were very clear . < br />it&#039;s very good to letting ask questions and explain again with suitable examples . < br sometimes , some codes on white board were unclear at the back . < br />overall very good ! < br \" The lectures were good and clear . And the weren&#039;t too fast . Writing code was somewhat confusing because I didn&#039;t know java before . Actually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well . thank-you\n",
            "\n",
            "---------------------------------------------------------------------------------------- \n",
            "----------  Twitter content before applying context sensitive word correction  --------\n",
            "--------------------------------------------------------------------------------------- \n",
            "\n",
            "Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada. #cdnpoli #LPC #CPCLDR��_ https://t.co/ZOZOSe1CqQ\n",
            "#immigration #integration #canada https://t.co/M5cKGyvV8F\n",
            "We want controlled immigration that contributes positively to the UK economy. Same as Australia &amp; Canada. https://t.co/99mYliuOes\n",
            "Is the new Manitoba immigration fee a head tax? https://t.co/LsG7C3vLe9\n",
            "Canada immigration profit influence modernistic delhi yet abhinav: XKofy https://t.co/becgusY2i6\n",
            "Canada Immigration Minister to ���Substantially Increase  Immigration Numbers https://t.co/nEFw30MRaa https://t.co/cyI867PZRV\n",
            "M��me les #USA=pays d'immigration par excellence CONTR��LE RIGOUREUSEMENT l'immigration et acc��s �� la #GreenCARD!��_ https://t.co/IHpVhW2BaG\n",
            "@Shawhelp what changes should be made to Canada's immigration laws due to the influx of immigration and violence?\n",
            "L��immigration irr��guli��re au Canada d��cortiqu��e en 5 questions https://t.co/f4utO5A7ZF\n",
            "L'immigration irr��guli��re au Canada d��cortiqu��e en 5 questions - https://t.co/UiBsEZOqas https://t.co/j77dEvjoiX https://t.co/XXDeIG7Dbu\n",
            "Will Media ask the Liberals if they actually have a solid plan for Canada _��_�_?? From my view -- immigration out of C��_ https://t.co/YAgwmZ8ECp\n",
            "Dan Murray of��Immigration Watch Canada is xenophobic racist fear-mongering liar #racism #canada #cdnpoli #hatecrime��_ https://t.co/kwZ3csvYxM\n",
            "Le Canada lance une vaste campagne d'immigration pour faire face �� son besoin de main d'��uvre https://t.co/kXdfMGTZzN\n",
            "L��#immigration irr��guli��re au #Canada d��cortiqu��e en 5��questions https://t.co/s3hu1OKKIG\n",
            "@Canadidly I've read the Immigration laws of Canada much stricter than the US\n",
            "Canada Immigration Website Traffic Surges And Crashes In Wake Of Trump #fasttraffic,#sitetraffic,#website,#traffic https://t.co/zRlJ26jnkC\n",
            "Mr Know-all of Canada Immigration https://t.co/wTQK4QDiKI\n",
            "Move to Canada @LadyMadonna___ Oh, immigration rules, you can't... https://t.co/5LIEVHO7A4\n",
            "#OnThisDay Annette Toft becomes Canada's 2 millionth immigrant since 1945. Do you know your family's immigration st��_ https://t.co/UvRuw8eR1b\n",
            ".@TheEconomist profiles Canada's open immigration policies &amp; how they contribute to our economic success:��_ https://t.co/4K84EE8Y63\n",
            "Hundreds may lose Canadian citizenship, resident status because of one corrupt immigration consultant https://t.co/x2IfO0EXI2\n",
            "Immigration for canada without india: an compassionate handle: deyFy\n",
            "\"#Jamaican #immigrants #Canada\n",
            "\n",
            "https://t.co/vcmfYGadR5\n",
            "\n",
            "#statistics #immigration\"\n",
            "Mexican visa lift expected to cost Canada $262M over a decade https://t.co/9i72fRhtij\n",
            "Are people still moving to #Canada ??? Oh that's right, they have real immigration laws and it's��_ https://t.co/0C5OBfmxLG\n",
            "Here are more details on the Richmond, B.C. Immigration Consultant Sunny Wang who was sentenced to 7 years in... https://t.co/YXH5W53srO\n",
            "I added a video to a @YouTube playlist https://t.co/CnEyWN40x3 Funny Talking of Haryanavi Jat with Canada Immigration Girl Agent\n",
            "Mexicans Can Now Travel Visa-Free To Canada https://t.co/Ec3XHORO2s https://t.co/RQRr5nebcG\n",
            "L��immigration irr��guli��re au Canada d��cortiqu��e en 5��questions https://t.co/DkpuKyWmaK\n",
            "@SweetnessShawnB Hes the POS that ramped up immigration for Canada, among other globalist policies.\n",
            "Canada lifted visa requirements to Mexico as of Dec 1, 2016. Thoughts? #visa #immigration\n",
            "@HuffingtonPost people Keep praising Canada and Canada has way stricter immigration laws then us they willl boot your liberal American ass\n",
            "\n",
            "---------------------------------------------------------------------------------------\n",
            "----------  Twitter content after applying context sensitive word correction  ---------\n",
            "--------------------------------------------------------------------------------------- \n",
            "\n",
            "Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada .# cdnpoli # LPC # CPCLDR and � _ https://t.co/ZOZOSe1CqQ # immigration # integration # canada https://t.co/M5cKGyvV8F We want controlled immigration that contributes positively to the UK economy . Same as Australia & amp ; Canada . https://t.co/99mYliuOes Is the new Manitoba immigration fee a head tax ? https://t.co/LsG7C3vLe9 Canada immigration profit influence modernistic delhi yet abhinav : XKofy https://t.co/becgusY2i6 Canada Immigration Minister to and and and Substantially Increase Immigration Numbers https://t.co/nEFw30MRaa https://t.co/cyI867PZRV M and and me les # USA = pays d'immigration par excellence CONTR and � LE RIGOUREUSEMENT l'immigration et acc and and s and and la # GreenCARD ! and � _ https://t.co/IHpVhW2BaG @Shawhelp what changes should be made to Canada 's immigration laws due to the influx of immigration and violence ? L � and immigration irr � and guli and � re au Canada d � and cortiqu � and e en 5 questions https://t.co/f4utO5A7ZF L'immigration irr � and guli and � re au Canada d � and cortiqu � and e en 5 questions -- https://t.co/UiBsEZOqas https://t.co/j77dEvjoiX https://t.co/XXDeIG7Dbu Will Media ask the Liberals if they actually have a solid plan for Canada _ and � _ � _ From my view -- immigration out of C and � _ https://t.co/YAgwmZ8ECp Dan Murray of and and Immigration Watch Canada is xenophobic racist fear -- mongering liar # racism # canada # cdnpoli # hatecrime and � _ https://t.co/kwZ3csvYxM Le Canada lance use vast campagne d'immigration pour faire face and and son besoin de main d � and uvre https://t.co/kXdfMGTZzN L � and #immigration irr � � guli � � re au # Canada d � and cortiqu � and e en 5 and and questions https://t.co/s3hu1OKKIG @Canadidly I 've read the Immigration laws of Canada much stricter than the US Canada Immigration Website Traffic Surges And Crashes In Wake Of Trump # fasttraffic,#sitetraffic,#website,#traffic https://t.co/zRlJ26jnkC Mr Know -- all of Canada Immigration https://t.co/wTQK4QDiKI Move to Canada @LadyMadonna _ Oh , immigration rules , you is not ... https://t.co/5LIEVHO7A4 # OnThisDay Annette Toft becomes Canada 's 2 millionth immigrant since 1945 . Do you know your family 's immigration set and � _ https://t.co/UvRuw8eR1b .@TheEconomist profiles Canada 's open immigration policies & amp ; how they contribute to our economic success : and and _ https://t.co/4K84EE8Y63 Hundreds may lose Canadian citizenship , resident status because of one corrupt immigration consultant https://t.co/x2IfO0EXI2 Immigration for canada without india : an compassionate handle : deyFy \" Jamaican # immigrants # Canada https://t.co/vcmfYGadR5 # statistics # immigration \" Mexican visa lift expected to cost Canada $ 262 M over a decade https://t.co/9i72fRhtij Are people still moving to # Canada ? Oh that 's right , they have real immigration laws and it 's and and _ https://t.co/0C5OBfmxLG Here are more details on the Richmond , B.C. Immigration Consultant Sunny Wang who was sentenced to 7 years in ... https://t.co/YXH5W53srO I added a video to a YouTube playlist https://t.co/CnEyWN40x3 Funny Talking of Haryanavi Jay with Canada Immigration Girl Agent Mexicans Can Now Travel Visa - Free To Canada https://t.co/Ec3XHORO2s https://t.co/RQRr5nebcG L and and immigration irr � and guli and � re au Canada d � and cortiqu � and e en 5 and and questions https://t.co/DkpuKyWmaK @SweetnessShawnB He 's the POS that ramped up immigration for Canada , among other globalist policies . Canada lifted visa requirements to Mexico as of Dec 1 , 2016 . Thoughts # visa # immigration @HuffingtonPost people Keep praising Canada and Canada has way stricter immigration laws then us they will boot your liberal American as\n",
            "\n",
            "-------------------------------------------------------------------------------------------- \n",
            "----------  Research paper content before applying context sensitive word correction  --------\n",
            "-------------------------------------------------------------------------------------------- \n",
            "\n",
            "Neural network models have shown their promising opportunities for multi-task\n",
            "learning, which focus on learning the shared layers to extract the common and\n",
            "task-invariant features. However, in most existing approaches, the extracted shared\n",
            "features are prone to be contaminated by task-specific features or the noise brought\n",
            "by other tasks. In this paper, we propose an adversarial multi-task learning framework,\n",
            "alleviating the shared and private latent feature spaces from interfering with\n",
            "each other. We conduct extensive experiments on 16 different text classification\n",
            "tasks, which demonstrates the benefits of our approach. Besides, we show that the\n",
            "shared knowledge learned by our proposed model can be regarded as off-the-shelf\n",
            "knowledge and easily transferred to new tasks.\n",
            "\n",
            "Multi-task learning is an effective approach to improve the performance of a single task with\n",
            "the help of other related tasks. Recently, neural-based models for multi-task learning have become\n",
            "very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural\n",
            "language processing (Collobert andWeston, 2008; Luong et al., 2015), since they provide a convenient\n",
            "way of combining information from multiple tasks.\n",
            "However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the\n",
            "features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1-(a), the general shared-private model introduces\n",
            "two feature spaces for any task: one is used to store task-dependent features, the other is\n",
            "used to capture shared features. The major limitation of this framework is that the shared feature\n",
            "space could contain some unnecessary task-specific features, while some sharable features\n",
            "could also be mixed in private space, suffering from feature redundancy.\n",
            "Taking the following two sentences as examples, which are extracted from two different sentiment\n",
            "classification tasks: Movie reviews and Baby products reviews.\n",
            "The infantile cart is simple and easy to use. This kind of humour is infantile and boring.\n",
            "The word �infantile� indicates negative sentiment in Movie task while it is neutral in Baby task.\n",
            "However, the general shared-private model could place the task-specific word �infantile� in a\n",
            "shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space\n",
            "could also be wasted by some unnecessary features.\n",
            "To address this problem, in this paper we propose an adversarial multi-task framework, in\n",
            "which the shared and private feature spaces are in herently disjoint by introducing orthogonality constraints.Specifically, we design a generic shared private learning framework to model the text sequence.\n",
            "\n",
            "---------------------------------------------------------------------------------------------\n",
            "----------  Research paper content after applying context sensitive word correction  ---------\n",
            "---------------------------------------------------------------------------------------------- \n",
            "\n",
            "Natural network models have shown their promising opportunities for multi -- task learning , which focus on learning the shared layers to extract the common and task -- vibrant features . However , in most existing approaches , the extracted shared features are prone to be contaminated by task -- specific features or the noise brought by other tasks . In this paper , we propose an adversaries multi -- task learning framework , alleviating the shared and private latent feature spaces from interfering with each other . We conduct extensive experiments on 16 different text classification tasks , which demonstrates the benefits of our approach . Besides , we show that the shared knowledge learned by our proposed model can be regarded as off -- the -- shelf knowledge and easily transferred to new tasks . Multi -- task learning is an effective approach to improve the performance of a single task with the help of other related tasks . Recently , neural -- based models for multi -- task learning have become very popular , ranging from computer vision ( Misra et al , 2016 ; Zhang et al , 2014 ) to natural language processing ( Collobert andWeston , 2008 ; Luong et al , 2015 , since they provide a convenient way of combining information from multiple tasks . However , most existing work on multi -- task learning ( Liu et al , 2016c , b ) attempts to divide the features of different tasks into private and shared spaces , merely based on whether parameters of some components should be shared . As shown in Figure 1-(a , the general shared -- private model introduces to feature spaces for any task : one is used to store task -- dependent features , the other is used to capture shared features . The major limitation of this framework is that the shared feature space could contain some unnecessary task -- specific features , while some sharable features could also be mixed in private space , suffering from feature redundancy . Taking the following two sentences as examples , which are extracted from two different sentiment classification tasks : Movie reviews and Baby products reviews . The infantile card is simple and easy to use . This kind of humour is infantile and boring . The word and infantile and indicates negative sentiment in Movie task while it is neutral in Baby task . However , the general shared -- private model could place the task -- specific word and infantile and in a shared space , leaving potential hazards for other tasks . Additionally , the capacity of shared space could also be wasted by some unnecessary features . To address this problem , in this paper we propose an adversaries multi -- task framework , in which the shared and private feature spaces are in entirely dishonest by introducing orthogonality constraints . Specifically , we design a generic shared private learning framework to model the text sequence .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWNiA_dgL0uL"
      },
      "source": [
        "## **Isolated word correction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENlrEWAh1jF3",
        "outputId": "2b9fefde-1f60-40ca-cd36-e25d1634b707"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.metrics.distance  import edit_distance\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "correct_words = words.words()\n",
        "\n",
        "def correctSpellingInText(text):\n",
        "\n",
        "  preprocessedWords = []\n",
        "\n",
        "  for word in text:\n",
        "    #remove HTML tags\n",
        "    html_tags = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    cleantext = re.sub(html_tags, '', word)\n",
        "\n",
        "    #replace punctuation marks by empty string\n",
        "    regular_punct = list(string.punctuation)\n",
        "    for punc in regular_punct:\n",
        "          if punc in cleantext:\n",
        "              cleantext = cleantext.replace(punc, '')\n",
        "\n",
        "    preprocessedWords.append(cleantext)\n",
        "\n",
        "  \n",
        "  print(\"preprocessedWords\", preprocessedWords)\n",
        "  print(\"cleantext\",cleantext)\n",
        "\n",
        "  spellCorrectedText = \"\"\n",
        "\n",
        "  for word in preprocessedWords:\n",
        "    if(len(word)>1 and not(word.isnumeric())):\n",
        "      temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
        "      print(word + \" -> \" + sorted(temp, key = lambda val:val[0])[0][1])\n",
        "      spellCorrectedText = spellCorrectedText + \" \" + sorted(temp, key = lambda val:val[0])[0][1]\n",
        "    else:\n",
        "      spellCorrectedText = spellCorrectedText + \" \" + word\n",
        "\n",
        "  print(spellCorrectedText)\n",
        "\n",
        "  return spellCorrectedText\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFFss_uy7hpp",
        "outputId": "da83c53a-5205-4251-a15e-c9c767f58ae0"
      },
      "source": [
        "second_spell_corrected_student_feedback_content = correctSpellingInText(tokenized_student_feedback_content)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessedWords ['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '', 'Lectures', 'are', 'understandable', '', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'selfstudy', 'also', '', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '', '', 'Good', '', '', '', 'br', '', '', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '', '', '039', '', 's', 'better', 'for', 'us', '', '', 'br', '', '', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', '', '', 'br', '', '', '', 'br', '', '', 'Thanks', '', '', '', '', 'br', '', '', '', 'The', 'lectures', 'are', 'goodbut', 'a', 'bit', 'speedA', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'oneSo', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame', '', '', 'br', '', '', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', '', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', '', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', '', '', 'br', '', '', '', 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', '', 'It', 'was', 'easy', 'to', 'understand', '', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', '', 'Lectures', 'were', 'good', '', 'understandable', '', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'OOP', '', 'Motivated', 'to', 'well', '', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', '', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', '', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', '', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', '', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'exampleslectures', 'were', 'interestingwe', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', '', 'I', 'satisfy', 'about', 'first', '7', 'lectures', '', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', '', 'lectuers', 'are', 'very', 'good', '', 'take', 'good', 'effort', 'to', 'make', 'undersatand', 'every', 'student', 'in', 'the', 'room', '', 'very', 'helpfull', '', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'OOP', 'and', 'its', 'concepts', '', '', 'lecture', 'slides', '', 'explanations', 'were', 'very', 'clear', '', '', 'br', '', '', 'it', '', '', '039', '', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', '', '', 'br', '', '', 'sometimes', '', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', '', '', 'br', '', '', 'overall', 'very', 'good', '', '', '', '', 'br', '', '', '', 'The', 'lectures', 'were', 'good', 'and', 'clear', '', 'And', 'they', 'weren', '', '', '039', '', 't', 'too', 'fast', '', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', '', '', '039', '', 't', 'know', 'java', 'before', '', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'classit', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', '', '', 'thankyou']\n",
            "cleantext thankyou\n",
            "Honestly -> Hester\n",
            "last -> last\n",
            "seven -> seven\n",
            "lectures -> lecture\n",
            "are -> are\n",
            "good -> good\n",
            "Lectures -> Leonurus\n",
            "are -> are\n",
            "understandable -> understandable\n",
            "Lecture -> Lactuca\n",
            "slides -> sides\n",
            "are -> are\n",
            "very -> very\n",
            "useful -> useful\n",
            "to -> to\n",
            "selfstudy -> saleslady\n",
            "also -> also\n",
            "The -> The\n",
            "given -> given\n",
            "opportunity -> opportunity\n",
            "to -> to\n",
            "ask -> ask\n",
            "questions -> question\n",
            "from -> from\n",
            "the -> the\n",
            "lecturer -> lecturer\n",
            "is -> is\n",
            "appreciative -> appreciative\n",
            "Good -> God\n",
            "br -> b\n",
            "please -> please\n",
            "do -> do\n",
            "recap -> recap\n",
            "at -> at\n",
            "class -> class\n",
            "starting -> starting\n",
            "it -> it\n",
            "better -> better\n",
            "for -> for\n",
            "us -> us\n",
            "br -> b\n",
            "sometimes -> sometimes\n",
            "teaching -> teaching\n",
            "speed -> speed\n",
            "is -> is\n",
            "very -> very\n",
            "high -> high\n",
            "br -> b\n",
            "br -> b\n",
            "Thanks -> Thais\n",
            "br -> b\n",
            "The -> The\n",
            "lectures -> lecture\n",
            "are -> are\n",
            "goodbut -> goldbug\n",
            "bit -> bit\n",
            "speedA -> speed\n",
            "in -> in\n",
            "class -> class\n",
            "working -> working\n",
            "activity -> activity\n",
            "is -> is\n",
            "must -> must\n",
            "oneSo -> ohelo\n",
            "please -> please\n",
            "take -> take\n",
            "another -> another\n",
            "hour -> hour\n",
            "in -> in\n",
            "thursdays -> thenadays\n",
            "madame -> madame\n",
            "br -> b\n",
            "We -> W\n",
            "can -> can\n",
            "hear -> hear\n",
            "your -> your\n",
            "voice -> voice\n",
            "clearly -> clearly\n",
            "and -> and\n",
            "can -> can\n",
            "understand -> understand\n",
            "the -> the\n",
            "things -> thing\n",
            "you -> you\n",
            "teach -> teach\n",
            "Presentation -> Predentata\n",
            "slides -> sides\n",
            "also -> also\n",
            "good -> good\n",
            "source -> source\n",
            "to -> to\n",
            "refer -> refer\n",
            "lf -> l\n",
            "you -> you\n",
            "can -> can\n",
            "do -> do\n",
            "more -> more\n",
            "example -> example\n",
            "questions -> question\n",
            "within -> within\n",
            "the -> the\n",
            "classroom -> classroom\n",
            "and -> and\n",
            "it -> it\n",
            "will -> will\n",
            "help -> help\n",
            "us -> us\n",
            "to -> to\n",
            "understand -> understand\n",
            "the -> the\n",
            "principles -> principes\n",
            "well -> well\n",
            "br -> b\n",
            "Lectures -> Leonurus\n",
            "was -> was\n",
            "well -> well\n",
            "structured -> structured\n",
            "and -> and\n",
            "well -> well\n",
            "organized -> organized\n",
            "It -> I\n",
            "was -> was\n",
            "easy -> easy\n",
            "to -> to\n",
            "understand -> understand\n",
            "Lecture -> Lactuca\n",
            "slides -> sides\n",
            "and -> and\n",
            "labs -> lab\n",
            "were -> were\n",
            "also -> also\n",
            "well -> well\n",
            "organized -> organized\n",
            "Lectures -> Leonurus\n",
            "were -> were\n",
            "good -> good\n",
            "understandable -> understandable\n",
            "The -> The\n",
            "lecture -> lecture\n",
            "slides -> sides\n",
            "were -> were\n",
            "well -> well\n",
            "organized -> organized\n",
            "and -> and\n",
            "the -> the\n",
            "examples -> example\n",
            "done -> done\n",
            "in -> in\n",
            "the -> the\n",
            "class -> class\n",
            "helped -> helmed\n",
            "lot -> lot\n",
            "to -> to\n",
            "learn -> learn\n",
            "this -> this\n",
            "new -> new\n",
            "language -> language\n",
            "and -> and\n",
            "also -> also\n",
            "the -> the\n",
            "principles -> principes\n",
            "of -> of\n",
            "OOP -> O\n",
            "Motivated -> Molidae\n",
            "to -> to\n",
            "well -> well\n",
            "Would -> Wolf\n",
            "have -> have\n",
            "been -> been\n",
            "better -> better\n",
            "if -> if\n",
            "we -> we\n",
            "discussed -> discusser\n",
            "more -> more\n",
            "about -> about\n",
            "the -> the\n",
            "solutions -> solution\n",
            "of -> of\n",
            "coding -> codding\n",
            "exercisers -> exerciser\n",
            "think -> think\n",
            "learned -> learned\n",
            "lot -> lot\n",
            "from -> from\n",
            "the -> the\n",
            "codes -> code\n",
            "you -> you\n",
            "write -> write\n",
            "in -> in\n",
            "the -> the\n",
            "board -> board\n",
            "When -> Wren\n",
            "compare -> compare\n",
            "my -> my\n",
            "codes -> code\n",
            "with -> with\n",
            "yours -> yours\n",
            "can -> can\n",
            "learn -> learn\n",
            "about -> about\n",
            "my -> my\n",
            "mistakes -> mistake\n",
            "and -> and\n",
            "good -> good\n",
            "coding -> codding\n",
            "practices -> practice\n",
            "that -> that\n",
            "should -> should\n",
            "follow -> follow\n",
            "There -> Teri\n",
            "fore -> fore\n",
            "think -> think\n",
            "it -> it\n",
            "would -> would\n",
            "be -> be\n",
            "great -> great\n",
            "if -> if\n",
            "we -> we\n",
            "can -> can\n",
            "discuss -> discuss\n",
            "more -> more\n",
            "examples -> example\n",
            "in -> in\n",
            "the -> the\n",
            "class -> class\n",
            "madam -> madam\n",
            "explained -> explainer\n",
            "the -> the\n",
            "oop -> o\n",
            "concepts -> concept\n",
            "clearly -> clearly\n",
            "with -> with\n",
            "exampleslectures -> exampleless\n",
            "were -> were\n",
            "interestingwe -> interesting\n",
            "want -> want\n",
            "more -> more\n",
            "scenario -> scenario\n",
            "examples -> example\n",
            "and -> and\n",
            "answers -> answer\n",
            "with -> with\n",
            "explanations -> explanation\n",
            "in -> in\n",
            "future -> future\n",
            "satisfy -> satisfy\n",
            "about -> about\n",
            "first -> first\n",
            "lectures -> lecture\n",
            "That -> Tat\n",
            "way -> way\n",
            "of -> of\n",
            "teaching -> teaching\n",
            "is -> is\n",
            "really -> really\n",
            "good -> good\n",
            "for -> for\n",
            "coming -> coming\n",
            "lectures -> lecture\n",
            "too -> too\n",
            "lectuers -> lectern\n",
            "are -> are\n",
            "very -> very\n",
            "good -> good\n",
            "take -> take\n",
            "good -> good\n",
            "effort -> effort\n",
            "to -> to\n",
            "make -> make\n",
            "undersatand -> understand\n",
            "every -> every\n",
            "student -> student\n",
            "in -> in\n",
            "the -> the\n",
            "room -> room\n",
            "very -> very\n",
            "helpfull -> helpful\n",
            "was -> was\n",
            "able -> able\n",
            "to -> to\n",
            "obtain -> obtain\n",
            "clear -> clear\n",
            "picture -> picture\n",
            "about -> about\n",
            "OOP -> O\n",
            "and -> and\n",
            "its -> its\n",
            "concepts -> concept\n",
            "lecture -> lecture\n",
            "slides -> sides\n",
            "explanations -> explanation\n",
            "were -> were\n",
            "very -> very\n",
            "clear -> clear\n",
            "br -> b\n",
            "it -> it\n",
            "very -> very\n",
            "good -> good\n",
            "to -> to\n",
            "letting -> lasting\n",
            "ask -> ask\n",
            "questions -> question\n",
            "and -> and\n",
            "explain -> explain\n",
            "again -> again\n",
            "with -> with\n",
            "suitable -> suitable\n",
            "examples -> example\n",
            "br -> b\n",
            "sometimes -> sometimes\n",
            "some -> some\n",
            "codes -> code\n",
            "on -> on\n",
            "white -> white\n",
            "board -> board\n",
            "were -> were\n",
            "unclear -> unclear\n",
            "at -> at\n",
            "the -> the\n",
            "back -> back\n",
            "br -> b\n",
            "overall -> overall\n",
            "very -> very\n",
            "good -> good\n",
            "br -> b\n",
            "The -> The\n",
            "lectures -> lecture\n",
            "were -> were\n",
            "good -> good\n",
            "and -> and\n",
            "clear -> clear\n",
            "And -> Ana\n",
            "they -> they\n",
            "weren -> ween\n",
            "too -> too\n",
            "fast -> fast\n",
            "Writing -> Waibling\n",
            "code -> code\n",
            "was -> was\n",
            "somewhat -> somewhat\n",
            "confusing -> conducing\n",
            "because -> because\n",
            "didn -> dian\n",
            "know -> know\n",
            "java -> jama\n",
            "before -> before\n",
            "Actually -> Achuas\n",
            "teaching -> teaching\n",
            "is -> is\n",
            "very -> very\n",
            "good -> good\n",
            "and -> and\n",
            "can -> can\n",
            "understand -> understand\n",
            "easily -> easily\n",
            "the -> the\n",
            "concepts -> concept\n",
            "by -> by\n",
            "examples -> example\n",
            "which -> which\n",
            "are -> are\n",
            "given -> given\n",
            "in -> in\n",
            "the -> the\n",
            "classit -> classic\n",
            "will -> will\n",
            "be -> be\n",
            "more -> more\n",
            "helpful -> helpful\n",
            "if -> if\n",
            "provide -> provide\n",
            "solved -> solve\n",
            "questions -> question\n",
            "as -> as\n",
            "well -> well\n",
            "thankyou -> thanedom\n",
            " Hester last seven lecture are good  Leonurus are understandable  Lactuca sides are very useful to saleslady also  The given opportunity to ask question from the lecturer is appreciative   God    b   please do recap at class starting it   039  s better for us   b   sometimes teaching speed is very high   b    b   Thais     b    The lecture are goldbug a bit speed in class working activity is a must ohelo please take another hour in thenadays madame   b   W can hear your voice clearly and can understand the thing you teach  Predentata sides also good source to refer  l you can do more example question within the classroom and it will help us to understand the principes well   b    Leonurus was well structured and well organized  I was easy to understand  Lactuca sides and lab were also well organized  Leonurus were good  understandable  The lecture sides were well organized and the example done in the class helmed a lot to learn this new language and also the principes of O  Molidae to well  Wolf have been better if we discusser more about the solution of codding exerciser  I think i learned a lot from the code you write in the board  Wren i compare my code with yours i can learn about my mistake and good codding practice that i should follow  Teri fore i think it would be great if we can discuss more example in the class  madam explainer the o concept clearly with exampleless were interesting want more scenario example and answer with explanation in future  I satisfy about first 7 lecture  Tat way of teaching is really good for coming lecture too  lectern are very good  take good effort to make understand every student in the room  very helpful  I was able to obtain a clear picture about O and its concept   lecture sides  explanation were very clear   b   it   039  s very good to lasting ask question and explain again with suitable example   b   sometimes  some code on white board were unclear at the back   b   overall very good     b    The lecture were good and clear  Ana they ween   039  t too fast  Waibling code was somewhat conducing because I dian   039  t know jama before  Achuas teaching is very good and can understand easily the concept by example which are given in the classic will be more helpful if provide solve question as well   thanedom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJEupR4N_-pI",
        "outputId": "726dcb8e-97a2-4c06-d3cf-a623878ae714"
      },
      "source": [
        "print(\"------------------------------------------------------------------------------- \")\n",
        "print(\"----Student feedback content before applying isolated word correction  --------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(student_feedback_content + '\\n')\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(\"-- Student feedback content after applying isolated word correction correction --\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(second_spell_corrected_student_feedback_content)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------- \n",
            "----Student feedback content before applying isolated word correction  --------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful to self-study also. The given opportunity to ask questions from the lecturer is appreciative. \n",
            "\"Good :) \n",
            "<br />please do recap at class starting it&#039;s better for us.\n",
            "<br />sometimes teaching speed is very high.\n",
            "<br />\n",
            "<br />Thanks ! :)\n",
            "<br /> \"\n",
            "The lectures are good..but a bit speed.A in class working activity is a must one.So please take another hour in thursdays madame.\n",
            "\"\n",
            "<br />We can hear your voice clearly and can understand the things you teach. Presentation slides also good source to refer . lf you can do more example questions within the classroom and it will help us to understand the principles well. \n",
            "<br />\"\n",
            "Lectures was well structured and well organized. It was easy to understand. Lecture slides and labs were also well organized. \n",
            "Lectures were good. understandable. \n",
            "The lecture slides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP. Motivated to well. Would have been better if we discussed more about the solutions of coding exercisers.   \n",
            "I think i learned a lot from the codes you write in the board. When i compare my codes with yours i can learn about my mistakes and good coding practices that i should follow. There fore i think it would be great if we can discuss more examples in the class.\n",
            "madam explained  the oop concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future.\n",
            "I satisfy about first 7 lectures. That way of teaching is really good for coming lectures too.\n",
            "lectuers are very good. take good effort to make undersatand every student in the room. very helpfull. \n",
            "I was able to obtain a clear picture about OOP and its concepts. \n",
            "\"lecture slides, explanations were very clear.\n",
            "<br />it&#039;s very good to letting ask questions and explain again with suitable examples.\n",
            "<br />sometimes, some codes on white board were unclear at the back.\n",
            "<br />overall very good!!!\n",
            "<br />\"\n",
            "The lectures were good and clear. And they weren&#039;t too fast. Writing code was somewhat confusing because I didn&#039;t know java before. \n",
            "Actually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well!. thankyou\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "-- Student feedback content after applying isolated word correction correction --\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            " Hester last seven lecture are good  Leonurus are understandable  Lactuca sides are very useful to saleslady also  The given opportunity to ask question from the lecturer is appreciative   God    b   please do recap at class starting it   039  s better for us   b   sometimes teaching speed is very high   b    b   Thais     b    The lecture are goldbug a bit speed in class working activity is a must ohelo please take another hour in thenadays madame   b   W can hear your voice clearly and can understand the thing you teach  Predentata sides also good source to refer  l you can do more example question within the classroom and it will help us to understand the principes well   b    Leonurus was well structured and well organized  I was easy to understand  Lactuca sides and lab were also well organized  Leonurus were good  understandable  The lecture sides were well organized and the example done in the class helmed a lot to learn this new language and also the principes of O  Molidae to well  Wolf have been better if we discusser more about the solution of codding exerciser  I think i learned a lot from the code you write in the board  Wren i compare my code with yours i can learn about my mistake and good codding practice that i should follow  Teri fore i think it would be great if we can discuss more example in the class  madam explainer the o concept clearly with exampleless were interesting want more scenario example and answer with explanation in future  I satisfy about first 7 lecture  Tat way of teaching is really good for coming lecture too  lectern are very good  take good effort to make understand every student in the room  very helpful  I was able to obtain a clear picture about O and its concept   lecture sides  explanation were very clear   b   it   039  s very good to lasting ask question and explain again with suitable example   b   sometimes  some code on white board were unclear at the back   b   overall very good     b    The lecture were good and clear  Ana they ween   039  t too fast  Waibling code was somewhat conducing because I dian   039  t know jama before  Achuas teaching is very good and can understand easily the concept by example which are given in the classic will be more helpful if provide solve question as well   thanedom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvMgZ2zu8Yyl"
      },
      "source": [
        "## **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ-AyRjV4gdh",
        "outputId": "70d23080-76e2-4bb7-f2eb-fbca55066555"
      },
      "source": [
        "#required libraries for Lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Create WordNetLemmatizer object\n",
        "wordLemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizeText(text):\n",
        "    tokenized_words = word_tokenize(text)\n",
        "    lemmatized_word = [wordLemmatizer.lemmatize(word) for word in tokenized_words]\n",
        "    return \" \".join(lemmatized_word)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqNGQp8z419N"
      },
      "source": [
        "lemmatized_student_feedback = lemmatizeText(student_feedback_content) \n",
        "lemmatized_twitter_content = lemmatizeText(twitter_content)\n",
        "lemmatized_research_paper_content = lemmatizeText(research_paper_content)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO_XnASkL4sC",
        "outputId": "4d4d46de-3ff0-4842-c499-2a1d46cdca93"
      },
      "source": [
        "print(\"------------------------------------------------------------------------------- \")\n",
        "print(\"---------------   Research paper content before lemmatization  -----------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(research_paper_content + '\\n')\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(\"-------------  Research paper content after lemmatization  -------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(lemmatized_research_paper_content)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------- \n",
            "---------------   Research paper content before lemmatization  -----------------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Neural network models have shown their promising opportunities for multi-task\n",
            "learning, which focus on learning the shared layers to extract the common and\n",
            "task-invariant features. However, in most existing approaches, the extracted shared\n",
            "features are prone to be contaminated by task-specific features or the noise brought\n",
            "by other tasks. In this paper, we propose an adversarial multi-task learning framework,\n",
            "alleviating the shared and private latent feature spaces from interfering with\n",
            "each other. We conduct extensive experiments on 16 different text classification\n",
            "tasks, which demonstrates the benefits of our approach. Besides, we show that the\n",
            "shared knowledge learned by our proposed model can be regarded as off-the-shelf\n",
            "knowledge and easily transferred to new tasks.\n",
            "\n",
            "Multi-task learning is an effective approach to improve the performance of a single task with\n",
            "the help of other related tasks. Recently, neural-based models for multi-task learning have become\n",
            "very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural\n",
            "language processing (Collobert andWeston, 2008; Luong et al., 2015), since they provide a convenient\n",
            "way of combining information from multiple tasks.\n",
            "However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the\n",
            "features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1-(a), the general shared-private model introduces\n",
            "two feature spaces for any task: one is used to store task-dependent features, the other is\n",
            "used to capture shared features. The major limitation of this framework is that the shared feature\n",
            "space could contain some unnecessary task-specific features, while some sharable features\n",
            "could also be mixed in private space, suffering from feature redundancy.\n",
            "Taking the following two sentences as examples, which are extracted from two different sentiment\n",
            "classification tasks: Movie reviews and Baby products reviews.\n",
            "The infantile cart is simple and easy to use. This kind of humour is infantile and boring.\n",
            "The word �infantile� indicates negative sentiment in Movie task while it is neutral in Baby task.\n",
            "However, the general shared-private model could place the task-specific word �infantile� in a\n",
            "shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space\n",
            "could also be wasted by some unnecessary features.\n",
            "To address this problem, in this paper we propose an adversarial multi-task framework, in\n",
            "which the shared and private feature spaces are in herently disjoint by introducing orthogonality constraints.Specifically, we design a generic shared private learning framework to model the text sequence.\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "-------------  Research paper content after lemmatization  -------------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Neural network model have shown their promising opportunity for multi-task learning , which focus on learning the shared layer to extract the common and task-invariant feature . However , in most existing approach , the extracted shared feature are prone to be contaminated by task-specific feature or the noise brought by other task . In this paper , we propose an adversarial multi-task learning framework , alleviating the shared and private latent feature space from interfering with each other . We conduct extensive experiment on 16 different text classification task , which demonstrates the benefit of our approach . Besides , we show that the shared knowledge learned by our proposed model can be regarded a off-the-shelf knowledge and easily transferred to new task . Multi-task learning is an effective approach to improve the performance of a single task with the help of other related task . Recently , neural-based model for multi-task learning have become very popular , ranging from computer vision ( Misra et al. , 2016 ; Zhang et al. , 2014 ) to natural language processing ( Collobert andWeston , 2008 ; Luong et al. , 2015 ) , since they provide a convenient way of combining information from multiple task . However , most existing work on multi-task learning ( Liu et al. , 2016c , b ) attempt to divide the feature of different task into private and shared space , merely based on whether parameter of some component should be shared . As shown in Figure 1- ( a ) , the general shared-private model introduces two feature space for any task : one is used to store task-dependent feature , the other is used to capture shared feature . The major limitation of this framework is that the shared feature space could contain some unnecessary task-specific feature , while some sharable feature could also be mixed in private space , suffering from feature redundancy . Taking the following two sentence a example , which are extracted from two different sentiment classification task : Movie review and Baby product review . The infantile cart is simple and easy to use . This kind of humour is infantile and boring . The word �infantile� indicates negative sentiment in Movie task while it is neutral in Baby task . However , the general shared-private model could place the task-specific word �infantile� in a shared space , leaving potential hazard for other task . Additionally , the capacity of shared space could also be wasted by some unnecessary feature . To address this problem , in this paper we propose an adversarial multi-task framework , in which the shared and private feature space are in herently disjoint by introducing orthogonality constraints.Specifically , we design a generic shared private learning framework to model the text sequence .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud-xAS5pToxg"
      },
      "source": [
        "## **Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5e_8GGGQdcl"
      },
      "source": [
        "#required libraries for Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "porterStemmer = PorterStemmer()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L3pNmWdQ-qL"
      },
      "source": [
        "def stemText(text):\n",
        "\n",
        "  stemedText = \"\"\n",
        "\n",
        "  for token in text:\n",
        "    stemedText = stemedText + \" \" + porterStemmer.stem(token)\n",
        "      \n",
        "  return stemedText\n",
        "  \n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDHARYOI9QtO"
      },
      "source": [
        "stemmed_research_paper_content = stemText(tokenized_student_feedback_content)\n",
        "stemmed_twitter_content = stemText(tokenized_twitter_content)\n",
        "stemmed_research_paper_content = stemText(tokenized_research_paper_content)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2lrwpj2Rdt9",
        "outputId": "32f3cfa4-bd23-421c-d336-1d8d5d99c7ff"
      },
      "source": [
        "print(\"------------------------------------------------------------------------------- \")\n",
        "print(\"--------------  Research paper content before Stemming  -----------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(research_paper_content + '\\n')\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(\"--------------  Research paper content after Stemming  -------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(stemmed_research_paper_content)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------- \n",
            "--------------  Research paper content before Stemming  -----------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Neural network models have shown their promising opportunities for multi-task\n",
            "learning, which focus on learning the shared layers to extract the common and\n",
            "task-invariant features. However, in most existing approaches, the extracted shared\n",
            "features are prone to be contaminated by task-specific features or the noise brought\n",
            "by other tasks. In this paper, we propose an adversarial multi-task learning framework,\n",
            "alleviating the shared and private latent feature spaces from interfering with\n",
            "each other. We conduct extensive experiments on 16 different text classification\n",
            "tasks, which demonstrates the benefits of our approach. Besides, we show that the\n",
            "shared knowledge learned by our proposed model can be regarded as off-the-shelf\n",
            "knowledge and easily transferred to new tasks.\n",
            "\n",
            "Multi-task learning is an effective approach to improve the performance of a single task with\n",
            "the help of other related tasks. Recently, neural-based models for multi-task learning have become\n",
            "very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural\n",
            "language processing (Collobert andWeston, 2008; Luong et al., 2015), since they provide a convenient\n",
            "way of combining information from multiple tasks.\n",
            "However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the\n",
            "features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1-(a), the general shared-private model introduces\n",
            "two feature spaces for any task: one is used to store task-dependent features, the other is\n",
            "used to capture shared features. The major limitation of this framework is that the shared feature\n",
            "space could contain some unnecessary task-specific features, while some sharable features\n",
            "could also be mixed in private space, suffering from feature redundancy.\n",
            "Taking the following two sentences as examples, which are extracted from two different sentiment\n",
            "classification tasks: Movie reviews and Baby products reviews.\n",
            "The infantile cart is simple and easy to use. This kind of humour is infantile and boring.\n",
            "The word �infantile� indicates negative sentiment in Movie task while it is neutral in Baby task.\n",
            "However, the general shared-private model could place the task-specific word �infantile� in a\n",
            "shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space\n",
            "could also be wasted by some unnecessary features.\n",
            "To address this problem, in this paper we propose an adversarial multi-task framework, in\n",
            "which the shared and private feature spaces are in herently disjoint by introducing orthogonality constraints.Specifically, we design a generic shared private learning framework to model the text sequence.\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "--------------  Research paper content after Stemming  -------------------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            " neural network model have shown their promis opportun for multi-task learn , which focu on learn the share layer to extract the common and task-invari featur . howev , in most exist approach , the extract share featur are prone to be contamin by task-specif featur or the nois brought by other task . In thi paper , we propos an adversari multi-task learn framework , allevi the share and privat latent featur space from interf with each other . We conduct extens experi on 16 differ text classif task , which demonstr the benefit of our approach . besid , we show that the share knowledg learn by our propos model can be regard as off-the-shelf knowledg and easili transfer to new task . multi-task learn is an effect approach to improv the perform of a singl task with the help of other relat task . recent , neural-bas model for multi-task learn have becom veri popular , rang from comput vision ( misra et al. , 2016 ; zhang et al. , 2014 ) to natur languag process ( collobert andweston , 2008 ; luong et al. , 2015 ) , sinc they provid a conveni way of combin inform from multipl task . howev , most exist work on multi-task learn ( liu et al. , 2016c , b ) attempt to divid the featur of differ task into privat and share space , mere base on whether paramet of some compon should be share . As shown in figur 1- ( a ) , the gener shared-priv model introduc two featur space for ani task : one is use to store task-depend featur , the other is use to captur share featur . the major limit of thi framework is that the share featur space could contain some unnecessari task-specif featur , while some sharabl featur could also be mix in privat space , suffer from featur redund . take the follow two sentenc as exampl , which are extract from two differ sentiment classif task : movi review and babi product review . the infantil cart is simpl and easi to use . thi kind of humour is infantil and bore . the word �infantile� indic neg sentiment in movi task while it is neutral in babi task . howev , the gener shared-priv model could place the task-specif word �infantile� in a share space , leav potenti hazard for other task . addit , the capac of share space could also be wast by some unnecessari featur . To address thi problem , in thi paper we propos an adversari multi-task framework , in which the share and privat featur space are in herent disjoint by introduc orthogon constraints.specif , we design a gener share privat learn framework to model the text sequenc .\n"
          ]
        }
      ]
    }
  ]
}