{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MfB45bJ_uAUe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CRAVPmMcuQSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "9abb0ae3-40d0-40bd-d9bf-35076ac1f553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 690 entries, 0 to 689\n",
            "Data columns (total 16 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   A1      690 non-null    object \n",
            " 1   A2      690 non-null    object \n",
            " 2   A3      690 non-null    float64\n",
            " 3   A4      690 non-null    object \n",
            " 4   A5      690 non-null    object \n",
            " 5   A6      690 non-null    object \n",
            " 6   A7      690 non-null    object \n",
            " 7   A8      690 non-null    float64\n",
            " 8   A9      690 non-null    object \n",
            " 9   A10     690 non-null    object \n",
            " 10  A11     690 non-null    int64  \n",
            " 11  A12     690 non-null    object \n",
            " 12  A13     690 non-null    object \n",
            " 13  A14     690 non-null    object \n",
            " 14  A15     690 non-null    int64  \n",
            " 15  Class   690 non-null    object \n",
            "dtypes: float64(2), int64(2), object(12)\n",
            "memory usage: 86.4+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  A1     A2     A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 Class\n",
              "0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  00202    0     +\n",
              "1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g  00043  560     +\n",
              "2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  00280  824     +\n",
              "3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  00100    3     +\n",
              "4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  00120    0     +"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a3893bd-cf79-40df-b103-be2d3c1de5f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>A11</th>\n",
              "      <th>A12</th>\n",
              "      <th>A13</th>\n",
              "      <th>A14</th>\n",
              "      <th>A15</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b</td>\n",
              "      <td>30.83</td>\n",
              "      <td>0.000</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.25</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>1</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>00202</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a</td>\n",
              "      <td>58.67</td>\n",
              "      <td>4.460</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>3.04</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>6</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>00043</td>\n",
              "      <td>560</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>24.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>1.50</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>00280</td>\n",
              "      <td>824</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b</td>\n",
              "      <td>27.83</td>\n",
              "      <td>1.540</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>3.75</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>5</td>\n",
              "      <td>t</td>\n",
              "      <td>g</td>\n",
              "      <td>00100</td>\n",
              "      <td>3</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b</td>\n",
              "      <td>20.17</td>\n",
              "      <td>5.625</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.71</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>s</td>\n",
              "      <td>00120</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a3893bd-cf79-40df-b103-be2d3c1de5f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a3893bd-cf79-40df-b103-be2d3c1de5f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a3893bd-cf79-40df-b103-be2d3c1de5f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data = pd.read_csv('/content/crx.csv')\n",
        "data.info()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piruI-cGEMfu"
      },
      "source": [
        "## Retrive columns with null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "b8htrvKK-NCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5294cd-3b2c-4fba-9cab-1a199365ac0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A1', 0),\n",
              " ('A2', 0),\n",
              " ('A3', 0),\n",
              " ('A4', 0),\n",
              " ('A5', 0),\n",
              " ('A6', 0),\n",
              " ('A7', 0),\n",
              " ('A8', 0),\n",
              " ('A9', 0),\n",
              " ('A10', 0),\n",
              " ('A11', 0),\n",
              " ('A12', 0),\n",
              " ('A13', 0),\n",
              " ('A14', 0),\n",
              " ('A15', 0),\n",
              " ('Class', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "columns_with_null_values = list(data.isnull().sum().items())\n",
        "columns_with_null_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XoCaUQXEAm-"
      },
      "source": [
        "## Encode categorical variables in train data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nQNTQoTZDXtm"
      },
      "outputs": [],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "le = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhjHoY8y-xh0",
        "outputId": "b1cb1984-8918-46cb-c2bd-f21c88881575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 690 entries, 0 to 689\n",
            "Data columns (total 16 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   A1      690 non-null    int64  \n",
            " 1   A2      690 non-null    int64  \n",
            " 2   A3      690 non-null    float64\n",
            " 3   A4      690 non-null    int64  \n",
            " 4   A5      690 non-null    int64  \n",
            " 5   A6      690 non-null    int64  \n",
            " 6   A7      690 non-null    int64  \n",
            " 7   A8      690 non-null    float64\n",
            " 8   A9      690 non-null    int64  \n",
            " 9   A10     690 non-null    int64  \n",
            " 10  A11     690 non-null    int64  \n",
            " 11  A12     690 non-null    int64  \n",
            " 12  A13     690 non-null    int64  \n",
            " 13  A14     690 non-null    int64  \n",
            " 14  A15     690 non-null    int64  \n",
            " 15  Class   690 non-null    int64  \n",
            "dtypes: float64(2), int64(14)\n",
            "memory usage: 86.4 KB\n"
          ]
        }
      ],
      "source": [
        "objFeatures = data.select_dtypes(include=\"object\").columns\n",
        "objFeatures\n",
        "for feat in objFeatures:\n",
        "    data[feat] = le.fit_transform(data[feat].astype(str))\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "raG0C6Kwm-zL",
        "outputId": "0987781d-d36c-4ea8-a863-223c04823241"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   A1   A2     A3  A4  A5  A6  A7    A8  A9  A10  A11  A12  A13  A14  A15  \\\n",
              "0   2  156  0.000   2   1  13   8  1.25   1    1    1    0    0   68    0   \n",
              "1   1  328  4.460   2   1  11   4  3.04   1    1    6    0    0   11  560   \n",
              "2   1   89  0.500   2   1  11   4  1.50   1    0    0    0    0   96  824   \n",
              "3   2  125  1.540   2   1  13   8  3.75   1    1    5    1    0   31    3   \n",
              "4   2   43  5.625   2   1  13   8  1.71   1    0    0    0    2   37    0   \n",
              "\n",
              "   Class  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a35e9f02-b819-460a-a419-89ee377196ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>A11</th>\n",
              "      <th>A12</th>\n",
              "      <th>A13</th>\n",
              "      <th>A14</th>\n",
              "      <th>A15</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>156</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>328</td>\n",
              "      <td>4.460</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>3.04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>560</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>0.500</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>824</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>125</td>\n",
              "      <td>1.540</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>3.75</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "      <td>5.625</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>1.71</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a35e9f02-b819-460a-a419-89ee377196ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a35e9f02-b819-460a-a419-89ee377196ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a35e9f02-b819-460a-a419-89ee377196ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4uTdw6xL08jh"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['A11','A13'],axis=1)\n",
        "data = data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSWWr6l_123L",
        "outputId": "6faf0917-64c9-4d5b-e49f-45d8336d4763"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.000e+00, 1.560e+02, 0.000e+00, ..., 6.800e+01, 0.000e+00,\n",
              "        0.000e+00],\n",
              "       [1.000e+00, 3.280e+02, 4.460e+00, ..., 1.100e+01, 5.600e+02,\n",
              "        0.000e+00],\n",
              "       [1.000e+00, 8.900e+01, 5.000e-01, ..., 9.600e+01, 8.240e+02,\n",
              "        0.000e+00],\n",
              "       ...,\n",
              "       [1.000e+00, 9.700e+01, 1.350e+01, ..., 6.700e+01, 1.000e+00,\n",
              "        1.000e+00],\n",
              "       [2.000e+00, 2.000e+01, 2.050e-01, ..., 9.600e+01, 7.500e+02,\n",
              "        1.000e+00],\n",
              "       [2.000e+00, 1.970e+02, 3.375e+00, ..., 0.000e+00, 0.000e+00,\n",
              "        1.000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsiUvLsypBgt"
      },
      "source": [
        "## Seperate features and labels | Seperate Train and test data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DcC65uNfoqnA"
      },
      "outputs": [],
      "source": [
        "X,y = data[:,0:13],data[:,13]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                y,\n",
        "                                test_size=0.33,\n",
        "                                random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check label distribution on train and test data set"
      ],
      "metadata": {
        "id": "Uh_xE4i4ntAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "b8g7XohSncEW",
        "outputId": "51c1fe97-81ab-4204-8e96-91fed2a3eeb0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([204.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 258.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "vmb0t5G_nnxX",
        "outputId": "d89eccd7-7ec3-49b9-abee-f03a171b6029"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([103.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 125.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize the data"
      ],
      "metadata": {
        "id": "p-w6jDRmmBBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot 0th column data before normalizing"
      ],
      "metadata": {
        "id": "Vr0I6flToQsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "KlWdnyKxwhXG",
        "outputId": "7d697150-2840-47f7-eae9-e8c6ac9e163b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  8.,   0.,   0.,   0.,   0., 140.,   0.,   0.,   0., 314.]),\n",
              " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(X_train[:,0]) ## plot 0th column data to check data distridution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vBsYSnV1xMkR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaledX_train = scaler.fit_transform(X_train)\n",
        "rescaledX_test = scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot same data after normalizing"
      ],
      "metadata": {
        "id": "jMd0N2TLoD1B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "Jm-4TVQ8xP9y",
        "outputId": "c65098da-cc62-4043-d8e2-01e11efacf16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  8.,   0.,   0.,   0.,   0., 140.,   0.,   0.,   0., 314.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist(rescaledX_train[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check dimension of the train and test data set"
      ],
      "metadata": {
        "id": "87UjbYhppKEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_train[0:3,])\n",
        "print(y_train.shape)\n",
        "print(y_train[0:3,])\n",
        "print(X_test.shape)\n",
        "print(X_test[0:3,])\n",
        "print(y_test.shape)\n",
        "print(y_test[0:3,])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WDaNnzVo-S2",
        "outputId": "c75c0774-32c4-4158-982e-442943cc0786"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(462, 13)\n",
            "[[1.00e+00 8.70e+01 2.50e+00 3.00e+00 3.00e+00 7.00e+00 1.00e+00 4.50e+00\n",
            "  0.00e+00 0.00e+00 0.00e+00 6.70e+01 4.56e+02]\n",
            " [2.00e+00 1.82e+02 2.75e+00 2.00e+00 1.00e+00 1.00e+01 8.00e+00 4.25e+00\n",
            "  1.00e+00 1.00e+00 0.00e+00 6.90e+01 0.00e+00]\n",
            " [0.00e+00 1.70e+02 1.50e+00 2.00e+00 1.00e+00 2.00e+00 8.00e+00 2.50e-01\n",
            "  0.00e+00 0.00e+00 1.00e+00 1.20e+02 1.22e+02]]\n",
            "(462,)\n",
            "[1. 0. 1.]\n",
            "(228, 13)\n",
            "[[1.00e+00 3.49e+02 1.50e+00 2.00e+00 1.00e+00 6.00e+00 3.00e+00 0.00e+00\n",
            "  0.00e+00 1.00e+00 1.00e+00 6.70e+01 1.05e+02]\n",
            " [1.00e+00 2.71e+02 4.00e+00 2.00e+00 1.00e+00 8.00e+00 5.00e+00 0.00e+00\n",
            "  1.00e+00 0.00e+00 0.00e+00 3.10e+01 9.60e+02]\n",
            " [2.00e+00 4.10e+01 0.00e+00 2.00e+00 1.00e+00 4.00e+00 8.00e+00 5.00e-01\n",
            "  0.00e+00 0.00e+00 0.00e+00 4.60e+01 0.00e+00]]\n",
            "(228,)\n",
            "[1. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtjPl7GBsYyO"
      },
      "source": [
        "# **Create Neural Network Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model 1\n",
        "\n",
        "**Input Layer** = 13 neurons\n",
        "\n",
        "**1st Hidden Layer** = 8 neurons\n",
        "\n",
        "**2nd Hidden Layer** = 4 neurons\n",
        "\n",
        "**Output Layer** = 1 neuron\n",
        "\n",
        "**Activation Function** = Rectified Linear Unit\n",
        "\n",
        "**Loss Function** = binary_crossentropy"
      ],
      "metadata": {
        "id": "-LIr9_gqOJw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train, test in kf.split(X):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "        \n",
        "    x_train = X[train]\n",
        "    y_train = y[train]\n",
        "    x_test = X[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    newModel = Sequential()\n",
        "    #specify input dimension using number of features in  \n",
        "    newModel.add(Dense(8, input_dim = len(X_train[0,:]), activation = 'relu'))\n",
        "    newModel.add(Dense(4, activation ='relu'))\n",
        "    #add 1(only one neuron in the last layer) since this do binary classificaion  \n",
        "    newModel.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    newModel.compile(loss = 'binary_crossentropy' , optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    newModel.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,\n",
        "              epochs=500)\n",
        "    \n",
        "    # Model Prediction\n",
        "    pred = newModel.predict(x_test)\n",
        "    \n",
        "    oos_y.append(y_test)\n",
        "    oos_pred.append(pred)    \n",
        "\n",
        "    # Measure this fold's RMSE\n",
        "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "    #print(f\"Fold score (RMSE): {score}\")\n",
        "    f1score = f1_score(y_test,pred.round())\n",
        "    print(\"F1-Score : %.2f\" % ( f1score))\n",
        "\n",
        "# Build the oos prediction list and calculate the error.\n",
        "oos_y = np.concatenate(oos_y)\n",
        "oos_pred = np.concatenate(oos_pred)\n",
        "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
        "\n",
        "print(f\"Mean squared error regression loss: {score}\") \n",
        " \n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_7aww-HxuTW",
        "outputId": "428a5aea-825f-4e5d-ac1d-f8f1be7a0734"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold #1\n",
            "F1-Score : 0.80\n",
            "Fold #2\n",
            "F1-Score : 0.87\n",
            "Fold #3\n",
            "F1-Score : 0.85\n",
            "Fold #4\n",
            "F1-Score : 0.89\n",
            "Fold #5\n",
            "F1-Score : 0.65\n",
            "Mean squared error regression loss: 0.3893486971690928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model 2\n",
        "\n",
        "**Input Layer** = 13 neurons\n",
        "\n",
        "**1st Hidden Layer** = 27 neurons\n",
        "\n",
        "**Output Layer** = 1 neuron\n",
        "\n",
        "**Activation Function** = sigmoid\n",
        "\n",
        "**Loss Function** = binary_crossentropy"
      ],
      "metadata": {
        "id": "po1LfrXlN-fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,log_loss\n",
        "from sklearn.model_selection import KFold\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train, test in kf.split(X):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "        \n",
        "    x_train = X[train]\n",
        "    y_train = y[train]\n",
        "    x_test = X[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    newModel = Sequential()\n",
        "    #specify input dimension using number of features in  \n",
        "    newModel.add(Dense(27, input_dim = len(X_train[0,:]), activation = 'sigmoid'))\n",
        "    #add 1(only one neuron in the last layer) since this do binary classificaion  \n",
        "    newModel.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    newModel.compile(loss = 'binary_crossentropy' , optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    newModel.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,\n",
        "              epochs=500)\n",
        "    \n",
        "    # Model Prediction\n",
        "    pred = newModel.predict(x_test)\n",
        "    \n",
        "    oos_y.append(y_test)\n",
        "    oos_pred.append(pred)    \n",
        "\n",
        "    # Measure this fold's RMSE\n",
        "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "    #print(f\"Fold score (RMSE): {score}\")\n",
        "    f1score = f1_score(y_test,pred.round())\n",
        "    print(\"F1-Score : %.2f\" % ( f1score))\n",
        "\n",
        "# Build the oos prediction list and calculate the error.\n",
        "oos_y = np.concatenate(oos_y)\n",
        "oos_pred = np.concatenate(oos_pred)\n",
        "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
        "logLoss = metrics.log_loss(oos_y,oos_pred)\n",
        "print(f\"Mean squared error regression loss: {score}\") \n",
        "print(f\"Cross-entropy loss: {logLoss}\")   \n",
        "    \n",
        "# Write the cross-validated prediction\n",
        "oos_y = pd.DataFrame(oos_y)\n",
        "oos_pred = pd.DataFrame(oos_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8ba8dd-d8f6-4b12-f272-23954af37df9",
        "id": "j4FtrC051JEH"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold #1\n",
            "F1-Score : 0.84\n",
            "Fold #2\n",
            "F1-Score : 0.88\n",
            "Fold #3\n",
            "F1-Score : 0.82\n",
            "Fold #4\n",
            "F1-Score : 0.88\n",
            "Fold #5\n",
            "F1-Score : 0.84\n",
            "Mean squared error regression loss: 0.3399642628816323\n",
            "Cross-entropy loss: 0.4074338646381005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model 3\n",
        "\n",
        "**Input Layer** = 13 neurons\n",
        "\n",
        "**1st Hidden Layer** = 8 neurons\n",
        "\n",
        "**2nd Hidden Layer** = 4 neurons\n",
        "\n",
        "**Output Layer** = 1 neuron\n",
        "\n",
        "**Activation Function** = sigmoid\n",
        "\n",
        "**Loss Function** = binary_crossentropy\n",
        "\n",
        "*Use callbacks in during training*"
      ],
      "metadata": {
        "id": "7slrOxeAy7QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
        "f1Scores = []\n",
        "oos_pred = []\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train, test in kf.split(X):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "        \n",
        "    x_train = X[train]\n",
        "    y_train = y[train]\n",
        "    x_test = X[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    model1 = Sequential()\n",
        "    #specify input dimension using number of features in  \n",
        "    model1.add(Dense(8, input_dim = len(X_train[0,:]), activation = 'relu'))\n",
        "    model1.add(Dense(4, activation ='relu'))\n",
        "    #add 1(only one neuron in the last layer) since this do binary classificaion  \n",
        "    model1.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    from keras import callbacks\n",
        "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "    callback_a = ModelCheckpoint (filepath='my_best_model.hdf5',monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
        "    callback_b = EarlyStopping(monitor='val_loss',mode='min',patience=20,verbose=1)\n",
        "\n",
        "    model1.compile(loss = 'binary_crossentropy' , optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=256, batch_size=10, callbacks = [callback_a,callback_b])\n",
        "    \n",
        "    pred = model1.predict(x_test)\n",
        "    \n",
        "    #print(f\"Fold score (RMSE): {score}\")\n",
        "    f1score = f1_score(y_test,pred.round())\n",
        "    f1Scores.append(f1score)\n",
        "    print(\"F1-Score : %.2f\" % ( f1score))\n",
        "\n",
        "foldNumber = 1\n",
        "for val in f1Scores:\n",
        "    print(f\"Fold #{foldNumber}\")\n",
        "    foldNumber+=1\n",
        "    print(\"F1-Score : %.2f\" % ( val))\n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "_q-w8DgOy5RC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f63181-516c-4963-c11f-4177348dce15"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold #1\n",
            "Epoch 1/256\n",
            "56/56 [==============================] - 1s 5ms/step - loss: 57.4058 - accuracy: 0.4293 - val_loss: 55.8404 - val_accuracy: 0.5072\n",
            "Epoch 2/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 35.0174 - accuracy: 0.4293 - val_loss: 32.9599 - val_accuracy: 0.5072\n",
            "Epoch 3/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 19.2798 - accuracy: 0.4312 - val_loss: 17.2177 - val_accuracy: 0.5072\n",
            "Epoch 4/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 9.4718 - accuracy: 0.4330 - val_loss: 8.1289 - val_accuracy: 0.4928\n",
            "Epoch 5/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 3.4516 - accuracy: 0.5290 - val_loss: 3.4642 - val_accuracy: 0.4710\n",
            "Epoch 6/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 1.6710 - accuracy: 0.6178 - val_loss: 2.4901 - val_accuracy: 0.5072\n",
            "Epoch 7/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 1.3381 - accuracy: 0.6214 - val_loss: 2.2165 - val_accuracy: 0.5072\n",
            "Epoch 8/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 1.1081 - accuracy: 0.6322 - val_loss: 1.7289 - val_accuracy: 0.5072\n",
            "Epoch 9/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.9951 - accuracy: 0.6522 - val_loss: 1.5105 - val_accuracy: 0.5217\n",
            "Epoch 10/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.8942 - accuracy: 0.6467 - val_loss: 1.1898 - val_accuracy: 0.5290\n",
            "Epoch 11/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.6486 - val_loss: 0.8910 - val_accuracy: 0.6232\n",
            "Epoch 12/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.6884 - val_loss: 0.8711 - val_accuracy: 0.5870\n",
            "Epoch 13/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6329 - accuracy: 0.6757 - val_loss: 0.6946 - val_accuracy: 0.6232\n",
            "Epoch 14/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6186 - accuracy: 0.6938 - val_loss: 0.6925 - val_accuracy: 0.6449\n",
            "Epoch 15/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6223 - accuracy: 0.6993 - val_loss: 0.6892 - val_accuracy: 0.6304\n",
            "Epoch 16/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.7011 - val_loss: 1.0077 - val_accuracy: 0.6087\n",
            "Epoch 17/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6975 - val_loss: 0.8095 - val_accuracy: 0.6159\n",
            "Epoch 18/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.7047 - val_loss: 0.8276 - val_accuracy: 0.6014\n",
            "Epoch 19/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.6830 - val_loss: 0.7820 - val_accuracy: 0.6232\n",
            "Epoch 20/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.7011 - val_loss: 0.7551 - val_accuracy: 0.6377\n",
            "Epoch 21/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7011 - val_loss: 0.7323 - val_accuracy: 0.6304\n",
            "Epoch 22/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.7011 - val_loss: 0.6882 - val_accuracy: 0.6304\n",
            "Epoch 23/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.6920 - val_loss: 0.7130 - val_accuracy: 0.6159\n",
            "Epoch 24/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6114 - accuracy: 0.6957 - val_loss: 0.6900 - val_accuracy: 0.6377\n",
            "Epoch 25/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6102 - accuracy: 0.7011 - val_loss: 0.6718 - val_accuracy: 0.6087\n",
            "Epoch 26/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.6938 - val_loss: 0.7867 - val_accuracy: 0.6377\n",
            "Epoch 27/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6560 - accuracy: 0.7029 - val_loss: 0.9064 - val_accuracy: 0.6159\n",
            "Epoch 28/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.7029 - val_loss: 0.7211 - val_accuracy: 0.6304\n",
            "Epoch 29/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6090 - accuracy: 0.7065 - val_loss: 0.6816 - val_accuracy: 0.6377\n",
            "Epoch 30/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.6975 - val_loss: 0.8186 - val_accuracy: 0.6449\n",
            "Epoch 31/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6110 - accuracy: 0.7011 - val_loss: 0.8494 - val_accuracy: 0.6159\n",
            "Epoch 32/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6203 - accuracy: 0.6975 - val_loss: 0.8822 - val_accuracy: 0.6159\n",
            "Epoch 33/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6090 - accuracy: 0.7029 - val_loss: 0.8015 - val_accuracy: 0.6014\n",
            "Epoch 34/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.6993 - val_loss: 0.7702 - val_accuracy: 0.6449\n",
            "Epoch 35/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6053 - accuracy: 0.7011 - val_loss: 0.8745 - val_accuracy: 0.6522\n",
            "Epoch 36/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6107 - accuracy: 0.7065 - val_loss: 0.7207 - val_accuracy: 0.6449\n",
            "Epoch 37/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7047 - val_loss: 0.7671 - val_accuracy: 0.6087\n",
            "Epoch 38/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.7011 - val_loss: 0.7063 - val_accuracy: 0.6449\n",
            "Epoch 39/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6191 - accuracy: 0.6957 - val_loss: 0.9047 - val_accuracy: 0.6377\n",
            "Epoch 40/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6061 - accuracy: 0.7047 - val_loss: 0.7236 - val_accuracy: 0.6087\n",
            "Epoch 41/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5986 - accuracy: 0.6866 - val_loss: 0.6987 - val_accuracy: 0.6522\n",
            "Epoch 42/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.6938 - val_loss: 0.6921 - val_accuracy: 0.6522\n",
            "Epoch 43/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6030 - accuracy: 0.7156 - val_loss: 0.7872 - val_accuracy: 0.6522\n",
            "Epoch 44/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5913 - accuracy: 0.7083 - val_loss: 0.6993 - val_accuracy: 0.6594\n",
            "Epoch 45/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.7156 - val_loss: 0.7136 - val_accuracy: 0.6594\n",
            "Epoch 45: early stopping\n",
            "F1-Score : 0.71\n",
            "Fold #2\n",
            "Epoch 1/256\n",
            "56/56 [==============================] - 1s 4ms/step - loss: 51.4264 - accuracy: 0.4547 - val_loss: 24.5619 - val_accuracy: 0.4058\n",
            "Epoch 2/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 27.7238 - accuracy: 0.5054 - val_loss: 12.3008 - val_accuracy: 0.6014\n",
            "Epoch 3/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 18.1275 - accuracy: 0.6232 - val_loss: 8.3540 - val_accuracy: 0.6232\n",
            "Epoch 4/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 11.1549 - accuracy: 0.6359 - val_loss: 4.9971 - val_accuracy: 0.6884\n",
            "Epoch 5/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 7.3216 - accuracy: 0.6268 - val_loss: 3.5348 - val_accuracy: 0.6957\n",
            "Epoch 6/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 4.9243 - accuracy: 0.5833 - val_loss: 2.6897 - val_accuracy: 0.6522\n",
            "Epoch 7/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 3.8817 - accuracy: 0.5525 - val_loss: 2.3852 - val_accuracy: 0.5435\n",
            "Epoch 8/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 3.2076 - accuracy: 0.5471 - val_loss: 2.0960 - val_accuracy: 0.6304\n",
            "Epoch 9/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 2.6680 - accuracy: 0.5688 - val_loss: 1.6300 - val_accuracy: 0.6957\n",
            "Epoch 10/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 2.1523 - accuracy: 0.5779 - val_loss: 1.5524 - val_accuracy: 0.5290\n",
            "Epoch 11/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 1.5206 - accuracy: 0.5960 - val_loss: 1.0026 - val_accuracy: 0.6667\n",
            "Epoch 12/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 1.2639 - accuracy: 0.5960 - val_loss: 0.8364 - val_accuracy: 0.5725\n",
            "Epoch 13/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.9382 - accuracy: 0.6286 - val_loss: 0.7943 - val_accuracy: 0.5652\n",
            "Epoch 14/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7974 - accuracy: 0.6322 - val_loss: 0.5829 - val_accuracy: 0.7391\n",
            "Epoch 15/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.6685 - val_loss: 1.2949 - val_accuracy: 0.6232\n",
            "Epoch 16/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7349 - accuracy: 0.6775 - val_loss: 0.6902 - val_accuracy: 0.5580\n",
            "Epoch 17/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6756 - accuracy: 0.6884 - val_loss: 0.5460 - val_accuracy: 0.7681\n",
            "Epoch 18/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8384 - accuracy: 0.7174 - val_loss: 0.5832 - val_accuracy: 0.7319\n",
            "Epoch 19/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6525 - accuracy: 0.7120 - val_loss: 0.6003 - val_accuracy: 0.7101\n",
            "Epoch 20/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6110 - accuracy: 0.7246 - val_loss: 1.0919 - val_accuracy: 0.6667\n",
            "Epoch 21/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.7446 - val_loss: 0.9381 - val_accuracy: 0.5362\n",
            "Epoch 22/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.7319 - val_loss: 0.4767 - val_accuracy: 0.8261\n",
            "Epoch 23/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6381 - accuracy: 0.7500 - val_loss: 0.5063 - val_accuracy: 0.7609\n",
            "Epoch 24/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.7283 - val_loss: 0.4623 - val_accuracy: 0.8406\n",
            "Epoch 25/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.7554 - val_loss: 0.4554 - val_accuracy: 0.7971\n",
            "Epoch 26/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.7699 - val_loss: 0.4658 - val_accuracy: 0.8261\n",
            "Epoch 27/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.7572 - val_loss: 0.4544 - val_accuracy: 0.7971\n",
            "Epoch 28/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7736 - val_loss: 0.4352 - val_accuracy: 0.8043\n",
            "Epoch 29/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7880 - val_loss: 0.7610 - val_accuracy: 0.6957\n",
            "Epoch 30/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.7681 - val_loss: 0.4636 - val_accuracy: 0.8333\n",
            "Epoch 31/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.7681 - val_loss: 0.6072 - val_accuracy: 0.7319\n",
            "Epoch 32/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7754 - val_loss: 0.4039 - val_accuracy: 0.8333\n",
            "Epoch 33/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.7917 - val_loss: 0.5340 - val_accuracy: 0.7391\n",
            "Epoch 34/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7736 - val_loss: 0.7195 - val_accuracy: 0.7246\n",
            "Epoch 35/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7935 - val_loss: 0.4647 - val_accuracy: 0.8188\n",
            "Epoch 36/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7880 - val_loss: 0.5387 - val_accuracy: 0.7609\n",
            "Epoch 37/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7880 - val_loss: 0.4890 - val_accuracy: 0.8188\n",
            "Epoch 38/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7862 - val_loss: 0.3774 - val_accuracy: 0.8261\n",
            "Epoch 39/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7935 - val_loss: 0.4049 - val_accuracy: 0.8406\n",
            "Epoch 40/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.8043 - val_loss: 0.4882 - val_accuracy: 0.7899\n",
            "Epoch 41/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7862 - val_loss: 0.6161 - val_accuracy: 0.7464\n",
            "Epoch 42/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.7844 - val_loss: 0.4602 - val_accuracy: 0.8188\n",
            "Epoch 43/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.8080 - val_loss: 0.4024 - val_accuracy: 0.8551\n",
            "Epoch 44/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7880 - val_loss: 0.7361 - val_accuracy: 0.7609\n",
            "Epoch 45/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4840 - accuracy: 0.8043 - val_loss: 0.7085 - val_accuracy: 0.7391\n",
            "Epoch 46/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7862 - val_loss: 0.3931 - val_accuracy: 0.8551\n",
            "Epoch 47/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.8134 - val_loss: 1.9294 - val_accuracy: 0.7101\n",
            "Epoch 48/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.8098 - val_loss: 0.5830 - val_accuracy: 0.7464\n",
            "Epoch 49/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.8134 - val_loss: 0.3724 - val_accuracy: 0.8623\n",
            "Epoch 50/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.8315 - val_loss: 0.4262 - val_accuracy: 0.8043\n",
            "Epoch 51/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7989 - val_loss: 0.3454 - val_accuracy: 0.8696\n",
            "Epoch 52/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8134 - val_loss: 0.4935 - val_accuracy: 0.7899\n",
            "Epoch 53/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8351 - val_loss: 0.3930 - val_accuracy: 0.8406\n",
            "Epoch 54/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.8207 - val_loss: 0.3570 - val_accuracy: 0.8913\n",
            "Epoch 55/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.8080 - val_loss: 0.5347 - val_accuracy: 0.8043\n",
            "Epoch 56/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.8170 - val_loss: 0.4831 - val_accuracy: 0.7899\n",
            "Epoch 57/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8297 - val_loss: 0.3216 - val_accuracy: 0.8768\n",
            "Epoch 58/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.8243 - val_loss: 0.3788 - val_accuracy: 0.8623\n",
            "Epoch 59/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8297 - val_loss: 0.3583 - val_accuracy: 0.8551\n",
            "Epoch 60/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.8080 - val_loss: 0.3588 - val_accuracy: 0.8768\n",
            "Epoch 61/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.8134 - val_loss: 0.3246 - val_accuracy: 0.8986\n",
            "Epoch 62/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.8207 - val_loss: 0.7531 - val_accuracy: 0.7681\n",
            "Epoch 63/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8170 - val_loss: 0.5178 - val_accuracy: 0.7754\n",
            "Epoch 64/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.8170 - val_loss: 0.8665 - val_accuracy: 0.7464\n",
            "Epoch 65/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.8170 - val_loss: 0.8010 - val_accuracy: 0.6957\n",
            "Epoch 66/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.8225 - val_loss: 0.4787 - val_accuracy: 0.8551\n",
            "Epoch 67/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.8279 - val_loss: 0.3481 - val_accuracy: 0.8841\n",
            "Epoch 68/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.8207 - val_loss: 0.4558 - val_accuracy: 0.8188\n",
            "Epoch 69/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.8297 - val_loss: 0.3912 - val_accuracy: 0.8551\n",
            "Epoch 70/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8134 - val_loss: 0.4588 - val_accuracy: 0.8188\n",
            "Epoch 71/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.8333 - val_loss: 2.2984 - val_accuracy: 0.7101\n",
            "Epoch 72/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.8279 - val_loss: 0.6760 - val_accuracy: 0.7319\n",
            "Epoch 73/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.8279 - val_loss: 0.3521 - val_accuracy: 0.8696\n",
            "Epoch 74/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.8370 - val_loss: 0.3237 - val_accuracy: 0.8841\n",
            "Epoch 75/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.8297 - val_loss: 0.3089 - val_accuracy: 0.8768\n",
            "Epoch 76/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.8442 - val_loss: 0.3666 - val_accuracy: 0.8768\n",
            "Epoch 77/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.8442 - val_loss: 0.3307 - val_accuracy: 0.8913\n",
            "Epoch 78/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8424 - val_loss: 0.3166 - val_accuracy: 0.8913\n",
            "Epoch 79/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.8243 - val_loss: 0.3693 - val_accuracy: 0.8841\n",
            "Epoch 80/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8333 - val_loss: 0.2994 - val_accuracy: 0.8841\n",
            "Epoch 81/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8297 - val_loss: 0.3286 - val_accuracy: 0.8841\n",
            "Epoch 82/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4389 - accuracy: 0.8406 - val_loss: 0.3421 - val_accuracy: 0.8913\n",
            "Epoch 83/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.8261 - val_loss: 0.3023 - val_accuracy: 0.8913\n",
            "Epoch 84/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.8279 - val_loss: 0.5518 - val_accuracy: 0.7754\n",
            "Epoch 85/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8315 - val_loss: 0.4181 - val_accuracy: 0.8333\n",
            "Epoch 86/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8261 - val_loss: 0.3619 - val_accuracy: 0.8768\n",
            "Epoch 87/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8333 - val_loss: 0.4228 - val_accuracy: 0.8478\n",
            "Epoch 88/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4585 - accuracy: 0.8460 - val_loss: 0.3246 - val_accuracy: 0.8986\n",
            "Epoch 89/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8460 - val_loss: 0.5489 - val_accuracy: 0.7899\n",
            "Epoch 90/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4424 - accuracy: 0.8333 - val_loss: 0.3429 - val_accuracy: 0.8696\n",
            "Epoch 91/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8351 - val_loss: 0.3290 - val_accuracy: 0.8768\n",
            "Epoch 92/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8243 - val_loss: 0.3424 - val_accuracy: 0.8841\n",
            "Epoch 93/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8370 - val_loss: 0.4186 - val_accuracy: 0.8478\n",
            "Epoch 94/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.8297 - val_loss: 0.4087 - val_accuracy: 0.8551\n",
            "Epoch 95/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8333 - val_loss: 0.3388 - val_accuracy: 0.8768\n",
            "Epoch 96/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8478 - val_loss: 0.3314 - val_accuracy: 0.8913\n",
            "Epoch 97/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8333 - val_loss: 0.3429 - val_accuracy: 0.8913\n",
            "Epoch 98/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.8406 - val_loss: 0.3198 - val_accuracy: 0.8913\n",
            "Epoch 99/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.8279 - val_loss: 0.3976 - val_accuracy: 0.8406\n",
            "Epoch 100/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8406 - val_loss: 0.5747 - val_accuracy: 0.7754\n",
            "Epoch 100: early stopping\n",
            "F1-Score : 0.82\n",
            "Fold #3\n",
            "Epoch 1/256\n",
            "56/56 [==============================] - 1s 5ms/step - loss: 15.9943 - accuracy: 0.4384 - val_loss: 8.6956 - val_accuracy: 0.4710\n",
            "Epoch 2/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 5.6992 - accuracy: 0.3822 - val_loss: 2.3446 - val_accuracy: 0.4710\n",
            "Epoch 3/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 1.3531 - accuracy: 0.4928 - val_loss: 1.2408 - val_accuracy: 0.5217\n",
            "Epoch 4/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.9932 - accuracy: 0.5543 - val_loss: 1.0706 - val_accuracy: 0.4420\n",
            "Epoch 5/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.9017 - accuracy: 0.5362 - val_loss: 0.9086 - val_accuracy: 0.5217\n",
            "Epoch 6/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8839 - accuracy: 0.5507 - val_loss: 0.8581 - val_accuracy: 0.5217\n",
            "Epoch 7/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.8197 - accuracy: 0.5417 - val_loss: 0.7909 - val_accuracy: 0.5000\n",
            "Epoch 8/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7977 - accuracy: 0.5380 - val_loss: 0.7682 - val_accuracy: 0.6014\n",
            "Epoch 9/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7653 - accuracy: 0.6033 - val_loss: 0.8621 - val_accuracy: 0.5652\n",
            "Epoch 10/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7437 - accuracy: 0.6196 - val_loss: 0.7912 - val_accuracy: 0.5652\n",
            "Epoch 11/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8695 - accuracy: 0.5978 - val_loss: 0.7853 - val_accuracy: 0.5580\n",
            "Epoch 12/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7440 - accuracy: 0.6069 - val_loss: 0.7204 - val_accuracy: 0.5725\n",
            "Epoch 13/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7362 - accuracy: 0.5996 - val_loss: 0.7554 - val_accuracy: 0.5580\n",
            "Epoch 14/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.6051 - val_loss: 0.8716 - val_accuracy: 0.5580\n",
            "Epoch 15/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7307 - accuracy: 0.6069 - val_loss: 0.7171 - val_accuracy: 0.5580\n",
            "Epoch 16/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7084 - accuracy: 0.6123 - val_loss: 0.7116 - val_accuracy: 0.5580\n",
            "Epoch 17/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.6087 - val_loss: 0.7527 - val_accuracy: 0.5652\n",
            "Epoch 18/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.6431 - val_loss: 0.7603 - val_accuracy: 0.5652\n",
            "Epoch 19/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.7096 - accuracy: 0.6123 - val_loss: 0.7803 - val_accuracy: 0.5652\n",
            "Epoch 20/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6817 - accuracy: 0.6196 - val_loss: 0.7284 - val_accuracy: 0.5797\n",
            "Epoch 21/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6974 - accuracy: 0.6196 - val_loss: 0.8102 - val_accuracy: 0.5652\n",
            "Epoch 22/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.6359 - val_loss: 0.7448 - val_accuracy: 0.5725\n",
            "Epoch 23/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.6286 - val_loss: 0.7484 - val_accuracy: 0.5725\n",
            "Epoch 24/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.6268 - val_loss: 0.7935 - val_accuracy: 0.5507\n",
            "Epoch 25/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6341 - val_loss: 0.7645 - val_accuracy: 0.5507\n",
            "Epoch 26/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.6268 - val_loss: 0.7614 - val_accuracy: 0.5725\n",
            "Epoch 27/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6587 - accuracy: 0.6196 - val_loss: 0.7278 - val_accuracy: 0.5652\n",
            "Epoch 28/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.6341 - val_loss: 0.7578 - val_accuracy: 0.5725\n",
            "Epoch 29/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6465 - accuracy: 0.6449 - val_loss: 0.7469 - val_accuracy: 0.5652\n",
            "Epoch 30/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.6250 - val_loss: 0.6824 - val_accuracy: 0.5797\n",
            "Epoch 31/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6304 - val_loss: 0.6772 - val_accuracy: 0.5870\n",
            "Epoch 32/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.6341 - val_loss: 0.6921 - val_accuracy: 0.5797\n",
            "Epoch 33/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.6304 - val_loss: 0.7540 - val_accuracy: 0.5652\n",
            "Epoch 34/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6322 - val_loss: 0.8102 - val_accuracy: 0.5797\n",
            "Epoch 35/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.6304 - val_loss: 0.6822 - val_accuracy: 0.5942\n",
            "Epoch 36/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6330 - accuracy: 0.6250 - val_loss: 0.7020 - val_accuracy: 0.5725\n",
            "Epoch 37/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6395 - val_loss: 0.6824 - val_accuracy: 0.5652\n",
            "Epoch 38/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6352 - accuracy: 0.6341 - val_loss: 0.6595 - val_accuracy: 0.5797\n",
            "Epoch 39/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.6540 - val_loss: 0.6474 - val_accuracy: 0.5797\n",
            "Epoch 40/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.6413 - val_loss: 0.6675 - val_accuracy: 0.5870\n",
            "Epoch 41/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.6359 - val_loss: 0.6269 - val_accuracy: 0.6014\n",
            "Epoch 42/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.6286 - val_loss: 0.6707 - val_accuracy: 0.5870\n",
            "Epoch 43/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.6413 - val_loss: 0.6961 - val_accuracy: 0.5870\n",
            "Epoch 44/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6227 - accuracy: 0.6413 - val_loss: 0.7042 - val_accuracy: 0.5725\n",
            "Epoch 45/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.6377 - val_loss: 0.6524 - val_accuracy: 0.5942\n",
            "Epoch 46/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6213 - accuracy: 0.6377 - val_loss: 0.6232 - val_accuracy: 0.6159\n",
            "Epoch 47/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6194 - accuracy: 0.6467 - val_loss: 0.6474 - val_accuracy: 0.6014\n",
            "Epoch 48/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6085 - accuracy: 0.6812 - val_loss: 0.6673 - val_accuracy: 0.6232\n",
            "Epoch 49/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7246 - val_loss: 0.6351 - val_accuracy: 0.6377\n",
            "Epoch 50/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7174 - val_loss: 0.6194 - val_accuracy: 0.6522\n",
            "Epoch 51/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7355 - val_loss: 0.6272 - val_accuracy: 0.6449\n",
            "Epoch 52/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.7428 - val_loss: 0.6116 - val_accuracy: 0.6957\n",
            "Epoch 53/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7500 - val_loss: 0.6872 - val_accuracy: 0.6159\n",
            "Epoch 54/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7464 - val_loss: 0.6442 - val_accuracy: 0.6522\n",
            "Epoch 55/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.7409 - val_loss: 0.6139 - val_accuracy: 0.6667\n",
            "Epoch 56/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7428 - val_loss: 0.6817 - val_accuracy: 0.6449\n",
            "Epoch 57/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.7464 - val_loss: 0.6192 - val_accuracy: 0.6957\n",
            "Epoch 58/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7518 - val_loss: 0.7143 - val_accuracy: 0.6377\n",
            "Epoch 59/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.7500 - val_loss: 0.6887 - val_accuracy: 0.6304\n",
            "Epoch 60/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5728 - accuracy: 0.7609 - val_loss: 0.6904 - val_accuracy: 0.6304\n",
            "Epoch 61/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5755 - accuracy: 0.7518 - val_loss: 0.6011 - val_accuracy: 0.6739\n",
            "Epoch 62/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.7609 - val_loss: 0.6263 - val_accuracy: 0.6739\n",
            "Epoch 63/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5547 - accuracy: 0.7500 - val_loss: 0.6763 - val_accuracy: 0.6449\n",
            "Epoch 64/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7482 - val_loss: 0.6179 - val_accuracy: 0.6667\n",
            "Epoch 65/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5592 - accuracy: 0.7482 - val_loss: 0.6808 - val_accuracy: 0.6522\n",
            "Epoch 66/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5600 - accuracy: 0.7572 - val_loss: 0.5944 - val_accuracy: 0.6739\n",
            "Epoch 67/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.7591 - val_loss: 0.5960 - val_accuracy: 0.6739\n",
            "Epoch 68/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7627 - val_loss: 0.6336 - val_accuracy: 0.6594\n",
            "Epoch 69/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.7518 - val_loss: 0.6321 - val_accuracy: 0.6667\n",
            "Epoch 70/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7591 - val_loss: 0.6486 - val_accuracy: 0.6812\n",
            "Epoch 71/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7681 - val_loss: 0.5881 - val_accuracy: 0.6957\n",
            "Epoch 72/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5483 - accuracy: 0.7681 - val_loss: 0.5889 - val_accuracy: 0.6884\n",
            "Epoch 73/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5483 - accuracy: 0.7518 - val_loss: 0.6186 - val_accuracy: 0.6812\n",
            "Epoch 74/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7500 - val_loss: 0.6327 - val_accuracy: 0.6667\n",
            "Epoch 75/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.7536 - val_loss: 0.6081 - val_accuracy: 0.6884\n",
            "Epoch 76/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7645 - val_loss: 0.5973 - val_accuracy: 0.6884\n",
            "Epoch 77/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7736 - val_loss: 0.6001 - val_accuracy: 0.7029\n",
            "Epoch 78/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7554 - val_loss: 0.7118 - val_accuracy: 0.6522\n",
            "Epoch 79/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7663 - val_loss: 0.6977 - val_accuracy: 0.6594\n",
            "Epoch 80/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.7699 - val_loss: 0.6960 - val_accuracy: 0.6594\n",
            "Epoch 81/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7609 - val_loss: 0.6044 - val_accuracy: 0.6594\n",
            "Epoch 82/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.7591 - val_loss: 0.6369 - val_accuracy: 0.6739\n",
            "Epoch 83/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5431 - accuracy: 0.7554 - val_loss: 0.6051 - val_accuracy: 0.6812\n",
            "Epoch 84/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7681 - val_loss: 0.7066 - val_accuracy: 0.6594\n",
            "Epoch 85/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.7572 - val_loss: 0.6425 - val_accuracy: 0.6739\n",
            "Epoch 86/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7681 - val_loss: 0.5924 - val_accuracy: 0.7101\n",
            "Epoch 87/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7572 - val_loss: 0.6477 - val_accuracy: 0.6812\n",
            "Epoch 88/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7572 - val_loss: 0.6716 - val_accuracy: 0.6739\n",
            "Epoch 89/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7736 - val_loss: 0.6258 - val_accuracy: 0.6884\n",
            "Epoch 90/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7699 - val_loss: 0.5889 - val_accuracy: 0.7246\n",
            "Epoch 91/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7663 - val_loss: 0.6620 - val_accuracy: 0.6739\n",
            "Epoch 91: early stopping\n",
            "F1-Score : 0.71\n",
            "Fold #4\n",
            "Epoch 1/256\n",
            "56/56 [==============================] - 1s 4ms/step - loss: 5.2848 - accuracy: 0.6250 - val_loss: 1.2861 - val_accuracy: 0.6884\n",
            "Epoch 2/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 1.4498 - accuracy: 0.6232 - val_loss: 0.8450 - val_accuracy: 0.5290\n",
            "Epoch 3/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 1.0044 - accuracy: 0.5761 - val_loss: 0.7741 - val_accuracy: 0.5145\n",
            "Epoch 4/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8695 - accuracy: 0.5616 - val_loss: 0.7886 - val_accuracy: 0.4493\n",
            "Epoch 5/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8006 - accuracy: 0.5598 - val_loss: 0.7583 - val_accuracy: 0.4783\n",
            "Epoch 6/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7628 - accuracy: 0.5543 - val_loss: 0.7366 - val_accuracy: 0.5072\n",
            "Epoch 7/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7927 - accuracy: 0.5634 - val_loss: 0.7823 - val_accuracy: 0.4130\n",
            "Epoch 8/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7064 - accuracy: 0.5743 - val_loss: 0.7668 - val_accuracy: 0.4130\n",
            "Epoch 9/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6986 - accuracy: 0.5489 - val_loss: 0.8755 - val_accuracy: 0.4203\n",
            "Epoch 10/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5761 - val_loss: 0.7763 - val_accuracy: 0.4203\n",
            "Epoch 11/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.5725 - val_loss: 0.7958 - val_accuracy: 0.4203\n",
            "Epoch 12/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6719 - accuracy: 0.5725 - val_loss: 0.7590 - val_accuracy: 0.4565\n",
            "Epoch 13/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.5851 - val_loss: 0.7298 - val_accuracy: 0.5000\n",
            "Epoch 14/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.5743 - val_loss: 0.7442 - val_accuracy: 0.4710\n",
            "Epoch 15/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.5924 - val_loss: 0.7360 - val_accuracy: 0.4855\n",
            "Epoch 16/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.5833 - val_loss: 0.7255 - val_accuracy: 0.5000\n",
            "Epoch 17/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.5851 - val_loss: 0.7395 - val_accuracy: 0.4928\n",
            "Epoch 18/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6525 - accuracy: 0.5815 - val_loss: 0.7232 - val_accuracy: 0.4855\n",
            "Epoch 19/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.5924 - val_loss: 0.7332 - val_accuracy: 0.4855\n",
            "Epoch 20/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.5942 - val_loss: 0.7078 - val_accuracy: 0.5000\n",
            "Epoch 21/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.5942 - val_loss: 0.7237 - val_accuracy: 0.5000\n",
            "Epoch 22/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6463 - accuracy: 0.6033 - val_loss: 0.7270 - val_accuracy: 0.5000\n",
            "Epoch 23/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6451 - accuracy: 0.6069 - val_loss: 0.7202 - val_accuracy: 0.5217\n",
            "Epoch 24/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6449 - accuracy: 0.6051 - val_loss: 0.7297 - val_accuracy: 0.5000\n",
            "Epoch 25/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.5888 - val_loss: 0.7103 - val_accuracy: 0.5145\n",
            "Epoch 26/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.5942 - val_loss: 0.7195 - val_accuracy: 0.5145\n",
            "Epoch 27/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.5924 - val_loss: 0.7035 - val_accuracy: 0.5290\n",
            "Epoch 28/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6033 - val_loss: 0.7115 - val_accuracy: 0.5290\n",
            "Epoch 29/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6033 - val_loss: 0.7545 - val_accuracy: 0.4855\n",
            "Epoch 30/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.5924 - val_loss: 0.7151 - val_accuracy: 0.5217\n",
            "Epoch 31/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6457 - accuracy: 0.6123 - val_loss: 0.7252 - val_accuracy: 0.5217\n",
            "Epoch 32/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6520 - accuracy: 0.6033 - val_loss: 0.7337 - val_accuracy: 0.5217\n",
            "Epoch 33/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.5996 - val_loss: 0.7394 - val_accuracy: 0.5217\n",
            "Epoch 34/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.6196 - val_loss: 0.7124 - val_accuracy: 0.5435\n",
            "Epoch 35/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.5996 - val_loss: 0.7097 - val_accuracy: 0.5290\n",
            "Epoch 36/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.5978 - val_loss: 0.7045 - val_accuracy: 0.5435\n",
            "Epoch 37/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6069 - val_loss: 0.7065 - val_accuracy: 0.5217\n",
            "Epoch 38/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.6051 - val_loss: 0.7125 - val_accuracy: 0.5435\n",
            "Epoch 39/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6393 - accuracy: 0.6087 - val_loss: 0.7233 - val_accuracy: 0.5217\n",
            "Epoch 40/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6343 - accuracy: 0.6069 - val_loss: 0.7318 - val_accuracy: 0.5000\n",
            "Epoch 41/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.5978 - val_loss: 0.7357 - val_accuracy: 0.4855\n",
            "Epoch 42/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.6014 - val_loss: 0.7245 - val_accuracy: 0.5000\n",
            "Epoch 43/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.6033 - val_loss: 0.6851 - val_accuracy: 0.5507\n",
            "Epoch 44/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.6141 - val_loss: 0.6973 - val_accuracy: 0.5652\n",
            "Epoch 45/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.6141 - val_loss: 0.6780 - val_accuracy: 0.5797\n",
            "Epoch 46/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.6268 - val_loss: 0.7142 - val_accuracy: 0.5145\n",
            "Epoch 47/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6224 - accuracy: 0.6123 - val_loss: 0.6878 - val_accuracy: 0.5362\n",
            "Epoch 48/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.6214 - val_loss: 0.7168 - val_accuracy: 0.5145\n",
            "Epoch 49/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6178 - val_loss: 0.6896 - val_accuracy: 0.5580\n",
            "Epoch 50/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.6322 - val_loss: 0.6898 - val_accuracy: 0.5362\n",
            "Epoch 51/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6214 - accuracy: 0.6214 - val_loss: 0.6926 - val_accuracy: 0.5217\n",
            "Epoch 52/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.6286 - val_loss: 0.7022 - val_accuracy: 0.5290\n",
            "Epoch 53/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6163 - accuracy: 0.6159 - val_loss: 0.6731 - val_accuracy: 0.5362\n",
            "Epoch 54/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.6304 - val_loss: 0.6870 - val_accuracy: 0.5362\n",
            "Epoch 55/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6177 - accuracy: 0.6141 - val_loss: 0.6773 - val_accuracy: 0.5290\n",
            "Epoch 56/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6080 - accuracy: 0.6196 - val_loss: 0.6802 - val_accuracy: 0.5290\n",
            "Epoch 57/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.6268 - val_loss: 0.6678 - val_accuracy: 0.5435\n",
            "Epoch 58/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.6214 - val_loss: 0.6865 - val_accuracy: 0.5217\n",
            "Epoch 59/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6152 - accuracy: 0.6105 - val_loss: 0.6649 - val_accuracy: 0.5362\n",
            "Epoch 60/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.6214 - val_loss: 0.6738 - val_accuracy: 0.5362\n",
            "Epoch 61/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6064 - accuracy: 0.6087 - val_loss: 0.6858 - val_accuracy: 0.5290\n",
            "Epoch 62/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6304 - val_loss: 0.6616 - val_accuracy: 0.5580\n",
            "Epoch 63/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.6232 - val_loss: 0.8480 - val_accuracy: 0.5362\n",
            "Epoch 64/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.6123 - val_loss: 0.6773 - val_accuracy: 0.5290\n",
            "Epoch 65/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6015 - accuracy: 0.6087 - val_loss: 0.6469 - val_accuracy: 0.5435\n",
            "Epoch 66/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.6159 - val_loss: 0.6589 - val_accuracy: 0.5435\n",
            "Epoch 67/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5985 - accuracy: 0.6250 - val_loss: 0.6641 - val_accuracy: 0.5290\n",
            "Epoch 68/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.6141 - val_loss: 0.6729 - val_accuracy: 0.5362\n",
            "Epoch 69/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.6178 - val_loss: 0.6482 - val_accuracy: 0.5580\n",
            "Epoch 70/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.6286 - val_loss: 0.6417 - val_accuracy: 0.5652\n",
            "Epoch 71/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.6214 - val_loss: 0.6496 - val_accuracy: 0.5725\n",
            "Epoch 72/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6060 - accuracy: 0.6159 - val_loss: 0.6394 - val_accuracy: 0.5507\n",
            "Epoch 73/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.6322 - val_loss: 0.6451 - val_accuracy: 0.5435\n",
            "Epoch 74/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.6268 - val_loss: 0.6620 - val_accuracy: 0.5290\n",
            "Epoch 75/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.6214 - val_loss: 0.6571 - val_accuracy: 0.5435\n",
            "Epoch 76/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.6214 - val_loss: 0.6598 - val_accuracy: 0.5362\n",
            "Epoch 77/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.6196 - val_loss: 0.6671 - val_accuracy: 0.5362\n",
            "Epoch 78/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6250 - val_loss: 0.6736 - val_accuracy: 0.5290\n",
            "Epoch 79/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.6196 - val_loss: 0.9335 - val_accuracy: 0.5362\n",
            "Epoch 80/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.6159 - val_loss: 0.6531 - val_accuracy: 0.5580\n",
            "Epoch 81/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.6268 - val_loss: 0.8562 - val_accuracy: 0.5290\n",
            "Epoch 82/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.6123 - val_loss: 0.6415 - val_accuracy: 0.5652\n",
            "Epoch 83/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6196 - val_loss: 0.6420 - val_accuracy: 0.5652\n",
            "Epoch 84/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7000 - accuracy: 0.6214 - val_loss: 0.6685 - val_accuracy: 0.5362\n",
            "Epoch 85/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5900 - accuracy: 0.6286 - val_loss: 0.6460 - val_accuracy: 0.5580\n",
            "Epoch 86/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.6286 - val_loss: 0.6490 - val_accuracy: 0.5652\n",
            "Epoch 87/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5873 - accuracy: 0.6322 - val_loss: 0.6531 - val_accuracy: 0.5507\n",
            "Epoch 88/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.6268 - val_loss: 0.6512 - val_accuracy: 0.5652\n",
            "Epoch 89/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.6196 - val_loss: 0.6560 - val_accuracy: 0.5362\n",
            "Epoch 90/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.6286 - val_loss: 0.6556 - val_accuracy: 0.5507\n",
            "Epoch 91/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.6341 - val_loss: 0.6403 - val_accuracy: 0.5725\n",
            "Epoch 92/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6061 - accuracy: 0.6196 - val_loss: 0.6329 - val_accuracy: 0.5507\n",
            "Epoch 93/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.6359 - val_loss: 0.6358 - val_accuracy: 0.5580\n",
            "Epoch 94/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.6322 - val_loss: 0.6364 - val_accuracy: 0.5580\n",
            "Epoch 95/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6358 - accuracy: 0.6304 - val_loss: 0.6322 - val_accuracy: 0.5652\n",
            "Epoch 96/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.6268 - val_loss: 0.6573 - val_accuracy: 0.5580\n",
            "Epoch 97/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.6268 - val_loss: 0.6329 - val_accuracy: 0.5725\n",
            "Epoch 98/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6196 - val_loss: 0.6647 - val_accuracy: 0.5580\n",
            "Epoch 99/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5830 - accuracy: 0.6304 - val_loss: 0.6320 - val_accuracy: 0.5725\n",
            "Epoch 100/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.6395 - val_loss: 0.6270 - val_accuracy: 0.5652\n",
            "Epoch 101/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.6377 - val_loss: 0.7143 - val_accuracy: 0.5580\n",
            "Epoch 102/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.6286 - val_loss: 0.6399 - val_accuracy: 0.5580\n",
            "Epoch 103/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5794 - accuracy: 0.6341 - val_loss: 0.6375 - val_accuracy: 0.5507\n",
            "Epoch 104/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.6359 - val_loss: 0.7645 - val_accuracy: 0.5507\n",
            "Epoch 105/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6209 - accuracy: 0.6304 - val_loss: 0.6334 - val_accuracy: 0.5580\n",
            "Epoch 106/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.6359 - val_loss: 0.6379 - val_accuracy: 0.5725\n",
            "Epoch 107/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5774 - accuracy: 0.6486 - val_loss: 0.6575 - val_accuracy: 0.5652\n",
            "Epoch 108/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.6504 - val_loss: 0.6386 - val_accuracy: 0.5725\n",
            "Epoch 109/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5807 - accuracy: 0.6449 - val_loss: 0.6372 - val_accuracy: 0.5797\n",
            "Epoch 110/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.6341 - val_loss: 0.6430 - val_accuracy: 0.5725\n",
            "Epoch 111/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.6431 - val_loss: 0.6287 - val_accuracy: 0.5725\n",
            "Epoch 112/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.6431 - val_loss: 0.6260 - val_accuracy: 0.5725\n",
            "Epoch 113/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.6431 - val_loss: 0.6241 - val_accuracy: 0.5870\n",
            "Epoch 114/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.6576 - val_loss: 0.6459 - val_accuracy: 0.5652\n",
            "Epoch 115/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.6431 - val_loss: 0.6452 - val_accuracy: 0.5652\n",
            "Epoch 116/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.6413 - val_loss: 0.6543 - val_accuracy: 0.5725\n",
            "Epoch 117/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.6558 - val_loss: 0.6764 - val_accuracy: 0.5652\n",
            "Epoch 118/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6008 - accuracy: 0.6467 - val_loss: 0.6829 - val_accuracy: 0.5870\n",
            "Epoch 119/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5724 - accuracy: 0.6486 - val_loss: 0.6469 - val_accuracy: 0.5797\n",
            "Epoch 120/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.6467 - val_loss: 0.6351 - val_accuracy: 0.6014\n",
            "Epoch 121/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.6594 - val_loss: 0.6068 - val_accuracy: 0.6232\n",
            "Epoch 122/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.6612 - val_loss: 0.6407 - val_accuracy: 0.5797\n",
            "Epoch 123/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.6395 - val_loss: 0.6307 - val_accuracy: 0.5797\n",
            "Epoch 124/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.6486 - val_loss: 0.6708 - val_accuracy: 0.5580\n",
            "Epoch 125/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.6576 - val_loss: 0.6431 - val_accuracy: 0.5942\n",
            "Epoch 126/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.6540 - val_loss: 0.6558 - val_accuracy: 0.5870\n",
            "Epoch 127/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.6522 - val_loss: 0.6622 - val_accuracy: 0.5652\n",
            "Epoch 128/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.6649 - val_loss: 0.6064 - val_accuracy: 0.6594\n",
            "Epoch 129/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.6522 - val_loss: 0.6161 - val_accuracy: 0.6014\n",
            "Epoch 130/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.6612 - val_loss: 0.6627 - val_accuracy: 0.5870\n",
            "Epoch 131/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.6576 - val_loss: 0.6307 - val_accuracy: 0.5942\n",
            "Epoch 132/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5774 - accuracy: 0.6630 - val_loss: 0.6365 - val_accuracy: 0.6014\n",
            "Epoch 133/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.6612 - val_loss: 0.6372 - val_accuracy: 0.6014\n",
            "Epoch 134/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.6576 - val_loss: 0.6495 - val_accuracy: 0.6014\n",
            "Epoch 135/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.6649 - val_loss: 0.6362 - val_accuracy: 0.6087\n",
            "Epoch 136/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5604 - accuracy: 0.6703 - val_loss: 0.6462 - val_accuracy: 0.5942\n",
            "Epoch 137/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.6649 - val_loss: 0.6297 - val_accuracy: 0.6377\n",
            "Epoch 138/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.6667 - val_loss: 0.6070 - val_accuracy: 0.6667\n",
            "Epoch 139/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.6667 - val_loss: 0.6150 - val_accuracy: 0.6304\n",
            "Epoch 140/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.6884 - val_loss: 0.5936 - val_accuracy: 0.6739\n",
            "Epoch 141/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5962 - accuracy: 0.6685 - val_loss: 0.5918 - val_accuracy: 0.6812\n",
            "Epoch 142/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.6866 - val_loss: 0.5720 - val_accuracy: 0.7174\n",
            "Epoch 143/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7319 - val_loss: 0.5353 - val_accuracy: 0.7536\n",
            "Epoch 144/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5405 - accuracy: 0.7246 - val_loss: 0.5485 - val_accuracy: 0.7536\n",
            "Epoch 145/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5219 - accuracy: 0.7536 - val_loss: 0.5271 - val_accuracy: 0.7609\n",
            "Epoch 146/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7572 - val_loss: 0.5259 - val_accuracy: 0.7609\n",
            "Epoch 147/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7790 - val_loss: 0.5167 - val_accuracy: 0.7609\n",
            "Epoch 148/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7681 - val_loss: 0.5116 - val_accuracy: 0.7826\n",
            "Epoch 149/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7754 - val_loss: 0.5193 - val_accuracy: 0.7681\n",
            "Epoch 150/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.7736 - val_loss: 0.4989 - val_accuracy: 0.7754\n",
            "Epoch 151/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7790 - val_loss: 0.6089 - val_accuracy: 0.7609\n",
            "Epoch 152/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7754 - val_loss: 0.4953 - val_accuracy: 0.7826\n",
            "Epoch 153/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7717 - val_loss: 0.5145 - val_accuracy: 0.7681\n",
            "Epoch 154/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7681 - val_loss: 0.5114 - val_accuracy: 0.7826\n",
            "Epoch 155/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7699 - val_loss: 0.5150 - val_accuracy: 0.7754\n",
            "Epoch 156/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.7790 - val_loss: 0.4721 - val_accuracy: 0.8043\n",
            "Epoch 157/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7862 - val_loss: 0.4864 - val_accuracy: 0.8116\n",
            "Epoch 158/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7917 - val_loss: 0.4957 - val_accuracy: 0.7826\n",
            "Epoch 159/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7917 - val_loss: 0.5003 - val_accuracy: 0.7826\n",
            "Epoch 160/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7935 - val_loss: 0.4673 - val_accuracy: 0.8116\n",
            "Epoch 161/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7953 - val_loss: 0.4751 - val_accuracy: 0.8043\n",
            "Epoch 162/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7862 - val_loss: 0.4690 - val_accuracy: 0.8188\n",
            "Epoch 163/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4715 - accuracy: 0.7844 - val_loss: 0.4649 - val_accuracy: 0.8188\n",
            "Epoch 164/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7880 - val_loss: 2.4203 - val_accuracy: 0.7826\n",
            "Epoch 165/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.7772 - val_loss: 0.4497 - val_accuracy: 0.8188\n",
            "Epoch 166/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.7880 - val_loss: 0.5004 - val_accuracy: 0.7899\n",
            "Epoch 167/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.7862 - val_loss: 0.4939 - val_accuracy: 0.7754\n",
            "Epoch 168/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.8025 - val_loss: 0.4450 - val_accuracy: 0.8261\n",
            "Epoch 169/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.7917 - val_loss: 0.4831 - val_accuracy: 0.8188\n",
            "Epoch 170/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7953 - val_loss: 0.4776 - val_accuracy: 0.8188\n",
            "Epoch 171/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4585 - accuracy: 0.8043 - val_loss: 0.4258 - val_accuracy: 0.8333\n",
            "Epoch 172/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.8025 - val_loss: 0.4608 - val_accuracy: 0.8261\n",
            "Epoch 173/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.8025 - val_loss: 0.4400 - val_accuracy: 0.8478\n",
            "Epoch 174/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.7989 - val_loss: 0.4392 - val_accuracy: 0.8478\n",
            "Epoch 175/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7989 - val_loss: 2.1274 - val_accuracy: 0.7464\n",
            "Epoch 176/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.8134 - val_loss: 0.4701 - val_accuracy: 0.8188\n",
            "Epoch 177/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7935 - val_loss: 0.4609 - val_accuracy: 0.8406\n",
            "Epoch 178/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.8062 - val_loss: 0.5787 - val_accuracy: 0.7464\n",
            "Epoch 179/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.7989 - val_loss: 0.4327 - val_accuracy: 0.8116\n",
            "Epoch 180/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.8025 - val_loss: 0.4360 - val_accuracy: 0.8333\n",
            "Epoch 181/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.8025 - val_loss: 0.4184 - val_accuracy: 0.8478\n",
            "Epoch 182/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7971 - val_loss: 0.4491 - val_accuracy: 0.8261\n",
            "Epoch 183/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.8043 - val_loss: 0.4588 - val_accuracy: 0.8116\n",
            "Epoch 184/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.8152 - val_loss: 0.5008 - val_accuracy: 0.8043\n",
            "Epoch 185/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8134 - val_loss: 0.4646 - val_accuracy: 0.8333\n",
            "Epoch 186/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.8170 - val_loss: 0.4277 - val_accuracy: 0.8406\n",
            "Epoch 187/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8043 - val_loss: 0.4603 - val_accuracy: 0.8261\n",
            "Epoch 188/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.8007 - val_loss: 0.4907 - val_accuracy: 0.8188\n",
            "Epoch 189/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4975 - accuracy: 0.8152 - val_loss: 0.4424 - val_accuracy: 0.8333\n",
            "Epoch 190/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8152 - val_loss: 0.4635 - val_accuracy: 0.8333\n",
            "Epoch 191/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.8098 - val_loss: 0.4563 - val_accuracy: 0.8478\n",
            "Epoch 192/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8152 - val_loss: 0.4415 - val_accuracy: 0.8551\n",
            "Epoch 193/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8116 - val_loss: 0.4286 - val_accuracy: 0.8478\n",
            "Epoch 194/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8062 - val_loss: 0.4548 - val_accuracy: 0.8188\n",
            "Epoch 195/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8098 - val_loss: 0.4438 - val_accuracy: 0.8043\n",
            "Epoch 196/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8225 - val_loss: 0.4253 - val_accuracy: 0.8551\n",
            "Epoch 197/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.8080 - val_loss: 0.4369 - val_accuracy: 0.8478\n",
            "Epoch 198/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8134 - val_loss: 0.4310 - val_accuracy: 0.8406\n",
            "Epoch 199/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4278 - accuracy: 0.8243 - val_loss: 0.7059 - val_accuracy: 0.8261\n",
            "Epoch 200/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.8098 - val_loss: 0.4967 - val_accuracy: 0.8043\n",
            "Epoch 201/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8134 - val_loss: 0.4725 - val_accuracy: 0.8043\n",
            "Epoch 201: early stopping\n",
            "F1-Score : 0.83\n",
            "Fold #5\n",
            "Epoch 1/256\n",
            "56/56 [==============================] - 1s 5ms/step - loss: 188.9546 - accuracy: 0.5272 - val_loss: 114.8237 - val_accuracy: 0.4130\n",
            "Epoch 2/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 95.4097 - accuracy: 0.4801 - val_loss: 47.7474 - val_accuracy: 0.4565\n",
            "Epoch 3/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 26.4569 - accuracy: 0.5000 - val_loss: 1.9434 - val_accuracy: 0.6087\n",
            "Epoch 4/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 2.3246 - accuracy: 0.5580 - val_loss: 0.6856 - val_accuracy: 0.6522\n",
            "Epoch 5/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.5580 - val_loss: 0.6845 - val_accuracy: 0.6449\n",
            "Epoch 6/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.5743 - val_loss: 0.6859 - val_accuracy: 0.6232\n",
            "Epoch 7/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.5634 - val_loss: 0.6874 - val_accuracy: 0.6522\n",
            "Epoch 8/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.5960 - val_loss: 0.6831 - val_accuracy: 0.6449\n",
            "Epoch 9/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6717 - accuracy: 0.6105 - val_loss: 0.6783 - val_accuracy: 0.6667\n",
            "Epoch 10/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.6196 - val_loss: 0.6855 - val_accuracy: 0.6739\n",
            "Epoch 11/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.8011 - accuracy: 0.6359 - val_loss: 0.6733 - val_accuracy: 0.6884\n",
            "Epoch 12/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6598 - accuracy: 0.6304 - val_loss: 0.6621 - val_accuracy: 0.6957\n",
            "Epoch 13/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.6431 - val_loss: 0.6600 - val_accuracy: 0.7029\n",
            "Epoch 14/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6522 - val_loss: 0.7271 - val_accuracy: 0.6739\n",
            "Epoch 15/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6554 - accuracy: 0.6793 - val_loss: 0.6916 - val_accuracy: 0.6957\n",
            "Epoch 16/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.6830 - val_loss: 0.6715 - val_accuracy: 0.7174\n",
            "Epoch 17/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.6975 - val_loss: 0.6779 - val_accuracy: 0.7029\n",
            "Epoch 18/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8198 - accuracy: 0.6975 - val_loss: 0.6695 - val_accuracy: 0.7319\n",
            "Epoch 19/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.7047 - val_loss: 0.6642 - val_accuracy: 0.7174\n",
            "Epoch 20/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.7047 - val_loss: 0.6532 - val_accuracy: 0.7536\n",
            "Epoch 21/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6326 - accuracy: 0.7065 - val_loss: 0.6461 - val_accuracy: 0.7464\n",
            "Epoch 22/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6248 - accuracy: 0.7174 - val_loss: 0.6850 - val_accuracy: 0.6812\n",
            "Epoch 23/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7430 - accuracy: 0.7174 - val_loss: 0.6801 - val_accuracy: 0.6957\n",
            "Epoch 24/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6264 - accuracy: 0.7156 - val_loss: 0.6533 - val_accuracy: 0.7609\n",
            "Epoch 25/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6235 - accuracy: 0.7210 - val_loss: 0.6623 - val_accuracy: 0.6957\n",
            "Epoch 26/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.7065 - val_loss: 0.6474 - val_accuracy: 0.7319\n",
            "Epoch 27/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7126 - accuracy: 0.7373 - val_loss: 0.6370 - val_accuracy: 0.7754\n",
            "Epoch 28/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.7337 - val_loss: 0.6264 - val_accuracy: 0.7536\n",
            "Epoch 29/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.7391 - val_loss: 0.6485 - val_accuracy: 0.7101\n",
            "Epoch 30/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.7337 - val_loss: 0.6316 - val_accuracy: 0.7536\n",
            "Epoch 31/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.8932 - accuracy: 0.7283 - val_loss: 0.6379 - val_accuracy: 0.7174\n",
            "Epoch 32/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.7355 - val_loss: 0.6389 - val_accuracy: 0.7391\n",
            "Epoch 33/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5943 - accuracy: 0.7464 - val_loss: 0.6149 - val_accuracy: 0.7609\n",
            "Epoch 34/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.7409 - val_loss: 0.6142 - val_accuracy: 0.7391\n",
            "Epoch 35/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.7500 - val_loss: 0.5985 - val_accuracy: 0.7681\n",
            "Epoch 36/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8049 - accuracy: 0.7446 - val_loss: 0.6045 - val_accuracy: 0.7609\n",
            "Epoch 37/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5822 - accuracy: 0.7373 - val_loss: 0.5954 - val_accuracy: 0.7464\n",
            "Epoch 38/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5707 - accuracy: 0.7572 - val_loss: 0.5928 - val_accuracy: 0.7246\n",
            "Epoch 39/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5726 - accuracy: 0.7536 - val_loss: 0.6030 - val_accuracy: 0.7391\n",
            "Epoch 40/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6949 - accuracy: 0.7482 - val_loss: 0.5945 - val_accuracy: 0.7319\n",
            "Epoch 41/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.7572 - val_loss: 0.5891 - val_accuracy: 0.7246\n",
            "Epoch 42/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.7536 - val_loss: 0.5727 - val_accuracy: 0.7754\n",
            "Epoch 43/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5623 - accuracy: 0.7572 - val_loss: 0.5856 - val_accuracy: 0.7319\n",
            "Epoch 44/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8552 - accuracy: 0.7663 - val_loss: 0.5869 - val_accuracy: 0.7319\n",
            "Epoch 45/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7591 - val_loss: 0.5719 - val_accuracy: 0.7391\n",
            "Epoch 46/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.7645 - val_loss: 0.5519 - val_accuracy: 0.7754\n",
            "Epoch 47/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7510 - accuracy: 0.7699 - val_loss: 0.6002 - val_accuracy: 0.7101\n",
            "Epoch 48/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.7717 - val_loss: 0.5438 - val_accuracy: 0.7536\n",
            "Epoch 49/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7754 - val_loss: 0.5386 - val_accuracy: 0.7536\n",
            "Epoch 50/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 1.1004 - accuracy: 0.7717 - val_loss: 0.7509 - val_accuracy: 0.7319\n",
            "Epoch 51/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7717 - val_loss: 0.5317 - val_accuracy: 0.7536\n",
            "Epoch 52/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5219 - accuracy: 0.7772 - val_loss: 0.5443 - val_accuracy: 0.7391\n",
            "Epoch 53/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7736 - val_loss: 0.5332 - val_accuracy: 0.7609\n",
            "Epoch 54/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7645 - val_loss: 0.6946 - val_accuracy: 0.7464\n",
            "Epoch 55/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7240 - accuracy: 0.7754 - val_loss: 0.5269 - val_accuracy: 0.7681\n",
            "Epoch 56/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7844 - val_loss: 0.5314 - val_accuracy: 0.7681\n",
            "Epoch 57/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7826 - val_loss: 0.6249 - val_accuracy: 0.6957\n",
            "Epoch 58/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5244 - accuracy: 0.7826 - val_loss: 0.5219 - val_accuracy: 0.7826\n",
            "Epoch 59/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7880 - val_loss: 0.5245 - val_accuracy: 0.7681\n",
            "Epoch 60/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7935 - val_loss: 0.5025 - val_accuracy: 0.8188\n",
            "Epoch 61/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.7862 - val_loss: 0.4997 - val_accuracy: 0.8116\n",
            "Epoch 62/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7917 - val_loss: 0.5137 - val_accuracy: 0.8043\n",
            "Epoch 63/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7844 - val_loss: 0.4882 - val_accuracy: 0.8188\n",
            "Epoch 64/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7953 - val_loss: 0.5435 - val_accuracy: 0.7681\n",
            "Epoch 65/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7880 - val_loss: 0.4969 - val_accuracy: 0.7899\n",
            "Epoch 66/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7953 - val_loss: 0.4966 - val_accuracy: 0.7899\n",
            "Epoch 67/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7917 - val_loss: 0.4801 - val_accuracy: 0.8261\n",
            "Epoch 68/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.8062 - val_loss: 0.5547 - val_accuracy: 0.7464\n",
            "Epoch 69/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7989 - val_loss: 0.4844 - val_accuracy: 0.8261\n",
            "Epoch 70/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.8025 - val_loss: 0.5802 - val_accuracy: 0.7464\n",
            "Epoch 71/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4860 - accuracy: 0.7790 - val_loss: 0.4514 - val_accuracy: 0.8478\n",
            "Epoch 72/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.8152 - val_loss: 0.5150 - val_accuracy: 0.7826\n",
            "Epoch 73/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.8025 - val_loss: 0.4879 - val_accuracy: 0.8116\n",
            "Epoch 74/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.8080 - val_loss: 0.4717 - val_accuracy: 0.8188\n",
            "Epoch 75/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.7971 - val_loss: 0.4574 - val_accuracy: 0.8261\n",
            "Epoch 76/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.8152 - val_loss: 0.4869 - val_accuracy: 0.7971\n",
            "Epoch 77/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8243 - val_loss: 0.5417 - val_accuracy: 0.7681\n",
            "Epoch 78/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.8170 - val_loss: 1.3227 - val_accuracy: 0.5797\n",
            "Epoch 79/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.8080 - val_loss: 0.4706 - val_accuracy: 0.8043\n",
            "Epoch 80/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7971 - val_loss: 0.4659 - val_accuracy: 0.8116\n",
            "Epoch 81/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.8080 - val_loss: 0.5304 - val_accuracy: 0.7464\n",
            "Epoch 82/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8062 - val_loss: 0.4816 - val_accuracy: 0.7971\n",
            "Epoch 83/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7989 - val_loss: 0.4660 - val_accuracy: 0.8116\n",
            "Epoch 84/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.8025 - val_loss: 0.4509 - val_accuracy: 0.8261\n",
            "Epoch 85/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4498 - accuracy: 0.8225 - val_loss: 0.4940 - val_accuracy: 0.7826\n",
            "Epoch 86/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.7953 - val_loss: 0.4411 - val_accuracy: 0.8333\n",
            "Epoch 87/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4492 - accuracy: 0.8152 - val_loss: 0.4520 - val_accuracy: 0.8116\n",
            "Epoch 88/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.8188 - val_loss: 0.6121 - val_accuracy: 0.6957\n",
            "Epoch 89/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8062 - val_loss: 0.4582 - val_accuracy: 0.7899\n",
            "Epoch 90/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.8188 - val_loss: 0.4685 - val_accuracy: 0.7971\n",
            "Epoch 91/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.8134 - val_loss: 0.4553 - val_accuracy: 0.8116\n",
            "Epoch 92/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8116 - val_loss: 0.5092 - val_accuracy: 0.7826\n",
            "Epoch 93/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.8207 - val_loss: 0.4447 - val_accuracy: 0.8188\n",
            "Epoch 94/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8351 - val_loss: 0.4686 - val_accuracy: 0.8116\n",
            "Epoch 95/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8279 - val_loss: 0.4567 - val_accuracy: 0.8188\n",
            "Epoch 96/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8098 - val_loss: 0.4520 - val_accuracy: 0.8043\n",
            "Epoch 97/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8007 - val_loss: 0.4994 - val_accuracy: 0.8043\n",
            "Epoch 98/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8134 - val_loss: 0.5489 - val_accuracy: 0.7971\n",
            "Epoch 99/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8152 - val_loss: 0.4484 - val_accuracy: 0.8116\n",
            "Epoch 100/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.8170 - val_loss: 0.4419 - val_accuracy: 0.8116\n",
            "Epoch 101/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8261 - val_loss: 0.4415 - val_accuracy: 0.8116\n",
            "Epoch 102/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8098 - val_loss: 0.4971 - val_accuracy: 0.7971\n",
            "Epoch 103/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8043 - val_loss: 0.4227 - val_accuracy: 0.8188\n",
            "Epoch 104/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8279 - val_loss: 0.6857 - val_accuracy: 0.6957\n",
            "Epoch 105/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.8188 - val_loss: 0.4523 - val_accuracy: 0.8261\n",
            "Epoch 106/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6674 - accuracy: 0.8279 - val_loss: 0.5238 - val_accuracy: 0.8043\n",
            "Epoch 107/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.8207 - val_loss: 0.4314 - val_accuracy: 0.8188\n",
            "Epoch 108/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8188 - val_loss: 0.4473 - val_accuracy: 0.8333\n",
            "Epoch 109/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.8207 - val_loss: 0.4455 - val_accuracy: 0.8116\n",
            "Epoch 110/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.9375 - accuracy: 0.8315 - val_loss: 0.4267 - val_accuracy: 0.8333\n",
            "Epoch 111/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.8442 - val_loss: 0.4444 - val_accuracy: 0.8116\n",
            "Epoch 112/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8333 - val_loss: 0.5485 - val_accuracy: 0.7971\n",
            "Epoch 113/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.8207 - val_loss: 0.4407 - val_accuracy: 0.8116\n",
            "Epoch 114/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8225 - val_loss: 0.4363 - val_accuracy: 0.8333\n",
            "Epoch 115/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.8207 - val_loss: 0.4511 - val_accuracy: 0.8261\n",
            "Epoch 116/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8279 - val_loss: 0.4826 - val_accuracy: 0.8116\n",
            "Epoch 117/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8225 - val_loss: 1.1909 - val_accuracy: 0.6087\n",
            "Epoch 118/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.8116 - val_loss: 0.4166 - val_accuracy: 0.8188\n",
            "Epoch 119/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.8152 - val_loss: 0.4227 - val_accuracy: 0.8406\n",
            "Epoch 120/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.8388 - val_loss: 0.4297 - val_accuracy: 0.8043\n",
            "Epoch 121/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8225 - val_loss: 0.4927 - val_accuracy: 0.7754\n",
            "Epoch 122/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8225 - val_loss: 0.4171 - val_accuracy: 0.8261\n",
            "Epoch 123/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8261 - val_loss: 0.4380 - val_accuracy: 0.8261\n",
            "Epoch 124/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8279 - val_loss: 0.4320 - val_accuracy: 0.8188\n",
            "Epoch 125/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.8225 - val_loss: 0.4468 - val_accuracy: 0.8188\n",
            "Epoch 126/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4487 - accuracy: 0.8098 - val_loss: 0.4815 - val_accuracy: 0.8043\n",
            "Epoch 127/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8032 - accuracy: 0.8261 - val_loss: 0.3933 - val_accuracy: 0.8478\n",
            "Epoch 128/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8587 - val_loss: 0.4039 - val_accuracy: 0.8333\n",
            "Epoch 129/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8261 - val_loss: 0.4475 - val_accuracy: 0.8333\n",
            "Epoch 130/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8188 - val_loss: 0.4674 - val_accuracy: 0.8188\n",
            "Epoch 131/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.8351 - val_loss: 0.4404 - val_accuracy: 0.8188\n",
            "Epoch 132/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.8080 - val_loss: 0.4565 - val_accuracy: 0.7899\n",
            "Epoch 133/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.8279 - val_loss: 0.4485 - val_accuracy: 0.8116\n",
            "Epoch 134/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.8333 - val_loss: 0.4427 - val_accuracy: 0.8116\n",
            "Epoch 135/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8297 - val_loss: 1.1507 - val_accuracy: 0.7174\n",
            "Epoch 136/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.8225 - val_loss: 0.4198 - val_accuracy: 0.8188\n",
            "Epoch 137/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4225 - accuracy: 0.8225 - val_loss: 0.4379 - val_accuracy: 0.8116\n",
            "Epoch 138/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8279 - val_loss: 0.4360 - val_accuracy: 0.8116\n",
            "Epoch 139/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8351 - val_loss: 0.4497 - val_accuracy: 0.8406\n",
            "Epoch 140/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8351 - val_loss: 0.4649 - val_accuracy: 0.8333\n",
            "Epoch 141/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8134 - val_loss: 0.4533 - val_accuracy: 0.7971\n",
            "Epoch 142/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.8297 - val_loss: 0.4146 - val_accuracy: 0.8116\n",
            "Epoch 143/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8279 - val_loss: 0.4522 - val_accuracy: 0.7899\n",
            "Epoch 144/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8170 - val_loss: 0.4439 - val_accuracy: 0.7899\n",
            "Epoch 145/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4086 - accuracy: 0.8333 - val_loss: 0.4787 - val_accuracy: 0.8261\n",
            "Epoch 146/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.8529 - accuracy: 0.8297 - val_loss: 0.3886 - val_accuracy: 0.8188\n",
            "Epoch 147/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8460 - val_loss: 1.1109 - val_accuracy: 0.7319\n",
            "Epoch 148/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8351 - val_loss: 0.4152 - val_accuracy: 0.8188\n",
            "Epoch 149/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8315 - val_loss: 0.4237 - val_accuracy: 0.8333\n",
            "Epoch 150/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8261 - val_loss: 0.4455 - val_accuracy: 0.8116\n",
            "Epoch 151/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8442 - val_loss: 0.4400 - val_accuracy: 0.8043\n",
            "Epoch 152/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4156 - accuracy: 0.8370 - val_loss: 0.3986 - val_accuracy: 0.8261\n",
            "Epoch 153/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8460 - val_loss: 0.4483 - val_accuracy: 0.7899\n",
            "Epoch 154/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8098 - val_loss: 0.4779 - val_accuracy: 0.7899\n",
            "Epoch 155/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8351 - val_loss: 1.1209 - val_accuracy: 0.6884\n",
            "Epoch 156/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8370 - val_loss: 0.4447 - val_accuracy: 0.7971\n",
            "Epoch 157/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.3998 - accuracy: 0.8297 - val_loss: 0.4210 - val_accuracy: 0.8043\n",
            "Epoch 158/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8261 - val_loss: 0.4198 - val_accuracy: 0.8188\n",
            "Epoch 159/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4088 - accuracy: 0.8279 - val_loss: 0.4000 - val_accuracy: 0.8188\n",
            "Epoch 160/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.8351 - val_loss: 0.4003 - val_accuracy: 0.8261\n",
            "Epoch 161/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8587 - val_loss: 0.3872 - val_accuracy: 0.8261\n",
            "Epoch 162/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8243 - val_loss: 0.4445 - val_accuracy: 0.8261\n",
            "Epoch 163/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8333 - val_loss: 0.4433 - val_accuracy: 0.8478\n",
            "Epoch 164/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8207 - val_loss: 0.4254 - val_accuracy: 0.8043\n",
            "Epoch 165/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8333 - val_loss: 0.4689 - val_accuracy: 0.8406\n",
            "Epoch 166/256\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8297 - val_loss: 0.4408 - val_accuracy: 0.8261\n",
            "Epoch 167/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.8442 - val_loss: 0.7100 - val_accuracy: 0.7754\n",
            "Epoch 168/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8460 - val_loss: 0.4430 - val_accuracy: 0.8188\n",
            "Epoch 169/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.8460 - val_loss: 0.4149 - val_accuracy: 0.8188\n",
            "Epoch 170/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4055 - accuracy: 0.8351 - val_loss: 0.3994 - val_accuracy: 0.8333\n",
            "Epoch 171/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8279 - val_loss: 0.4341 - val_accuracy: 0.8261\n",
            "Epoch 172/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.8388 - val_loss: 0.4157 - val_accuracy: 0.8261\n",
            "Epoch 173/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8333 - val_loss: 0.4604 - val_accuracy: 0.8333\n",
            "Epoch 174/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8406 - val_loss: 0.4133 - val_accuracy: 0.8188\n",
            "Epoch 175/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8370 - val_loss: 1.2923 - val_accuracy: 0.7899\n",
            "Epoch 176/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8406 - val_loss: 0.4202 - val_accuracy: 0.8261\n",
            "Epoch 177/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8297 - val_loss: 0.4151 - val_accuracy: 0.8188\n",
            "Epoch 178/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8243 - val_loss: 0.4281 - val_accuracy: 0.8188\n",
            "Epoch 179/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8424 - val_loss: 0.5471 - val_accuracy: 0.8333\n",
            "Epoch 180/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8406 - val_loss: 0.3872 - val_accuracy: 0.8478\n",
            "Epoch 181/256\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4145 - accuracy: 0.8496 - val_loss: 0.4335 - val_accuracy: 0.8188\n",
            "Epoch 181: early stopping\n",
            "F1-Score : 0.81\n",
            "Fold #1\n",
            "F1-Score : 0.71\n",
            "Fold #2\n",
            "F1-Score : 0.82\n",
            "Fold #3\n",
            "F1-Score : 0.71\n",
            "Fold #4\n",
            "F1-Score : 0.83\n",
            "Fold #5\n",
            "F1-Score : 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ML Model 4\n",
        "\n",
        "**Input Layer** = 13 neurons\n",
        "\n",
        "**1st Hidden Layer** = 20 neurons\n",
        "\n",
        "**2nd Hidden Layer** = 10 neurons\n",
        "\n",
        "**Output Layer** = 1 neuron\n",
        "\n",
        "**Activation Function** = sigmoid\n",
        "\n",
        "**Loss Function** = binary_crossentropy\n",
        "\n"
      ],
      "metadata": {
        "id": "jdk09Ga2paFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "kf = KFold(5, shuffle=True, random_state=10) # Use for KFold classification\n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train, test in kf.split(X):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "        \n",
        "    x_train = X[train]\n",
        "    y_train = y[train]\n",
        "    x_test = X[test]\n",
        "    y_test = y[test]\n",
        "    \n",
        "    Model2 = Sequential()\n",
        "    Model2.add(Dense(8, input_dim=X.shape[1], activation='relu',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))\n",
        "    Model2.add(Dense(4, activation='relu'))\n",
        "    Model2.add(Dense(1,activation='sigmoid'))\n",
        "    Model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Model2.compile(loss = 'binary_crossentropy' , optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    Model2.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,epochs=500)\n",
        "    \n",
        "    pred = Model2.predict(x_test)\n",
        "    \n",
        "    oos_y.append(y_test)\n",
        "    oos_pred.append(pred)    \n",
        "\n",
        "    # Measure this fold's RMSE\n",
        "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "    #print(f\"Fold score (RMSE): {score}\")\n",
        "    \n",
        "    f1score = f1_score(y_test,pred.round())\n",
        "    print(\"F1-Score : %.2f\" % ( f1score))\n",
        "\n",
        "# Build the oos prediction list and calculate the error.\n",
        "oos_y = np.concatenate(oos_y)\n",
        "oos_pred = np.concatenate(oos_pred)\n",
        "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
        "#print(f\"Final, out of sample score (RMSE): {score}\")    \n",
        "    \n",
        "# Write the cross-validated prediction\n",
        "oos_y = pd.DataFrame(oos_y)\n",
        "oos_pred = pd.DataFrame(oos_pred)"
      ],
      "metadata": {
        "id": "sP2pw16EpvQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd15137a-2f53-4841-8a23-863c948a268e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold #1\n",
            "F1-Score : 0.83\n",
            "Fold #2\n",
            "F1-Score : 0.86\n",
            "Fold #3\n",
            "F1-Score : 0.82\n",
            "Fold #4\n",
            "F1-Score : 0.84\n",
            "Fold #5\n",
            "F1-Score : 0.88\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WtjPl7GBsYyO"
      ],
      "name": "219315E_Feed_Forward_Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}