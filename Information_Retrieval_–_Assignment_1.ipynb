{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information Retrieval – Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUiUC0pr4kZLCLNGmZ+A1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaveeshBaddage/DataScienceImpl/blob/main/Information_Retrieval_%E2%80%93_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lblfiO0jPMPg"
      },
      "source": [
        "**Get Content from text files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpOC9gUH2JIm"
      },
      "source": [
        "def read_file(filename):\n",
        "    infile = open(filename) # windows users should use codecs.open after importing codecs\n",
        "    contents = infile.read()\n",
        "    infile.close()\n",
        "    return contents"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1X5KxwUPR4s"
      },
      "source": [
        "**Read contents in the text files into the runtime**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqlqjtamNRdH"
      },
      "source": [
        "student_feedback_content =  read_file(\"/content/Student Course Feedback.txt\") \n",
        "twitter_content = read_file(\"/content/Twitter Data.txt\")\n",
        "research_paper_content = read_file(\"/content/research paper.txt\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "z8L2h46nNg-a",
        "outputId": "34a973ec-1493-497d-dec4-be750b5e7898"
      },
      "source": [
        "student_feedback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful to self-study also. The given opportunity to ask questions from the lecturer is appreciative. \\n\"Good :) \\n<br />please do recap at class starting it&#039;s better for us.\\n<br />sometimes teaching speed is very high.\\n<br />\\n<br />Thanks ! :)\\n<br /> \"\\nThe lectures are good..but a bit speed.A in class working activity is a must one.So please take another hour in thursdays madame.\\n\"\\n<br />We can hear your voice clearly and can understand the things you teach. Presentation slides also good source to refer . lf you can do more example questions within the classroom and it will help us to understand the principles well. \\n<br />\"\\nLectures was well structured and well organized. It was easy to understand. Lecture slides and labs were also well organized. \\nLectures were good. understandable. \\nThe lecture slides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP. Motivated to well. Would have been better if we discussed more about the solutions of coding exercisers.   \\nI think i learned a lot from the codes you write in the board. When i compare my codes with yours i can learn about my mistakes and good coding practices that i should follow. There fore i think it would be great if we can discuss more examples in the class.\\nmadam explained  the oop concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future.\\nI satisfy about first 7 lectures. That way of teaching is really good for coming lectures too.\\nlectuers are very good. take good effort to make undersatand every student in the room. very helpfull. \\nI was able to obtain a clear picture about OOP and its concepts. \\n\"lecture slides, explanations were very clear.\\n<br />it&#039;s very good to letting ask questions and explain again with suitable examples.\\n<br />sometimes, some codes on white board were unclear at the back.\\n<br />overall very good!!!\\n<br />\"\\nThe lectures were good and clear. And they weren&#039;t too fast. Writing code was somewhat confusing because I didn&#039;t know java before. \\nActually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well!. thankyou'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMHA-sRPffz"
      },
      "source": [
        "**Tokenize the Content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ5UGVOJOjLX",
        "outputId": "747868d7-fa1c-4224-e51e-33afe3f39b75"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "tokenized_student_feedback_content = word_tokenize(student_feedback_content)\n",
        "tokenized_twitter_content = word_tokenize(twitter_content)\n",
        "tokenized_research_paper_content = word_tokenize(research_paper_content)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVmGth5oRTKH"
      },
      "source": [
        "len(tokenized_student_feedback_content)\n",
        "tokenized_student_feedback_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVsbIQBDo7P"
      },
      "source": [
        "**Context sensitive word correction on the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qjRwzW5ADkKz",
        "outputId": "5db40d22-eba7-4874-ac6a-d8a906b7f421"
      },
      "source": [
        "#required libraries for context sensitive Spell checker\n",
        "!pip install neuspell\n",
        "from neuspell import SclstmChecker\n",
        "checker = SclstmChecker()\n",
        "\n",
        "checker.from_pretrained()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neuspell\n",
            "  Downloading neuspell-1.0.0-py3-none-any.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neuspell) (1.19.5)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.7 MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from neuspell) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from neuspell) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->neuspell) (3.7.4.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert->neuspell) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert->neuspell) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.59-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 54.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.59\n",
            "  Downloading botocore-1.21.59-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 54.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.59->boto3->pytorch-pretrained-bert->neuspell) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.59->boto3->pytorch-pretrained-bert->neuspell) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert->neuspell) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert->neuspell) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert->neuspell) (2.10)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->neuspell) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 72.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->neuspell) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->neuspell) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->neuspell) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->neuspell) (1.0.1)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, pyyaml, tokenizers, sacremoses, huggingface-hub, boto3, transformers, sentencepiece, pytorch-pretrained-bert, jsonlines, neuspell\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.59 botocore-1.21.59 huggingface-hub-0.0.19 jmespath-0.10.0 jsonlines-2.0.0 neuspell-1.0.0 pytorch-pretrained-bert-0.6.2 pyyaml-5.4.1 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data folder is set to `/usr/local/lib/python3.7/dist-packages/neuspell/../data` script\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "916faa2f251148278d736491a2919e73",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c799a074d1014ab28f353acc3ac60ef5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21bedcda8f0e4dce8edc792f758c67a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0172cd017ee14fe6a5f38e89ecfdd62f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise created\n",
            "Pretrained model downloading start (may take few seconds to couple of minutes based on download speed) ...\n",
            "Pretrained model download success\n",
            "loading vocab from path:/usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "SCLSTM(\n",
            "  (lstmmodule): LSTM(294, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (dense): Linear(in_features=1024, out_features=100002, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n",
            "112111266\n",
            "loading pretrained weights from path:/usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise\n",
            "Loading model params from checkpoint dir: /usr/local/lib/python3.7/dist-packages/neuspell/../data/checkpoints/scrnn-probwordnoise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzMujc2EE017",
        "outputId": "ea74cbe5-1f4f-4691-863b-bbb7137877dd"
      },
      "source": [
        "spell_corrected_student_feedback_content = checker.correct_strings([student_feedback_content])\n",
        "spell_corrected_twitter_content = checker.correct_strings([twitter_content])\n",
        "spell_corrected_research_paper_content = checker.correct_strings([research_paper_content])\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 6.578122 secs\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 8.493353 secs\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 6.588574 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2UBdLHcF1Wc",
        "outputId": "f7e4440e-5ae8-4e50-d0a5-9ce2aaf62c8f"
      },
      "source": [
        "print(\"------------------------------------------------------------------------------- \")\n",
        "print(\"----------  Student feedback content before applying spell correction  --------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(student_feedback_content + '\\n')\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(\"----------  Student feedback content after applying spell correction  ---------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(spell_corrected_student_feedback_content[0])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------- \n",
            "----------  Student feedback content before applying spell correction  --------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful to self-study also. The given opportunity to ask questions from the lecturer is appreciative. \n",
            "\"Good :) \n",
            "<br />please do recap at class starting it&#039;s better for us.\n",
            "<br />sometimes teaching speed is very high.\n",
            "<br />\n",
            "<br />Thanks ! :)\n",
            "<br /> \"\n",
            "The lectures are good..but a bit speed.A in class working activity is a must one.So please take another hour in thursdays madame.\n",
            "\"\n",
            "<br />We can hear your voice clearly and can understand the things you teach. Presentation slides also good source to refer . lf you can do more example questions within the classroom and it will help us to understand the principles well. \n",
            "<br />\"\n",
            "Lectures was well structured and well organized. It was easy to understand. Lecture slides and labs were also well organized. \n",
            "Lectures were good. understandable. \n",
            "The lecture slides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP. Motivated to well. Would have been better if we discussed more about the solutions of coding exercisers.   \n",
            "I think i learned a lot from the codes you write in the board. When i compare my codes with yours i can learn about my mistakes and good coding practices that i should follow. There fore i think it would be great if we can discuss more examples in the class.\n",
            "madam explained  the oop concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future.\n",
            "I satisfy about first 7 lectures. That way of teaching is really good for coming lectures too.\n",
            "lectuers are very good. take good effort to make undersatand every student in the room. very helpfull. \n",
            "I was able to obtain a clear picture about OOP and its concepts. \n",
            "\"lecture slides, explanations were very clear.\n",
            "<br />it&#039;s very good to letting ask questions and explain again with suitable examples.\n",
            "<br />sometimes, some codes on white board were unclear at the back.\n",
            "<br />overall very good!!!\n",
            "<br />\"\n",
            "The lectures were good and clear. And they weren&#039;t too fast. Writing code was somewhat confusing because I didn&#039;t know java before. \n",
            "Actually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well!. thankyou\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "----------  Student feedback content after applying spell correction  ---------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Honestly last seven lectures are good . Lecturers are understandable . Lecture slides are very useful to self -- study also . The given opportunity to ask questions from the lecturer is appreciative . \" Good : < br />please do recap at class starting it&#039;s better for us . < br sometimes teaching speed is very high . < br / < br />Thanks ) < br \" The lectures are good . but a bit speed . A in class working activity is a must one . So please take another hour in thursdays manmade . \" < br />We can hear your voice clearly and can understand the things you teach . Presentation slides also good source to refer .. if you can do more example questions within the classroom and it will help us to understand the principles well . < br \" Lecturers was well structured and well organized . It was easy to understand . Lecture slides and labs were also well organized . Lecturers were good . understandable . The lecture slides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP . Motivated to well . Would have been better if we discussed more about the solutions of coding exercisers . I think I learned a lot from the codes you write in the board . When i compare my codes with yours I can learn about my mistakes and good coding practices that I should follow . There for I think it would be great if we can discuss more examples in the class . madam explained the pop concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future . I satisfy about first 7 lectures . That way of teaching is really good for coming lectures too . lectures are very good . take good effort to make understand every student in the room . very helpful . I was able to obtain a clear picture about OOP and its concepts . \" lecture slides , explanations were very clear . < br />it&#039;s very good to letting ask questions and explain again with suitable examples . < br sometimes , some codes on white board were unclear at the back . < br />overall very good ! < br \" The lectures were good and clear . And the weren&#039;t too fast . Writing code was somewhat confusing because I didn&#039;t know java before . Actually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well . thank-you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EM_LahBQo9K",
        "outputId": "6b7f8126-4d54-4ee9-dbf8-fe1ab7b85830"
      },
      "source": [
        "# importing jaccard distance\n",
        "# and ngrams from nltk.util\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Downloading and importing\n",
        "# package 'words' from nltk corpus\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "\n",
        "\n",
        "correct_words = words.words()\n",
        "\n",
        "incorrect_words=['happpy', 'azmaing', 'intelliengt']\n",
        "\n",
        "for word in tokenized_student_feedback_content:\n",
        "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
        "                              set(ngrams(w, 2))),w)\n",
        "            for w in correct_words if w[0]==word[0]]\n",
        "    print(sorted(temp, key = lambda val:val[0])[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "happy\n",
            "amazing\n",
            "intelligent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvMgZ2zu8Yyl"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ-AyRjV4gdh",
        "outputId": "1ef9ecc1-ee11-4693-a93d-f19cf90903db"
      },
      "source": [
        "#required libraries for Lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Create WordNetLemmatizer object\n",
        "wordLemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizeText(text):\n",
        "    tokenized_words = word_tokenize(text)\n",
        "    lemmatized_word = [wordLemmatizer.lemmatize(word) for word in tokenized_words]\n",
        "    return \" \".join(lemmatized_word)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqNGQp8z419N"
      },
      "source": [
        "lemmatized_student_feedback = lemmatizeText(student_feedback_content) \n",
        "lemmatized_twitter_content = lemmatizeText(twitter_content)\n",
        "lemmatized_research_paper_content = lemmatizeText(research_paper_content)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO_XnASkL4sC",
        "outputId": "71111096-bcbd-4a34-bb61-01582c2eadf1"
      },
      "source": [
        "print(\"------------------------------------------------------------------------------- \")\n",
        "print(\"---------------   Twitter content before lemmatization  -----------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(twitter_content + '\\n')\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(\"----------------  Twitter content after lemmatization  ------------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(lemmatized_twitter_content)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------- \n",
            "----------  Twitter content before lemmatization  --------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada. #cdnpoli #LPC #CPCLDR��_ https://t.co/ZOZOSe1CqQ\n",
            "#immigration #integration #canada https://t.co/M5cKGyvV8F\n",
            "We want controlled immigration that contributes positively to the UK economy. Same as Australia &amp; Canada. https://t.co/99mYliuOes\n",
            "Is the new Manitoba immigration fee a head tax? https://t.co/LsG7C3vLe9\n",
            "Canada immigration profit influence modernistic delhi yet abhinav: XKofy https://t.co/becgusY2i6\n",
            "Canada Immigration Minister to ���Substantially Increase  Immigration Numbers https://t.co/nEFw30MRaa https://t.co/cyI867PZRV\n",
            "M��me les #USA=pays d'immigration par excellence CONTR��LE RIGOUREUSEMENT l'immigration et acc��s �� la #GreenCARD!��_ https://t.co/IHpVhW2BaG\n",
            "@Shawhelp what changes should be made to Canada's immigration laws due to the influx of immigration and violence?\n",
            "L��immigration irr��guli��re au Canada d��cortiqu��e en 5 questions https://t.co/f4utO5A7ZF\n",
            "L'immigration irr��guli��re au Canada d��cortiqu��e en 5 questions - https://t.co/UiBsEZOqas https://t.co/j77dEvjoiX https://t.co/XXDeIG7Dbu\n",
            "Will Media ask the Liberals if they actually have a solid plan for Canada _��_�_?? From my view -- immigration out of C��_ https://t.co/YAgwmZ8ECp\n",
            "Dan Murray of��Immigration Watch Canada is xenophobic racist fear-mongering liar #racism #canada #cdnpoli #hatecrime��_ https://t.co/kwZ3csvYxM\n",
            "Le Canada lance une vaste campagne d'immigration pour faire face �� son besoin de main d'��uvre https://t.co/kXdfMGTZzN\n",
            "L��#immigration irr��guli��re au #Canada d��cortiqu��e en 5��questions https://t.co/s3hu1OKKIG\n",
            "@Canadidly I've read the Immigration laws of Canada much stricter than the US\n",
            "Canada Immigration Website Traffic Surges And Crashes In Wake Of Trump #fasttraffic,#sitetraffic,#website,#traffic https://t.co/zRlJ26jnkC\n",
            "Mr Know-all of Canada Immigration https://t.co/wTQK4QDiKI\n",
            "Move to Canada @LadyMadonna___ Oh, immigration rules, you can't... https://t.co/5LIEVHO7A4\n",
            "#OnThisDay Annette Toft becomes Canada's 2 millionth immigrant since 1945. Do you know your family's immigration st��_ https://t.co/UvRuw8eR1b\n",
            ".@TheEconomist profiles Canada's open immigration policies &amp; how they contribute to our economic success:��_ https://t.co/4K84EE8Y63\n",
            "Hundreds may lose Canadian citizenship, resident status because of one corrupt immigration consultant https://t.co/x2IfO0EXI2\n",
            "Immigration for canada without india: an compassionate handle: deyFy\n",
            "\"#Jamaican #immigrants #Canada\n",
            "\n",
            "https://t.co/vcmfYGadR5\n",
            "\n",
            "#statistics #immigration\"\n",
            "Mexican visa lift expected to cost Canada $262M over a decade https://t.co/9i72fRhtij\n",
            "Are people still moving to #Canada ??? Oh that's right, they have real immigration laws and it's��_ https://t.co/0C5OBfmxLG\n",
            "Here are more details on the Richmond, B.C. Immigration Consultant Sunny Wang who was sentenced to 7 years in... https://t.co/YXH5W53srO\n",
            "I added a video to a @YouTube playlist https://t.co/CnEyWN40x3 Funny Talking of Haryanavi Jat with Canada Immigration Girl Agent\n",
            "Mexicans Can Now Travel Visa-Free To Canada https://t.co/Ec3XHORO2s https://t.co/RQRr5nebcG\n",
            "L��immigration irr��guli��re au Canada d��cortiqu��e en 5��questions https://t.co/DkpuKyWmaK\n",
            "@SweetnessShawnB Hes the POS that ramped up immigration for Canada, among other globalist policies.\n",
            "Canada lifted visa requirements to Mexico as of Dec 1, 2016. Thoughts? #visa #immigration\n",
            "@HuffingtonPost people Keep praising Canada and Canada has way stricter immigration laws then us they willl boot your liberal American ass\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "----------  Twitter content after lemmatization  ---------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada . # cdnpoli # LPC # CPCLDR��_ http : //t.co/ZOZOSe1CqQ # immigration # integration # canada http : //t.co/M5cKGyvV8F We want controlled immigration that contributes positively to the UK economy . Same a Australia & amp ; Canada . http : //t.co/99mYliuOes Is the new Manitoba immigration fee a head tax ? http : //t.co/LsG7C3vLe9 Canada immigration profit influence modernistic delhi yet abhinav : XKofy http : //t.co/becgusY2i6 Canada Immigration Minister to ���Substantially Increase Immigration Numbers http : //t.co/nEFw30MRaa http : //t.co/cyI867PZRV M��me le # USA=pays d'immigration par excellence CONTR��LE RIGOUREUSEMENT l'immigration et acc��s �� la # GreenCARD ! ��_ http : //t.co/IHpVhW2BaG @ Shawhelp what change should be made to Canada 's immigration law due to the influx of immigration and violence ? L��immigration irr��guli��re au Canada d��cortiqu��e en 5 question http : //t.co/f4utO5A7ZF L'immigration irr��guli��re au Canada d��cortiqu��e en 5 question - http : //t.co/UiBsEZOqas http : //t.co/j77dEvjoiX http : //t.co/XXDeIG7Dbu Will Media ask the Liberals if they actually have a solid plan for Canada _��_�_ ? ? From my view -- immigration out of C��_ http : //t.co/YAgwmZ8ECp Dan Murray of��Immigration Watch Canada is xenophobic racist fear-mongering liar # racism # canada # cdnpoli # hatecrime��_ http : //t.co/kwZ3csvYxM Le Canada lance une vaste campagne d'immigration pour faire face �� son besoin de main d'��uvre http : //t.co/kXdfMGTZzN L�� # immigration irr��guli��re au # Canada d��cortiqu��e en 5��questions http : //t.co/s3hu1OKKIG @ Canadidly I 've read the Immigration law of Canada much stricter than the US Canada Immigration Website Traffic Surges And Crashes In Wake Of Trump # fasttraffic , # sitetraffic , # website , # traffic http : //t.co/zRlJ26jnkC Mr Know-all of Canada Immigration http : //t.co/wTQK4QDiKI Move to Canada @ LadyMadonna___ Oh , immigration rule , you ca n't ... http : //t.co/5LIEVHO7A4 # OnThisDay Annette Toft becomes Canada 's 2 millionth immigrant since 1945 . Do you know your family 's immigration st��_ http : //t.co/UvRuw8eR1b . @ TheEconomist profile Canada 's open immigration policy & amp ; how they contribute to our economic success : ��_ http : //t.co/4K84EE8Y63 Hundreds may lose Canadian citizenship , resident status because of one corrupt immigration consultant http : //t.co/x2IfO0EXI2 Immigration for canada without india : an compassionate handle : deyFy '' # Jamaican # immigrant # Canada http : //t.co/vcmfYGadR5 # statistic # immigration '' Mexican visa lift expected to cost Canada $ 262M over a decade http : //t.co/9i72fRhtij Are people still moving to # Canada ? ? ? Oh that 's right , they have real immigration law and it's��_ http : //t.co/0C5OBfmxLG Here are more detail on the Richmond , B.C . Immigration Consultant Sunny Wang who wa sentenced to 7 year in ... http : //t.co/YXH5W53srO I added a video to a @ YouTube playlist http : //t.co/CnEyWN40x3 Funny Talking of Haryanavi Jat with Canada Immigration Girl Agent Mexicans Can Now Travel Visa-Free To Canada http : //t.co/Ec3XHORO2s http : //t.co/RQRr5nebcG L��immigration irr��guli��re au Canada d��cortiqu��e en 5��questions http : //t.co/DkpuKyWmaK @ SweetnessShawnB Hes the POS that ramped up immigration for Canada , among other globalist policy . Canada lifted visa requirement to Mexico a of Dec 1 , 2016 . Thoughts ? # visa # immigration @ HuffingtonPost people Keep praising Canada and Canada ha way stricter immigration law then u they willl boot your liberal American as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5e_8GGGQdcl"
      },
      "source": [
        "#required libraries for Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "porterStemmer = PorterStemmer()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L3pNmWdQ-qL"
      },
      "source": [
        "stemmed_student_feedback = porterStemmer.stem(student_feedback_content) \n",
        "stemmed_twitter_content = porterStemmer.stem(twitter_content)\n",
        "stemmed_research_paper_content = porterStemmer.stem(research_paper_content)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2lrwpj2Rdt9",
        "outputId": "da4dbb11-637d-4a41-fd06-7ebd961f99af"
      },
      "source": [
        "print(\"------------------------------------------------------------------------------- \")\n",
        "print(\"--------------  Research paper content before lemmatization  -----------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(research_paper_content + '\\n')\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(\"--------------  Research paper content after lemmatization  -------------------\")\n",
        "print(\"------------------------------------------------------------------------------- \\n\")\n",
        "print(stemmed_research_paper_content)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------- \n",
            "----------  Research paper content before lemmatization  --------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "Neural network models have shown their promising opportunities for multi-task\n",
            "learning, which focus on learning the shared layers to extract the common and\n",
            "task-invariant features. However, in most existing approaches, the extracted shared\n",
            "features are prone to be contaminated by task-specific features or the noise brought\n",
            "by other tasks. In this paper, we propose an adversarial multi-task learning framework,\n",
            "alleviating the shared and private latent feature spaces from interfering with\n",
            "each other. We conduct extensive experiments on 16 different text classification\n",
            "tasks, which demonstrates the benefits of our approach. Besides, we show that the\n",
            "shared knowledge learned by our proposed model can be regarded as off-the-shelf\n",
            "knowledge and easily transferred to new tasks.\n",
            "\n",
            "Multi-task learning is an effective approach to improve the performance of a single task with\n",
            "the help of other related tasks. Recently, neural-based models for multi-task learning have become\n",
            "very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural\n",
            "language processing (Collobert andWeston, 2008; Luong et al., 2015), since they provide a convenient\n",
            "way of combining information from multiple tasks.\n",
            "However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the\n",
            "features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1-(a), the general shared-private model introduces\n",
            "two feature spaces for any task: one is used to store task-dependent features, the other is\n",
            "used to capture shared features. The major limitation of this framework is that the shared feature\n",
            "space could contain some unnecessary task-specific features, while some sharable features\n",
            "could also be mixed in private space, suffering from feature redundancy.\n",
            "Taking the following two sentences as examples, which are extracted from two different sentiment\n",
            "classification tasks: Movie reviews and Baby products reviews.\n",
            "The infantile cart is simple and easy to use. This kind of humour is infantile and boring.\n",
            "The word �infantile� indicates negative sentiment in Movie task while it is neutral in Baby task.\n",
            "However, the general shared-private model could place the task-specific word �infantile� in a\n",
            "shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space\n",
            "could also be wasted by some unnecessary features.\n",
            "To address this problem, in this paper we propose an adversarial multi-task framework, in\n",
            "which the shared and private feature spaces are in herently disjoint by introducing orthogonality constraints.Specifically, we design a generic shared private learning framework to model the text sequence.\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "----------  Research paper content after lemmatization  ---------\n",
            "------------------------------------------------------------------------------- \n",
            "\n",
            "neural network models have shown their promising opportunities for multi-task\n",
            "learning, which focus on learning the shared layers to extract the common and\n",
            "task-invariant features. however, in most existing approaches, the extracted shared\n",
            "features are prone to be contaminated by task-specific features or the noise brought\n",
            "by other tasks. in this paper, we propose an adversarial multi-task learning framework,\n",
            "alleviating the shared and private latent feature spaces from interfering with\n",
            "each other. we conduct extensive experiments on 16 different text classification\n",
            "tasks, which demonstrates the benefits of our approach. besides, we show that the\n",
            "shared knowledge learned by our proposed model can be regarded as off-the-shelf\n",
            "knowledge and easily transferred to new tasks.\n",
            "\n",
            "multi-task learning is an effective approach to improve the performance of a single task with\n",
            "the help of other related tasks. recently, neural-based models for multi-task learning have become\n",
            "very popular, ranging from computer vision (misra et al., 2016; zhang et al., 2014) to natural\n",
            "language processing (collobert andweston, 2008; luong et al., 2015), since they provide a convenient\n",
            "way of combining information from multiple tasks.\n",
            "however, most existing work on multi-task learning (liu et al., 2016c,b) attempts to divide the\n",
            "features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. as shown in figure 1-(a), the general shared-private model introduces\n",
            "two feature spaces for any task: one is used to store task-dependent features, the other is\n",
            "used to capture shared features. the major limitation of this framework is that the shared feature\n",
            "space could contain some unnecessary task-specific features, while some sharable features\n",
            "could also be mixed in private space, suffering from feature redundancy.\n",
            "taking the following two sentences as examples, which are extracted from two different sentiment\n",
            "classification tasks: movie reviews and baby products reviews.\n",
            "the infantile cart is simple and easy to use. this kind of humour is infantile and boring.\n",
            "the word �infantile� indicates negative sentiment in movie task while it is neutral in baby task.\n",
            "however, the general shared-private model could place the task-specific word �infantile� in a\n",
            "shared space, leaving potential hazards for other tasks. additionally, the capacity of shared space\n",
            "could also be wasted by some unnecessary features.\n",
            "to address this problem, in this paper we propose an adversarial multi-task framework, in\n",
            "which the shared and private feature spaces are in herently disjoint by introducing orthogonality constraints.specifically, we design a generic shared private learning framework to model the text sequence.\n"
          ]
        }
      ]
    }
  ]
}