{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe341O/Jhrifnx8QgRkIwl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaveeshBaddage/DataScienceImpl/blob/organize/Text_Classification-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACl75eG3Uog9"
      },
      "source": [
        "**Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TurV6C0_EF08",
        "outputId": "c5a6a717-db97-4b7b-dd34-d9b952e18809"
      },
      "source": [
        "#required libraries to import dataset\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "#required library for preprocessing\n",
        "import re\n",
        "\n",
        "#required libraries to remove stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#required libraries for Lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#required libraries for POS Tagging\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#required libraries for Named Entity Recognizion\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "#required libraries for headword extraction\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#required libraries for TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#required libraries for Count Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#required libraries for finalize features\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from scipy.sparse import coo_matrix, csr_matrix, hstack\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#required libraries for SVM Model\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "#required libraries for evaluate the model\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#required libraries for fastText\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText\n",
        "!pip install fastText\n",
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# required libraries for LSTM input data preparation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# required libraries for LSTM \n",
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
        "from tensorflow.keras import Sequential\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.22 MiB | 32.39 MiB/s, done.\n",
            "Resolving deltas: 100% (2417/2417), done.\n",
            "Collecting fastText\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fastText) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastText) (1.19.5)\n",
            "Building wheels for collected packages: fastText\n",
            "  Building wheel for fastText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastText: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3090461 sha256=26f28a36a507ef9df0e89069c422fba7725bead90382acaf934f0901714e4807\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fastText\n",
            "Installing collected packages: pybind11, fastText\n",
            "Successfully installed fastText-0.9.2 pybind11-2.7.1\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEubDPuD9WwS"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
        "from tensorflow.keras import Sequential\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxKIitA3UnHX"
      },
      "source": [
        "**Import Travel Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bobi7o9IjAe"
      },
      "source": [
        "excel_file_url = 'https://github.com/KaveeshBaddage/DataScienceImpl/blob/main/NLP/Text%20Classification/Data/5000TravelQuestionsDataset.xlsx?raw=true'\n",
        "\n",
        "columns = ['questions', 'coarse_classes', 'fine_classes']\n",
        "dataset = pd.read_excel(excel_file_url, header=None, names=columns)\n",
        "dataset['questions'].dropna(inplace=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fhhx0X7J7jB",
        "outputId": "15306214-85ef-42f7-d5d1-8feeecbf6374"
      },
      "source": [
        "print(dataset.info())\n",
        "print(dataset['coarse_classes'].unique())\n",
        "print(len(dataset['fine_classes'].unique()))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   questions       5000 non-null   object\n",
            " 1   coarse_classes  5000 non-null   object\n",
            " 2   fine_classes    5000 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 117.3+ KB\n",
            "None\n",
            "['TTD' 'TGU' 'ACM' 'TRS' 'WTH' 'FOD' 'ENT' 'TGU\\n' 'TTD\\n' '\\nENT']\n",
            "79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fea5U2t5VBKC"
      },
      "source": [
        "# **Preprocess the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoC8PLfB89cm"
      },
      "source": [
        "def preprocessText(text):\n",
        "\n",
        "\n",
        "  #Converting a word to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  #remove HTML tags\n",
        "  html_tags = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(html_tags, '', text)\n",
        "\n",
        "  #replace punctuation marks by space\n",
        "  regular_punct = list(string.punctuation)\n",
        "  for punc in regular_punct:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, ' ')\n",
        "  \n",
        "  # replace special characters by spaces\n",
        "  filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "  translate_dict = dict((c, \" \") for c in filters)\n",
        "  translate_map = str.maketrans(translate_dict)\n",
        "  text = text.translate(translate_map)\n",
        "\n",
        "  return text;"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIiSrshR8yic"
      },
      "source": [
        "# Remove HTML tags, capitalization, punctuation marks from the questions\n",
        "dataset['preprocessed_questions'] = [preprocessText(question) for question in dataset['questions']]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV_ep13fHP_d",
        "outputId": "c05cf21a-7c21-45ac-dc7d-f87f5dec306b"
      },
      "source": [
        "#testing pre processing results\n",
        "print('Sample question before preprocessing - ',dataset['questions'][1])\n",
        "print('Same question after preprocessing - ',dataset['preprocessed_questions'][1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample question before preprocessing -  What are the companies which organize shark feeding events for scuba divers?\n",
            "Same question after preprocessing -  what are the companies which organize shark feeding events for scuba divers \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnDDNSd6I8V1",
        "outputId": "ba6faaa2-c277-4293-d5e5-25cf78a7221a"
      },
      "source": [
        "\n",
        "def removeStopwords(text):\n",
        "    tokenized_words = word_tokenize(text)\n",
        "    return \" \".join([tokenized_word for index, tokenized_word in enumerate(tokenized_words) if (index < 1) or (tokenized_word not in stop_words)])\n",
        "\n",
        "\n",
        "dataset['stopwordsRemoved_questions'] = [removeStopwords(question) for question in dataset['preprocessed_questions']]\n",
        "print('Sample question before removing stop words - ',dataset['questions'][1])\n",
        "print('Same question after removing its stop words - ',dataset['stopwordsRemoved_questions'][1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample question before removing stop words -  What are the companies which organize shark feeding events for scuba divers?\n",
            "Same question after removing its stop words -  what companies organize shark feeding events scuba divers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74daIPzcYP4X"
      },
      "source": [
        "# **Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SDmJgn4cgcI"
      },
      "source": [
        "**Lemmatization the questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTgCklmscauV",
        "outputId": "e3772068-c09e-4feb-c4ed-ab5e98541cb6"
      },
      "source": [
        " # Create WordNetLemmatizer object\n",
        "wordLemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizeText(text):\n",
        "    tokenized_words = word_tokenize(text)\n",
        "    lemmatized_word = [wordLemmatizer.lemmatize(word) for word in tokenized_words]\n",
        "    return \" \".join(lemmatized_word)\n",
        "\n",
        "\n",
        "dataset['lemmatized_questions'] = [lemmatizeText(question) for question in dataset['stopwordsRemoved_questions']]\n",
        "print('Sample question before lemmatization - ',dataset['questions'][1])\n",
        "print('Same question after lemmatization - ',dataset['lemmatized_questions'][1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample question before lemmatization -  What are the companies which organize shark feeding events for scuba divers?\n",
            "Same question after lemmatization -  what company organize shark feeding event scuba diver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nT6u9SPfA0m"
      },
      "source": [
        "**POS Tagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKhBD0HG-eKR"
      },
      "source": [
        "POS Tagging using Hidden Markov Model Tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQjj09RTfE6e",
        "outputId": "aeb85cc3-df3b-4113-b66b-95c024d66316"
      },
      "source": [
        "from nltk.tag import HiddenMarkovModelTagger\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Import the toolkit and tags\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Train data - pretagged\n",
        "train_data = treebank.tagged_sents()[:3000]\n",
        "\n",
        "# Setup a trainer with default(None) values\n",
        "# And train with the data\n",
        "tagger = HiddenMarkovModelTagger.train(train_data)\n",
        "\n",
        "def hMMposTaggedText(text):\n",
        "    tokenized_words = word_tokenize(text)\n",
        "    lst = [ r[1] for r in tagger.tag(tokenized_words)] \n",
        "    return ' '.join(lst)\n",
        "    # posTagged_word =  [tagging.tag(word) for word in tokenized_words]\n",
        "    # return ' '.join(posTagged_word)\n",
        "\n",
        "dataset['hMMposTagged_questions'] = [hMMposTaggedText(question) for question in dataset['questions']]\n",
        "print('Sample question before HMM POS Tagging - ',dataset['questions'][1])\n",
        "print('Same question after HMM POS Tagging - ',dataset['hMMposTagged_questions'][1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "Sample question before HMM POS Tagging -  What are the companies which organize shark feeding events for scuba divers?\n",
            "Same question after HMM POS Tagging -  PRP VBP DT NNS WDT -NONE- TO VB NNS IN DT NN .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJmh0iSUHyhZ"
      },
      "source": [
        "POS Tagging using NLTK pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IOOFT-Z925x",
        "outputId": "a6329445-006d-4fb5-a38f-4c268efe3de1"
      },
      "source": [
        "def posTaggedText(text):\n",
        "    tokenized_words = nltk.word_tokenize(text)\n",
        "    lst = [ r[1] for r in pos_tag(tokenized_words)] \n",
        "    return ' '.join(lst)\n",
        "\n",
        "dataset['posTagged_questions'] = [posTaggedText(question) for question in dataset['questions']]\n",
        "print('Sample question before POS Tagging - ',dataset['questions'][1])\n",
        "print('Same question after POS Tagging - ',dataset['posTagged_questions'][1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample question before POS Tagging -  What are the companies which organize shark feeding events for scuba divers?\n",
            "Same question after POS Tagging -  WP VBP DT NNS WDT VBP NN NN NNS IN NN NNS .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LySWCLmsMt3t"
      },
      "source": [
        "**Named Entity Recognition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XYZG8ZNMvef",
        "outputId": "c6f48392-47dc-427a-b654-0fff6cc8917c"
      },
      "source": [
        "\n",
        "def getContinuousChunks(text):\n",
        "    #chunks the text into a Tree\n",
        "    chunkedText = ne_chunk(pos_tag(word_tokenize(text)))\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "    prev = None\n",
        "\n",
        "    for i in chunkedText:\n",
        "        if type(i) == Tree:\n",
        "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
        "        elif current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    if continuous_chunk:\n",
        "        named_entity = \" \".join(current_chunk)\n",
        "        if named_entity not in continuous_chunk:\n",
        "            continuous_chunk.append(named_entity)\n",
        "    \n",
        "    def removeNull(continuousChunk):\n",
        "        if '' in continuousChunk:\n",
        "            continuousChunk.remove('')\n",
        "        return continuousChunk\n",
        "\n",
        "    newChunk = removeNull(continuous_chunk)\n",
        "    return ' '.join(newChunk)\n",
        "\n",
        "\n",
        "#call the function to get named entities\n",
        "dataset['named_entity_questions'] = [getContinuousChunks(question) for question in dataset['questions']]\n",
        "print(dataset['named_entity_questions'][1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08OL0Yn2iKfp"
      },
      "source": [
        "**Selecting as headwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxHtVRXpeHi0"
      },
      "source": [
        "# nsubjpass = A passive nominal subject is a noun phrase which is the syntactic subject of a passive clause.\n",
        "# nsubj = a nominal which is the syntactic subject and the proto-agent of a clause\n",
        "\n",
        "def head_word_tokenizer(text):\n",
        "    head_words = []\n",
        "    for token in nlp(text):\n",
        "        if token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\":\n",
        "            head_words.append(token.text)\n",
        "    return head_words"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IT4cpwYkOJf"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Convert the text documents to a matrix of token counts\n",
        "head_words_vectorizer = CountVectorizer(tokenizer = head_word_tokenizer,max_features=100,stop_words=stopwords.words('english'))\n",
        "head_words_vector = head_words_vectorizer.fit_transform(dataset[\"questions\"].values).toarray()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_3MHEdggzAC"
      },
      "source": [
        "**Create TF-IDF Vector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv6YTZ2AdT68",
        "outputId": "9e076d13-c3c8-44fe-9957-69d97b5ed96b"
      },
      "source": [
        "\n",
        "# build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "tfidf = vectorizer.fit_transform(dataset['lemmatized_questions'])\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "print(tfidf.shape)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00pm', '10', '100', '1000', '10am', '11', '12', '1200', '124', '124th', '13', '14', '148th', '14hr', '15', '150', '16', '17', '17km', '18', '180', '19', '1st', '20', '200', '2006', '2016', '2017', '2018', '21st', '22', '23', '24', '240v', '25', '25th', '2hr', '2nd', '2wd', '30', '300', '30am', '31st', '32', '3200', '37', '3day', '3g', '3ltr', '3rd', '40', '400', '400mm', '400s', '40th', '45', '46', '4g', '4pax', '4seasons', '4wd', '4x4', '50', '500', '5am', '60nzd', '60th', '65', '68', '6km', '6pm', '6th', '700', '71', '73', '7pm', 'a1', 'a2', 'a380', 'abel', 'able', 'aboard', 'abouts', 'abq', 'abseiling', 'absolutely', 'abu', 'ac', 'academy', 'acapulco', 'accept', 'acceptable', 'accepted', 'access', 'accessed', 'accessible', 'accessory', 'accomdate', 'accommodate', 'accommodation', 'accomodation', 'account', 'accurate', 'ace', 'achievable', 'achim', 'aci', 'acitrezza', 'acquire', 'across', 'acrothea', 'active', 'activity', 'actual', 'actually', 'adagio', 'adapter', 'add', 'added', 'addo', 'address', 'adeje', 'adelaide', 'adidas', 'adina', 'adler', 'admission', 'ado', 'adobe', 'adopt', 'adr', 'adult', 'advance', 'advantage', 'adventure', 'adventurous', 'advice', 'advisable', 'advise', 'advised', 'aed', 'aerial', 'af', 'affect', 'affiliated', 'affordable', 'africa', 'african', 'afrique', 'afternoon', 'afterwards', 'agadir', 'age', 'agency', 'agent', 'agio', 'agli', 'agonda', 'agosta', 'agra', 'agrigento', 'ai', 'aida', 'air', 'airbnb', 'airboat', 'airbus', 'aire', 'airfare', 'airlie', 'airline', 'airpark', 'airport', 'aitken', 'akaroa', 'akhdar', 'akko', 'akti', 'akumal', 'al', 'ala', 'alacarte', 'alam', 'alamein', 'alamo', 'albans', 'albany', 'albatross', 'alboretti', 'albq', 'albuquerque', 'alcantara', 'alcatraz', 'alcazares', 'alcohol', 'alcoholic', 'alcudia', 'alesund', 'alexander', 'alexandria', 'alhambra', 'ali', 'alicante', 'alike', 'alilaguna', 'alive', 'aliwal', 'aljibes', 'alligator', 'allocate', 'allow', 'allowable', 'allowance', 'allowed', 'allowing', 'allows', 'alone', 'along', 'alotau', 'alp', 'alpine', 'already', 'also', 'alternative', 'altitude', 'alua', 'always', 'am', 'amalfi', 'amazing', 'amazon', 'amazonadventures', 'amb', 'ambar', 'amber', 'amboise', 'amboseli', 'ambre', 'amenity', 'america', 'american', 'amerika', 'amex', 'amigo', 'amman', 'ammenities', 'among', 'amount', 'amoxicillin', 'ampijoroa', 'ample', 'amr', 'amstel', 'amsterdam', 'amtrak', 'amusement', 'amway', 'anantara', 'anau', 'ancient', 'ancona', 'andaman', 'andaura', 'android', 'andros', 'angel', 'angela', 'angeles', 'angkor', 'anglesey', 'animal', 'anissaras', 'anna', 'annae', 'annapurna', 'anne', 'annecy', 'annette', 'anniversary', 'announced', 'annual', 'anonymous', 'another', 'anse', 'anselmo', 'antananarivo', 'antelope', 'anthony', 'anti', 'antibes', 'antica', 'antimalarial', 'antique', 'antonio', 'antwerp', 'anuradhapura', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'ao', 'apart', 'aparthotel', 'apartment', 'apex', 'apia', 'app', 'apple', 'applicable', 'application', 'apply', 'appointment', 'appreciate', 'appropriate', 'approved', 'approx', 'approximate', 'approximately', 'apps', 'april', 'apts', 'aqaba', 'aqua', 'aquarium', 'aquarius', 'arab', 'arabia', 'arancio', 'aransas', 'arboretum', 'arc', 'arcade', 'archeological', 'architecture', 'are', 'area', 'arenal', 'argentina', 'argentinian', 'argo', 'argonne', 'ari', 'arica', 'arillas', 'arizona', 'armory', 'around', 'arran', 'arrange', 'arranging', 'arrival', 'arrive', 'arriving', 'arrowtown', 'art', 'artemide', 'arthur', 'artist', 'arusha', 'asakusa', 'ashdod', 'asia', 'asian', 'aside', 'asilah', 'ask', 'asked', 'assisi', 'assistance', 'assos', 'aswan', 'at', 'atacama', 'atheist', 'athens', 'atlantis', 'atm', 'atmosphere', 'atoll', 'atrani', 'attarctions', 'attend', 'attire', 'attraction', 'atv', 'auckland', 'aud', 'audience', 'auds', 'aug', 'august', 'augustin', 'augustine', 'augusto', 'auschwitz', 'aussie', 'austin', 'australia', 'australian', 'austria', 'authentic', 'authority', 'auto', 'automatic', 'automatically', 'automobile', 'autumn', 'aux', 'availability', 'available', 'avalon', 'avani', 'avellanas', 'aventine', 'average', 'avg', 'aviv', 'avoid', 'avoiding', 'aware', 'away', 'awesome', 'az', 'azul', 'ba', 'baati', 'babusar', 'baby', 'babysitting', 'bachata', 'bachelorette', 'back', 'backpacker', 'backpacking', 'bacon', 'bad', 'badaling', 'baden', 'badgingarra', 'bag', 'bagel', 'baggage', 'bagues', 'bahamas', 'bahia', 'baht', 'bai', 'bakery', 'bal', 'balcony', 'balholm', 'bali', 'ballet', 'balloon', 'ballroom', 'baltistan', 'baltour', 'baluchistan', 'bancomat', 'band', 'bangkok', 'bangladeshi', 'bank', 'banning', 'banteay', 'bao', 'baptised', 'baptism', 'baptismal', 'bar', 'bara', 'barajas', 'barbara', 'barbarons', 'barcelona', 'bardolino', 'bargain', 'bari', 'bariloche', 'barmouth', 'baroque', 'barra', 'barraquito', 'barrier', 'bart', 'base', 'based', 'basel', 'bashful', 'bashing', 'basic', 'basilica', 'basis', 'bass', 'bath', 'bathroom', 'batok', 'battery', 'batticaloa', 'battuta', 'bauerfield', 'bavaria', 'bavaro', 'bay', 'bayahibe', 'bayern', 'bayeux', 'bazar', 'bbq', 'bbqs', 'beach', 'beachfront', 'beachwalk', 'beam', 'bean', 'bear', 'beatitude', 'beau', 'beaumaris', 'beautiful', 'become', 'bed', 'bedroom', 'beef', 'been', 'beer', 'beersheba', 'beforehand', 'begin', 'beginning', 'bei', 'beijing', 'belem', 'belfast', 'believing', 'bell', 'bellagio', 'belle', 'bellini', 'belvédère', 'ben', 'benchen', 'benidorm', 'benoa', 'berg', 'bergamo', 'bergen', 'bergerac', 'berjaya', 'berlin', 'bermuda', 'bernard', 'bernina', 'berth', 'beside', 'besides', 'best', 'bet', 'bethlehem', 'better', 'betty', 'beware', 'beyond', 'bhurban', 'bicycle', 'big', 'bigger', 'biggest', 'bike', 'biking', 'bikini', 'billing', 'bimini', 'bin', 'bio', 'bioluminescent', 'bird', 'birding', 'birgunj', 'birmingham', 'birthday', 'bison', 'bistro', 'bite', 'biting', 'bitter', 'black', 'blackpool', 'blanc', 'blanca', 'bleach', 'blessing', 'blocked', 'blocking', 'blood', 'bloom', 'blow', 'blue', 'bnl', 'board', 'boat', 'boating', 'boatyard', 'bob', 'bobcat', 'boca', 'bodo', 'bohara', 'bolgar', 'bolgheri', 'bologna', 'bolton', 'bona', 'bond', 'bonding', 'bonfire', 'bonifacio', 'bonito', 'boogie', 'book', 'booked', 'booking', 'bookmefiji', 'booster', 'boot', 'booth', 'booty', 'booze', 'bora', 'border', 'boring', 'borth', 'bossa', 'boston', 'botanical', 'botswana', 'botswanian', 'bottle', 'bottled', 'bottom', 'boudha', 'bought', 'boulangerie', 'boulevarde', 'bourdain', 'boutique', 'box', 'boy', 'bracelet', 'branch', 'brand', 'brava', 'brazil', 'brazilian', 'bread', 'break', 'breaker', 'breakfast', 'breaking', 'breathless', 'brew', 'brewery', 'brick', 'bridal', 'bridge', 'bright', 'brindisi', 'bring', 'bringing', 'brisbane', 'bristol', 'british', 'brook', 'broom', 'broth', 'brought', 'bruges', 'brugge', 'brunch', 'bruny', 'brushed', 'brussels', 'bts', 'bucerias', 'budapest', 'buddha', 'budget', 'buena', 'buenos', 'buffalo', 'buffet', 'bug', 'buggy', 'building', 'built', 'bukhansan', 'bukit', 'bulgaria', 'bullfighting', 'bumper', 'bumpy', 'bun', 'bunch', 'bundoran', 'bungalow', 'bure', 'bureau', 'burg', 'burger', 'burgundy', 'burj', 'bus', 'busan', 'business', 'busselton', 'busy', 'butchart', 'butcher', 'butler', 'buy', 'buying', 'buzios', 'bv', 'bw', 'byblos', 'ca', 'cab', 'cabana', 'cabin', 'cable', 'cabo', 'cachi', 'cadillac', 'caen', 'cafayate', 'cafe', 'café', 'cage', 'cagliari', 'cairn', 'cairo', 'cake', 'cal', 'cala', 'calabria', 'calafate', 'calama', 'calendar', 'caleta', 'california', 'call', 'called', 'calm', 'cama', 'cambodia', 'cambodian', 'camel', 'cameo', 'camera', 'camp', 'campervan', 'campervans', 'campground', 'camping', 'campo', 'campsite', 'can', 'cana', 'canada', 'canal', 'canaria', 'canaveral', 'canberra', 'cancel', 'canceling', 'cancellation', 'cancha', 'cancun', 'candy', 'canfali', 'canggu', 'cango', 'canister', 'canned', 'cannes', 'canoe', 'canon', 'canopy', 'canton', 'cantt', 'canyon', 'cap', 'cape', 'capetown', 'capital', 'cappuccino', 'capri', 'capture', 'car', 'caravan', 'carcassonne', 'card', 'caribbean', 'carihuela', 'carling', 'carlisle', 'carlos', 'carmel', 'carmen', 'carnarvon', 'carnforth', 'carnival', 'carolina', 'carrera', 'carriage', 'carrier', 'carry', 'cart', 'carte', 'carton', 'casa', 'casablanca', 'case', 'cash', 'cashless', 'casino', 'casitas', 'cask', 'casoli', 'cassone', 'cast', 'castello', 'castillo', 'castle', 'casual', 'cat', 'catalina', 'catania', 'catch', 'caterer', 'catering', 'caters', 'catete', 'cathedral', 'catherine', 'catholic', 'cause', 'caused', 'cave', 'cayan', 'cayo', 'cbd', 'cbs', 'cd', 'cdg', 'cedar', 'cefalu', 'celebrate', 'celebrated', 'celebration', 'cell', 'cellphone', 'centara', 'center', 'central', 'centrale', 'centre', 'centro', 'ceramic', 'cereal', 'ceremony', 'cerro', 'certain', 'certificate', 'ch', 'chaika', 'chair', 'chairlift', 'chalten', 'chamber', 'champ', 'champagne', 'chance', 'chandler', 'changdeokgung', 'change', 'changer', 'changi', 'changing', 'chania', 'channel', 'chapelle', 'character', 'charentes', 'charge', 'charged', 'charger', 'charlie', 'chart', 'charter', 'chau', 'cheap', 'cheaper', 'cheapest', 'check', 'checking', 'checkpoint', 'checkup', 'cheetah', 'chef', 'cheongpyeong', 'cheque', 'cherry', 'chester', 'chez', 'chi', 'chianti', 'child', 'childcare', 'chile', 'chill', 'chilled', 'chilly', 'china', 'chinatown', 'chinese', 'chinotimba', 'chintpurni', 'chip', 'chitral', 'cho', 'chocolate', 'choice', 'cholera', 'choose', 'choosing', 'christchurch', 'christian', 'christmas', 'chrysanthemum', 'chungju', 'chungking', 'church', 'chuseok', 'cialis', 'cicerto', 'cider', 'cielo', 'cigar', 'cigarette', 'ciguatera', 'cinematographer', 'cinghiale', 'cinque', 'cinqueterre', 'circolo', 'circuit', 'citadines', 'citizen', 'citrique', 'city', 'ciutadella', 'civil', 'civitavecchia', 'clam', 'clarion', 'clark', 'clarke', 'class', 'classic', 'classical', 'classy', 'clean', 'cleaner', 'cleanest', 'cleaning', 'clear', 'clearing', 'clearwater', 'cliff', 'climate', 'climb', 'climbing', 'clinic', 'clink', 'clonakilty', 'close', 'closed', 'closer', 'closest', 'closing', 'closure', 'clothes', 'clothing', 'cloud', 'club', 'clubbing', 'clyde', 'co', 'coach', 'coast', 'coastal', 'coastland', 'cockatoo', 'cocktail', 'coco', 'cocoa', 'coconut', 'code', 'coffee', 'coil', 'coin', 'colchester', 'cold', 'cole', 'collect', 'collection', 'collioure', 'cologne', 'colombo', 'colon', 'colonia', 'colorado', 'colorful', 'colosseum', 'colour', 'colourful', 'colville', 'com', 'come', 'comedy', 'comfort', 'comfortable', 'coming', 'commence', 'comment', 'commonly', 'communicate', 'community', 'commute', 'como', 'company', 'compared', 'complaint', 'complete', 'completely', 'complex', 'complimentary', 'comprehensive', 'compressed', 'con', 'concern', 'concert', 'condado', 'condition', 'conditioning', 'condo', 'conduit', 'confirm', 'confusion', 'congonhas', 'connected', 'connection', 'conquistador', 'conservation', 'consider', 'consideration', 'considering', 'consist', 'constant', 'contact', 'contain', 'control', 'controlled', 'convenience', 'convenient', 'convert', 'converter', 'convertible', 'cook', 'cooking', 'cookware', 'cool', 'coolest', 'copacabana', 'copenhagen', 'coral', 'coraya', 'corfu', 'cork', 'corniche', 'coromandel', 'corporate', 'corralejo', 'correct', 'cortes', 'cortina', 'cost', 'costa', 'costaadeje', 'costume', 'cotswold', 'cottage', 'could', 'counter', 'country', 'countryside', 'couple', 'coupon', 'course', 'courtesy', 'cove', 'covenient', 'coventry', 'cover', 'coverage', 'covered', 'covering', 'cph', 'cr', 'crab', 'craft', 'crawl', 'crazy', 'cream', 'create', 'creature', 'credit', 'creek', 'crete', 'crime', 'cristianos', 'crocodile', 'crocoloco', 'cromer', 'cross', 'crossing', 'crowd', 'crowded', 'crown', 'cruise', 'cruising', 'crux', 'crystal', 'cuba', 'cuddle', 'cuesta', 'cuisine', 'culebra', 'culinary', 'cultural', 'culture', 'cup', 'curling', 'currency', 'current', 'cushion', 'custard', 'custom', 'customer', 'cut', 'cutter', 'cycle', 'cyclone', 'cyprus', 'córdoba', 'daegu', 'dagger', 'dahab', 'dai', 'daily', 'daiwa', 'dalhousie', 'dallas', 'dam', 'damage', 'damaged', 'dambulla', 'danang', 'dance', 'dancing', 'danger', 'dangerous', 'daniel', 'danish', 'dar', 'darley', 'darling', 'darwin', 'dashain', 'data', 'date', 'dating', 'daughter', 'day', 'daytrip', 'dc', 'de', 'dead', 'deal', 'dealing', 'death', 'debit', 'debod', 'dec', 'december', 'decent', 'decided', 'deck', 'declaration', 'deep', 'definitely', 'dei', 'deira', 'del', 'delay', 'delhi', 'delicious', 'deliverance', 'delivers', 'delivery', 'delle', 'delta', 'deluxe', 'demand', 'denarau', 'dengue', 'denmark', 'dentist', 'deodoro', 'departing', 'departure', 'deposit', 'dept', 'der', 'derech', 'dermatologist', 'derry', 'descent', 'describe', 'desert', 'deserted', 'designated', 'designer', 'dessert', 'destination', 'detail', 'detour', 'devil', 'devon', 'devonport', 'dfw', 'dhabi', 'dharamshala', 'di', 'dialysis', 'diamond', 'diani', 'did', 'diego', 'diesel', 'difference', 'different', 'difficult', 'digue', 'dim', 'dine', 'diner', 'dingle', 'dining', 'dinner', 'dino', 'direct', 'direction', 'directly', 'dirham', 'disabled', 'disadvantage', 'discomfort', 'discount', 'discounted', 'discover', 'discovery', 'discrimination', 'dish', 'disney', 'disneyland', 'display', 'distance', 'distillery', 'distortion', 'district', 'dive', 'diver', 'diving', 'dj', 'dji', 'dlp', 'do', 'doable', 'dock', 'document', 'documentation', 'doe', 'dog', 'doha', 'dollar', 'dolphin', 'domestic', 'domingo', 'dominican', 'domodedovo', 'done', 'dong', 'donts', 'dorado', 'dot', 'douglas', 'download', 'downpour', 'downside', 'downtown', 'drakensberg', 'drawback', 'dream', 'dreamworld', 'dress', 'dresscode', 'dried', 'drink', 'drinking', 'drive', 'driver', 'driving', 'drone', 'drop', 'dropping', 'dry', 'dryer', 'dslr', 'dsquared2', 'du', 'dua', 'dubai', 'dublin', 'dubrovnik', 'due', 'dunas', 'dune', 'dunedin', 'dunkirk', 'dunshaughlin', 'duomo', 'duqm', 'durban', 'durham', 'during', 'dusseldorf', 'dust', 'duty', 'dwelling', 'early', 'ease', 'easier', 'easiest', 'easily', 'east', 'easter', 'eastern', 'easy', 'easyjet', 'eat', 'eatery', 'eating', 'ebc', 'eco', 'ecochile', 'economical', 'economy', 'ect', 'edinburgh', 'edith', 'edmonton', 'efate', 'effect', 'effective', 'efficient', 'eg', 'egeskov', 'egg', 'egged', 'egp', 'egypt', 'egyptian', 'eid', 'eiffel', 'eilat', 'eilet', 'eindhoven', 'either', 'el', 'elba', 'elder', 'electric', 'electrical', 'electronic', 'elephant', 'eleuthera', 'elevation', 'elgin', 'elia', 'elizabeth', 'ellaidhoo', 'elli', 'else', 'elsewhere', 'elysees', 'email', 'embudu', 'emh', 'emirate', 'empresa', 'en', 'enchanted', 'enchantment', 'encounter', 'end', 'endless', 'energy', 'england', 'english', 'enjoy', 'enough', 'enroute', 'enter', 'entering', 'entertainer', 'entertainment', 'enthusiast', 'entire', 'entirely', 'entrance', 'entry', 'environment', 'ephelia', 'equally', 'equipment', 'erice', 'escarcega', 'escort', 'especially', 'esperanza', 'espn', 'essaouira', 'esta', 'estancias', 'estepona', 'estimate', 'estimated', 'eta', 'etc', 'etihad', 'etiquette', 'etna', 'euro', 'euronet', 'europcar', 'europe', 'european', 'eurostar', 'eurowings', 'eve', 'even', 'evening', 'event', 'ever', 'everest', 'everglades', 'every', 'everyone', 'everything', 'evesham', 'evita', 'ex', 'exact', 'exactly', 'excellent', 'excelsior', 'except', 'exchange', 'exchanging', 'exciting', 'exclusive', 'excursion', 'executive', 'exeter', 'exhibition', 'existing', 'exit', 'exodus', 'expanse', 'expect', 'expected', 'expecting', 'expedition', 'expense', 'expensive', 'experience', 'experienced', 'expires', 'explain', 'explore', 'exploring', 'expo', 'express', 'extra', 'exuma', 'eye', 'fab', 'fabric', 'face', 'facial', 'facilitate', 'facility', 'factor', 'factory', 'fairfx', 'fairground', 'fairy', 'faito', 'fajardo', 'fakarava', 'faleolo', 'fales', 'faliraki', 'fall', 'family', 'famous', 'fan', 'fancy', 'fano', 'fantasia', 'far', 'fare', 'farm', 'farmer', 'farther', 'fashion', 'fashioned', 'fast', 'faster', 'fastest', 'fat', 'fauske', 'favela', 'favorite', 'fayrooz', 'fbg', 'fe', 'feasible', 'feb', 'february', 'fed', 'fee', 'feedback', 'feeding', 'feeling', 'felanitx', 'female', 'fender', 'fenix', 'ferrari', 'ferry', 'festa', 'festival', 'festive', 'festivity', 'fever', 'fewer', 'fi', 'fiafia', 'fiancee', 'fiancé', 'fiesta', 'fight', 'figure', 'fihalhohi', 'fiji', 'fijian', 'file', 'fill', 'film', 'filmed', 'fin', 'final', 'financial', 'find', 'findhorn', 'finding', 'fine', 'finer', 'finish', 'fiordland', 'fire', 'fireball', 'firefly', 'fireplace', 'firesky', 'firework', 'firm', 'first', 'fish', 'fisherman', 'fishing', 'fit', 'fitness', 'five', 'fixed', 'fj', 'fjord', 'fl', 'flac', 'flagstaff', 'flamenco', 'flamingo', 'flash', 'flat', 'flea', 'flic', 'flight', 'flingers', 'flipper', 'flixbus', 'fll', 'floating', 'flock', 'floor', 'florence', 'florida', 'floridian', 'florist', 'flower', 'flown', 'fly', 'flying', 'foix', 'foliage', 'folk', 'folkemuseum', 'following', 'food', 'foot', 'football', 'for', 'forbidden', 'forcat', 'forecast', 'forello', 'forest', 'forfjord', 'form', 'formal', 'formula', 'fornillo', 'foro', 'fort', 'fortaleza', 'found', 'fountain', 'four', 'fox', 'foz', 'fp', 'fragrance', 'fran', 'france', 'francesca', 'francisco', 'frank', 'frankfurt', 'franz', 'frasassi', 'frederick', 'fredericksburg', 'free', 'freely', 'freeport', 'freeze', 'freiburg', 'frejus', 'fremantle', 'french', 'frequent', 'frequently', 'fresh', 'freshen', 'fresno', 'friday', 'fridge', 'friend', 'friendly', 'frith', 'from', 'front', 'frozen', 'fruit', 'fry', 'ft', 'fuel', 'fuengirola', 'fuerte', 'fuji', 'fukuoka', 'full', 'fullerton', 'fumba', 'fun', 'function', 'fund', 'fungus', 'funivia', 'furnished', 'furniture', 'fushi', 'fuste', 'fuster', 'gadget', 'gaios', 'gala', 'galeao', 'galilee', 'galleria', 'gallery', 'gallup', 'galveston', 'game', 'gamewatcher', 'gandhi', 'gangnam', 'gapyeong', 'garda', 'garden', 'gare', 'garibaldi', 'gas', 'gastro', 'gate', 'gateway', 'gatwick', 'gaube', 'gautrain', 'gay', 'gbp', 'gc', 'gdansk', 'gear', 'geared', 'gelina', 'gemelos', 'gendarmenmarkt', 'general', 'generally', 'geneva', 'genoa', 'genuine', 'geology', 'george', 'german', 'germany', 'get', 'getting', 'gexa', 'ghent', 'ghuwaifat', 'giang', 'giant', 'giardini', 'gibraltar', 'giethoorn', 'gift', 'gig', 'gilbert', 'gilboa', 'gilgit', 'gimhae', 'gimpo', 'gin', 'ginger', 'ginkgo', 'girl', 'girlfriend', 'girona', 'girons', 'give', 'given', 'giving', 'giza', 'glacier', 'glasgow', 'glass', 'globe', 'gloucester', 'glove', 'glowworm', 'gluten', 'glutton', 'go', 'goa', 'goal', 'goat', 'god', 'goeco', 'goer', 'going', 'gokarna', 'gokyo', 'gold', 'golden', 'golf', 'gomera', 'gona', 'gondola', 'gonzalo', 'good', 'google', 'goose', 'gopro', 'gopros', 'gordios', 'gordon', 'gore', 'gorge', 'goroka', 'got', 'gotobus', 'gouves', 'government', 'gps', 'grade', 'gradual', 'graham', 'gran', 'granada', 'grand', 'grande', 'grandmother', 'grant', 'granted', 'grasmere', 'grass', 'grassmarket', 'graze', 'great', 'greece', 'greek', 'green', 'greenery', 'greenwich', 'greyhound', 'grill', 'grimaud', 'groat', 'grocery', 'groove', 'ground', 'group', 'grove', 'grub', 'guanacaste', 'guangzhou', 'guard', 'guardalavaca', 'guarulhos', 'guest', 'guesthouse', 'guidance', 'guide', 'guidebook', 'guided', 'guideline', 'guilin', 'guillermo', 'guinea', 'guinness', 'guiones', 'guitar', 'gulangyu', 'gulf', 'gulhi', 'gurion', 'guy', 'gwangjang', 'gyeongbokgung', 'gym', 'h10', 'ha', 'haanaya', 'haarlem', 'haast', 'haatzmaut', 'hacienda', 'hackescher', 'haeinsa', 'hagen', 'hague', 'haifa', 'hair', 'hairdresser', 'halal', 'half', 'halifax', 'hall', 'halle', 'halloween', 'hallstatt', 'halong', 'halt', 'ham', 'hamburg', 'hamilton', 'hammamet', 'hammock', 'hampton', 'hamra', 'hand', 'handcraft', 'handicraft', 'handle', 'handling', 'hang', 'hangzhou', 'hanikra', 'hanmer', 'hanoi', 'happen', 'happening', 'happens', 'happy', 'harare', 'harbin', 'harbor', 'harbour', 'hard', 'harris', 'harry', 'hasik', 'hasselt', 'hassle', 'hat', 'hatta', 'hauling', 'havana', 'havasu', 'hawea', 'haymarket', 'hazardous', 'hbf', 'heading', 'healing', 'health', 'healthy', 'hear', 'heat', 'heated', 'heathmount', 'heathrow', 'heavily', 'heavy', 'heidelberg', 'heineken', 'heirloom', 'hel', 'held', 'helding', 'helena', 'heli', 'helicopter', 'helix', 'hell', 'hellenic', 'help', 'helpful', 'helping', 'helsinki', 'hendaye', 'henderson', 'heraklion', 'herbal', 'herbivorous', 'hercules', 'heritage', 'hermanus', 'hermitage', 'herodion', 'hertz', 'hervey', 'hickory', 'high', 'higher', 'highest', 'highland', 'highlight', 'highway', 'hike', 'hiking', 'hill', 'hilly', 'hilton', 'hinna', 'hint', 'hip', 'hippo', 'hire', 'hiring', 'historic', 'historical', 'history', 'hit', 'hk', 'ho', 'hoan', 'hobart', 'hobbiton', 'hobby', 'hogsback', 'hohenwerfen', 'hoi', 'hold', 'holder', 'holguin', 'holiday', 'hollywood', 'holy', 'hom', 'home', 'homegoods', 'homeland', 'homeowner', 'honey', 'honeymoon', 'honfleur', 'hong', 'hongkong', 'hop', 'hope', 'horizon', 'horse', 'horseback', 'hospital', 'hostel', 'hosteria', 'hot', 'hotel', 'hotline', 'hou', 'hour', 'house', 'houseboat', 'household', 'houston', 'how', 'hr', 'hualien', 'hubby', 'hudson', 'hue', 'huechahue', 'hulhumale', 'humahuaca', 'humble', 'hummingbird', 'hunza', 'hurghada', 'hurling', 'husband', 'hustle', 'huston', 'hwange', 'hyatt', 'hydrofoil', 'hyperdome', 'i40', 'iah', 'iberostar', 'ibis', 'ibiza', 'ibn', 'ice', 'id', 'idd', 'idea', 'ideal', 'ideally', 'ie', 'if', 'ifaty', 'ignite', 'igoumenitsa', 'iguacu', 'iguana', 'iguazu', 'iguaçu', 'ii', 'ill', 'illuminated', 'im', 'immigration', 'important', 'in', 'in7', 'incheon', 'incidental', 'include', 'included', 'including', 'inclusive', 'increase', 'independently', 'india', 'indian', 'indicated', 'indication', 'indira', 'indonesia', 'indonesian', 'indoor', 'industry', 'inexpensive', 'infant', 'infinity', 'info', 'infoline', 'information', 'ing', 'ingles', 'inis', 'injury', 'inland', 'inn', 'input', 'insect', 'inside', 'insider', 'instant', 'instead', 'institute', 'insurance', 'intend', 'interaction', 'interconnecting', 'intercontinental', 'interest', 'interesting', 'intermediate', 'international', 'internet', 'interstate', 'intl', 'invercargill', 'invitation', 'involve', 'involves', 'io', 'ioannis', 'ipad', 'ipanema', 'iphone', 'iran', 'irctc', 'ireland', 'iris', 'irish', 'iron', 'is', 'isaac', 'ischia', 'isla', 'islamabad', 'island', 'isle', 'isolated', 'israel', 'issue', 'issued', 'italian', 'italy', 'item', 'iternary', 'ithaca', 'itinerary', 'ixtapa', 'jabal', 'jabiru', 'jack', 'jacket', 'jacuzzi', 'jaipur', 'jaisalmer', 'jakarta', 'jamaica', 'jamaican', 'james', 'jan', 'janeiro', 'january', 'japan', 'japanese', 'jaxson', 'jayapura', 'jaz', 'jazz', 'jb', 'jbr', 'jbv', 'jean', 'jebel', 'jedi', 'jeep', 'jeju', 'jelly', 'jellyfish', 'jeonju', 'jerusalem', 'jesmond', 'jesolo', 'jet', 'jewel', 'jeweler', 'jewellery', 'jewelry', 'jewish', 'jfk', 'jhb', 'jiuzhaigou', 'jodhpur', 'johannesburg', 'join', 'joint', 'jomo', 'joondalup', 'jordaan', 'jordan', 'jose', 'josef', 'journey', 'jr', 'jsl', 'juan', 'jucy', 'juice', 'jujuy', 'july', 'jumeirah', 'jumping', 'june', 'jungle', 'jungmun', 'junk', 'jurong', 'just', 'k2', 'kaas', 'kabukicho', 'kachori', 'kaeng', 'kaibab', 'kaikoura', 'kaiteriteri', 'kakarbhitta', 'kakarvitta', 'kalam', 'kalamaki', 'kalami', 'kamaladi', 'kamalpokhari', 'kandooma', 'kandy', 'kangaroo', 'kangra', 'kanifushi', 'kansa', 'karachchi', 'karachi', 'karama', 'karaoke', 'kardamena', 'karon', 'kassiopi', 'kat', 'kata', 'kathmandu', 'kav', 'kayak', 'kayaking', 'kazan', 'kbal', 'kedarnath', 'keema', 'keep', 'keeping', 'kefalos', 'keisei', 'kell', 'kempinski', 'kempsford', 'kendwa', 'kenmore', 'kennedy', 'kensington', 'kenya', 'kenyan', 'kenyatta', 'keppel', 'kept', 'kerala', 'key', 'keynes', 'kg', 'khaimah', 'khalifa', 'khaluf', 'khan', 'khanjars', 'khao', 'khasan', 'kibbutz', 'kid', 'kiem', 'kilimanjaro', 'kilkenny', 'kill', 'kilmarnock', 'kimbe', 'kina', 'kind', 'kinderdijk', 'kindles', 'king', 'kinneret', 'kintsugi', 'kip', 'kippur', 'kissimmee', 'kit', 'kitchen', 'kitesurfing', 'kl', 'knitting', 'know', 'knowledgeable', 'known', 'knysna', 'ko', 'koala', 'koblenz', 'kochi', 'koh', 'kokoda', 'kokomo', 'kokopo', 'koln', 'komandoo', 'komodo', 'kong', 'konus', 'korcula', 'korea', 'korean', 'koreana', 'korsakov', 'kotor', 'kpop', 'krabi', 'krachan', 'krakow', 'kremlin', 'kruger', 'kukur', 'kulans', 'kuramathi', 'kuredu', 'kurumba', 'kwandwe', 'kwazulu', 'kyoto', 'la', 'labor', 'labour', 'ladera', 'lady', 'lae', 'lago', 'lagoa', 'lagon', 'lagoon', 'laguardia', 'lahore', 'lai', 'lak', 'lake', 'lakeview', 'lampedusa', 'lamu', 'lancaster', 'landing', 'landsborough', 'landscape', 'laneways', 'language', 'lanka', 'lanta', 'lantern', 'lanzarote', 'lapa', 'lara', 'large', 'largest', 'last', 'lasting', 'late', 'later', 'latina', 'lauca', 'lauderdale', 'launceston', 'laundromat', 'laundry', 'law', 'lawhill', 'lax', 'lay', 'layout', 'layover', 'lazio', 'lazy', 'le', 'leaf', 'league', 'leaning', 'learn', 'least', 'leather', 'leave', 'leaving', 'lebara', 'leblon', 'ledbury', 'lederhosen', 'leeds', 'lefkada', 'lefkas', 'lefrenchmobile', 'left', 'leg', 'legal', 'legally', 'legian', 'legit', 'legoland', 'leidseplein', 'leisure', 'leisurely', 'lejre', 'lembongan', 'length', 'lens', 'leod', 'lesson', 'let', 'letter', 'letting', 'level', 'lexington', 'lhasa', 'liability', 'liberia', 'library', 'libyan', 'license', 'lick', 'lido', 'liege', 'life', 'lifeguard', 'lifestyle', 'lift', 'light', 'lightweight', 'like', 'likelihood', 'likely', 'lily', 'lilybank', 'lima', 'limit', 'limitada', 'limited', 'limo', 'limousine', 'lincoln', 'linderhof', 'lindos', 'line', 'linen', 'liner', 'lingas', 'lining', 'link', 'linking', 'lion', 'liqour', 'liquid', 'liquor', 'lira', 'lisbon', 'list', 'listen', 'listing', 'liter', 'litre', 'little', 'live', 'liverpool', 'livingstone', 'llangoed', 'llano', 'lobster', 'local', 'located', 'location', 'locker', 'lodge', 'lodging', 'loews', 'lofoten', 'london', 'long', 'longest', 'longtail', 'lonquimay', 'look', 'looking', 'lookout', 'loop', 'los', 'loss', 'lost', 'lot', 'louis', 'lounge', 'lounger', 'louvre', 'love', 'lovely', 'lover', 'low', 'lower', 'lowest', 'lucerne', 'lucia', 'luck', 'lug', 'lugano', 'luggage', 'luna', 'lunch', 'luton', 'lux', 'luxembourg', 'luxor', 'luxury', 'lv', 'lycamobile', 'lynott', 'lyon', 'm40', 'maafushi', 'maasai', 'maastricht', 'macau', 'machine', 'madagascar', 'made', 'madeira', 'madeleine', 'madina', 'madrid', 'magdala', 'magdalena', 'maggiore', 'magic', 'magical', 'maha', 'mahabaleshwar', 'mahahual', 'mahal', 'mahanakhon', 'mahe', 'mahon', 'mail', 'main', 'mainland', 'major', 'majorca', 'makadi', 'makalolo', 'make', 'makeup', 'making', 'malaga', 'malaria', 'malarone', 'malaysia', 'malcesine', 'maldives', 'maldivian', 'male', 'malelane', 'maleme', 'malindi', 'mall', 'malmo', 'malolo', 'malpensa', 'malta', 'mamlouk', 'mana', 'management', 'manaus', 'manchester', 'mando', 'mandopop', 'manila', 'mansfield', 'mansion', 'manta', 'manual', 'manuel', 'manuka', 'many', 'manyara', 'map', 'mar', 'mara', 'marahau', 'marais', 'marathon', 'marazion', 'march', 'marco', 'mare', 'margao', 'margaret', 'maria', 'marina', 'marine', 'mariner', 'marino', 'marittima', 'mark', 'market', 'marksburg', 'markt', 'maroma', 'marrakech', 'marriage', 'married', 'marriott', 'marsa', 'marsala', 'martin', 'masai', 'masirah', 'mask', 'maspalomas', 'mass', 'massage', 'mastercard', 'matamata', 'match', 'matemwe', 'mather', 'matterhorn', 'matured', 'maui', 'mauritius', 'max', 'maxi', 'maximize', 'maximum', 'may', 'maya', 'mayai', 'mayan', 'maybe', 'mayoral', 'mazarron', 'mazatlan', 'mazda', 'mc', 'mccloud', 'mcleodganj', 'mcneill', 'mct', 'meadow', 'meal', 'mean', 'meant', 'measurement', 'meat', 'meatshop', 'mechanic', 'med', 'medical', 'medication', 'medicine', 'medieval', 'medina', 'meditation', 'mediterranean', 'meena', 'meet', 'meeting', 'megeve', 'megiddo', 'mekong', 'melbourn', 'melbourne', 'melia', 'member', 'memento', 'memorable', 'memorial', 'men', 'menai', 'mendoza', 'menstra', 'menu', 'merch', 'merchandise', 'mercure', 'meru', 'mesa', 'mesilla', 'meteora', 'method', 'methodist', 'metro', 'mexican', 'mexico', 'miami', 'michel', 'mickey', 'mid', 'middle', 'midnight', 'midtown', 'midweek', 'might', 'mikumi', 'milan', 'milano', 'mild', 'milder', 'mile', 'milford', 'military', 'milky', 'mill', 'millor', 'millosevich', 'milo', 'milton', 'min', 'mine', 'mineral', 'minh', 'mini', 'minibus', 'minimum', 'minivan', 'minorca', 'minsu', 'minute', 'miquel', 'miracoli', 'mishkaki', 'mislead', 'miss', 'missing', 'mistake', 'mittenwald', 'mjs', 'mkate', 'mlini', 'mobal', 'mobay', 'mobile', 'mobiling', 'mochis', 'mode', 'modena', 'moderate', 'moderately', 'mokomae', 'mom', 'mombasa', 'moment', 'monaco', 'monastery', 'monclear', 'monday', 'money', 'mongolian', 'monkey', 'monsoon', 'mont', 'montalcino', 'montauk', 'monte', 'montego', 'montepulciano', 'monterey', 'monterosso', 'monteverde', 'montevideo', 'month', 'montparnasse', 'montpellier', 'montreal', 'montreux', 'monts', 'montt', 'monument', 'moorea', 'mop90', 'moped', 'moquegua', 'mor', 'moreno', 'moresby', 'moreton', 'morne', 'morning', 'morocco', 'morzine', 'moscow', 'moskenesoya', 'mosque', 'mosquito', 'mostar', 'mostly', 'motel', 'motion', 'motogp', 'motor', 'motorbike', 'motorcycle', 'motorhomes', 'motukiekie', 'mouche', 'moulin', 'mount', 'mountain', 'mountaineer', 'movie', 'mr', 'mrt', 'mt', 'mte', 'mtns', 'mtw', 'muang', 'muay', 'much', 'mud', 'muff', 'mujeres', 'mule', 'multan', 'multi', 'multiple', 'mum', 'mumbai', 'munich', 'murano', 'murcia', 'muscat', 'musee', 'museum', 'musgrave', 'music', 'musician', 'muslim', 'must', 'mustafa', 'mutare', 'mutianyu', 'mvuli', 'mwr', 'myeongdong', 'myfe', 'mykonos', 'myra', 'na', 'nabq', 'nachi', 'nadi', 'naejangsan', 'nahariya', 'nail', 'nairobi', 'naisoso', 'nakhon', 'nakuru', 'nalcas', 'namdaemun', 'name', 'nami', 'namibia', 'nana', 'nang', 'nanny', 'naoussa', 'napier', 'naples', 'napoli', 'naran', 'narita', 'narvik', 'nasama', 'nassau', 'natal', 'natales', 'natalie', 'natco', 'nathu', 'national', 'native', 'natural', 'nature', 'nautical', 'navigate', 'navy', 'naxos', 'nazareth', 'ne', 'near', 'nearby', 'nearer', 'nearest', 'necessarily', 'necessary', 'necessitate', 'necessity', 'need', 'needed', 'needle', 'negative', 'negotiate', 'negra', 'negril', 'neighborhood', 'neighbourhood', 'nelson', 'nepal', 'nerja', 'nespresso', 'nessebar', 'nestor', 'net', 'netherlands', 'network', 'neuschwanstein', 'new', 'newbery', 'newbie', 'newcastle', 'newly', 'newquay', 'nex', 'next', 'nf', 'nfl', 'nha', 'niagara', 'niagra', 'nice', 'nicer', 'nicest', 'nicobar', 'nicolaas', 'nicotine', 'night', 'nightclub', 'nightlife', 'nighttime', 'nike', 'nile', 'nini', 'nishitetsu', 'niteroi', 'nizhny', 'nizwa', 'nm', 'noi', 'noise', 'noma', 'non', 'none', 'noodle', 'noon', 'noord', 'noosa', 'nord', 'norfolk', 'normal', 'normandy', 'norreport', 'north', 'northern', 'norway', 'norwegian', 'nosy', 'note', 'notl', 'noto', 'notting', 'nov', 'nova', 'novel', 'november', 'novgorod', 'novice', 'novo', 'nrt', 'nu', 'nude', 'nudity', 'nuevo', 'number', 'nungwi', 'nusa', 'nut', 'ny', 'nyc', 'nye', 'nyeri', 'nyhavn', 'nz', 'oak', 'oamaru', 'ob', 'obergesteln', 'oblu', 'observation', 'observatory', 'obtain', 'obtained', 'obtaining', 'occasion', 'occupied', 'ocean', 'oceana', 'oceanic', 'oceanside', 'ocho', 'oct', 'october', 'octopus', 'offer', 'offered', 'offering', 'office', 'official', 'offline', 'offsite', 'often', 'oglio', 'ohio', 'oia', 'oil', 'ok', 'okama', 'okay', 'oktoberfest', 'old', 'older', 'olive', 'olonzac', 'olympia', 'olympic', 'olympics', 'oman', 'one', 'online', 'opal', 'open', 'opened', 'opening', 'opera', 'operate', 'operates', 'operating', 'operational', 'operator', 'opinion', 'opportunity', 'optimal', 'option', 'oral', 'orbit', 'orchard', 'orcia', 'order', 'orford', 'organise', 'organize', 'organizer', 'orientales', 'original', 'originate', 'orkney', 'orlando', 'orleans', 'orly', 'ornament', 'ornen', 'orphanage', 'orsay', 'orthopaedics', 'ortigia', 'oryx', 'osanri', 'oslo', 'osterbro', 'ostia', 'ostrog', 'other', 'oudtshoorn', 'ouidane', 'ounce', 'outbound', 'outbreak', 'outdoor', 'outer', 'outlet', 'outside', 'ovda', 'overhead', 'overland', 'overnight', 'overpriced', 'overseas', 'overweight', 'owen', 'oxford', 'oxfordshire', 'oxygen', 'pacific', 'pack', 'package', 'packed', 'padang', 'paddington', 'padre', 'paella', 'paestum', 'page', 'pago', 'paignton', 'pailin', 'paine', 'painting', 'pair', 'paje', 'pakistan', 'pakistani', 'palace', 'palacio', 'paleochora', 'palermo', 'palestinian', 'palladium', 'palm', 'palma', 'palo', 'panadol', 'pancras', 'panda', 'panorama', 'panoramic', 'pant', 'pantanal', 'pantheon', 'papeete', 'paper', 'pappardelle', 'papua', 'parade', 'paradise', 'paragliding', 'parakito', 'parasailing', 'parasol', 'paraty', 'parc', 'parcel', 'parent', 'parga', 'paris', 'parisian', 'park', 'parked', 'parking', 'parma', 'parsifal', 'part', 'particular', 'particularly', 'partner', 'party', 'pas', 'pasmo', 'paso', 'pass', 'passenger', 'passport', 'past', 'pastry', 'patagonia', 'path', 'pathumwan', 'patisserie', 'patong', 'paulo', 'pavilion', 'pavlopetri', 'pax', 'pay', 'peaceful', 'peacock', 'peak', 'pearl', 'pearson', 'pebble', 'pedro', 'peeping', 'pefkos', 'pelicanos', 'pemba', 'pembury', 'penasco', 'penguin', 'penh', 'peninsula', 'penmon', 'penn', 'people', 'pepa', 'per', 'pera', 'perfect', 'performance', 'performed', 'period', 'perito', 'perm', 'permission', 'permit', 'permitted', 'person', 'personal', 'personalised', 'perth', 'peru', 'peschiera', 'pet', 'petanque', 'peter', 'petersberg', 'petersburg', 'petite', 'petra', 'petroglyph', 'petrol', 'peñasco', 'phalaborwa', 'phan', 'phantom', 'pharmacy', 'phi', 'philharmonic', 'philip', 'philippine', 'phillip', 'phnom', 'phoenician', 'phoenix', 'phone', 'photo', 'photograph', 'photographer', 'photographic', 'photography', 'photopass', 'photoshoot', 'phra', 'phu', 'phuket', 'piazza', 'picafort', 'pick', 'picking', 'pickpocket', 'pickup', 'picnic', 'picton', 'picture', 'pie', 'piece', 'pienza', 'piercings', 'pig', 'pilar', 'pill', 'pin', 'pink', 'pisa', 'pixie', 'pizza', 'place', 'plads', 'plan', 'plane', 'planet', 'planned', 'planner', 'planning', 'plant', 'plantation', 'plastic', 'plata', 'plateau', 'platform', 'play', 'playa', 'playing', 'plaza', 'pleasant', 'please', 'pleasure', 'plenty', 'plitvice', 'plug', 'plus', 'pm', 'point', 'poisoning', 'poisonous', 'poitou', 'poker', 'pokhara', 'poland', 'police', 'policy', 'polio', 'pollenca', 'pollensa', 'polluted', 'polo', 'polonnaruwa', 'polynesia', 'pompeii', 'ponce', 'pongwe', 'pontoon', 'pool', 'popular', 'porridge', 'port', 'portable', 'porter', 'portion', 'porto', 'portofino', 'portrush', 'portugal', 'posiible', 'positano', 'position', 'positive', 'possibility', 'possible', 'posted', 'poster', 'potential', 'potos', 'potter', 'pound', 'powder', 'powell', 'power', 'pozzallo', 'ppl', 'prado', 'prague', 'pram', 'praslin', 'prayer', 'pre', 'prebook', 'prebooked', 'precaution', 'precision', 'prediction', 'preferable', 'preferably', 'preference', 'preferred', 'pregnant', 'premier', 'premium', 'prenzlauer', 'preorder', 'prepaid', 'prepay', 'prepurchase', 'prescribed', 'prescription', 'presenting', 'prestatyn', 'pretoria', 'prettier', 'prettiest', 'pretty', 'prevalent', 'prevent', 'price', 'priced', 'pricey', 'pricing', 'primer', 'princess', 'principe', 'print', 'prior', 'priority', 'private', 'privilege', 'pro', 'problem', 'proceeding', 'product', 'professional', 'program', 'progreso', 'prohibited', 'promenade', 'promo', 'promontory', 'promotion', 'promotional', 'prone', 'proof', 'propeller', 'proper', 'properly', 'property', 'proposal', 'propose', 'protect', 'provence', 'provide', 'provided', 'provider', 'providing', 'province', 'proximity', 'psoriasis', 'pub', 'public', 'pueblo', 'puema', 'puerto', 'pukkelpop', 'pulau', 'pulpit', 'puma', 'pumba', 'pump', 'puna', 'punaauia', 'punakaiki', 'punta', 'punto', 'purchase', 'purchased', 'purchasing', 'purevpn', 'puri', 'purmamarca', 'purple', 'pushchair', 'pushkin', 'put', 'puyuhuapi', 'puzzle', 'pyramid', 'pyrenees', 'qatar', 'qatari', 'qls', 'quad', 'quaint', 'qualcomm', 'quality', 'quarter', 'quay', 'qudra', 'queen', 'queensland', 'queenstown', 'queesntown', 'queue', 'quick', 'quickest', 'quickly', 'quiet', 'quigleys', 'quiz', 'quoc', 'ra', 'rabaul', 'race', 'radio', 'radius', 'rafina', 'rafting', 'rah', 'raices', 'rail', 'railay', 'railor', 'railway', 'rain', 'rainbow', 'rainfall', 'rainforest', 'raining', 'rainy', 'rajasthan', 'rak', 'rambuteau', 'ramen', 'ramses', 'ramsey', 'ranch', 'rand', 'range', 'rapeseed', 'raphael', 'rastro', 'ratatouille', 'rate', 'rated', 'rather', 'rating', 'rav', 'ravello', 'rawalpindi', 'raxaul', 'ray', 'rd', 'reach', 'reached', 'readily', 'reading', 'real', 'realism', 'realistic', 'realistically', 'really', 'reap', 'reason', 'reasonable', 'reasonably', 'recent', 'recently', 'reception', 'reciprocity', 'recoleta', 'recommend', 'recommendation', 'recommended', 'recommendtions', 'recorded', 'recreational', 'red', 'redrox', 'redwood', 'reef', 'reethi', 'refill', 'refillable', 'refund', 'regarding', 'regency', 'reggae', 'region', 'regis', 'register', 'registration', 'regulation', 'reine', 'rejsekort', 'related', 'relating', 'relation', 'relatively', 'relax', 'relaxing', 'reliable', 'religious', 'rely', 'remain', 'remote', 'remove', 'remy', 'renaissance', 'renovation', 'rent', 'rentacarspain', 'rental', 'rented', 'renting', 'reopened', 'repair', 'repaire', 'repeat', 'repellant', 'repellent', 'repels', 'replica', 'republic', 'reputable', 'reputed', 'require', 'required', 'requirement', 'reservation', 'reserve', 'reserved', 'residence', 'residential', 'resort', 'resource', 'respect', 'respectful', 'response', 'rest', 'restaurant', 'restraint', 'restrict', 'restriction', 'restroom', 'retford', 'rethymno', 'retiring', 'retiro', 'retreat', 'return', 'returning', 'review', 'rhodes', 'rhombus', 'riad', 'ribera', 'rica', 'rican', 'ricans', 'riccione', 'rick', 'rico', 'ride', 'ridge', 'riding', 'riel', 'rifugio', 'right', 'rijksmuseum', 'rijsttafel', 'rim', 'rinca', 'rincon', 'ring', 'rinjani', 'rio', 'riomaggiore', 'rise', 'risotto', 'ristorante', 'riu', 'river', 'riverwalk', 'riviera', 'rmb', 'road', 'roading', 'roaming', 'roca', 'rock', 'rocky', 'roda', 'rodeo', 'roermond', 'rolling', 'romana', 'romantic', 'rome', 'romsdalseggen', 'rooftop', 'room', 'rooster', 'rosada', 'rosario', 'rose', 'rosh', 'rossini', 'rossore', 'rothenburg', 'rotorua', 'rotterdam', 'rottnest', 'rouge', 'rough', 'roughly', 'round', 'roundtrip', 'route', 'router', 'rowley', 'royal', 'ruakuri', 'rubber', 'rue', 'rufiyaa', 'rufolo', 'rugged', 'ruidoso', 'rule', 'rum', 'run', 'runaway', 'running', 'rupee', 'russia', 'russian', 'rutherglen', 'rv', 'sa', 'saana', 'sabana', 'sabi', 'sadah', 'safari', 'safe', 'safed', 'safely', 'safest', 'safety', 'saga', 'sagnlandet', 'sahara', 'sailing', 'saint', 'sainte', 'sak', 'salaam', 'salalah', 'salam', 'salamanca', 'salata', 'sale', 'salerno', 'salina', 'salon', 'salsa', 'salt', 'salta', 'salted', 'salzburg', 'sam', 'samana', 'samara', 'samburu', 'samoa', 'samoan', 'samoens', 'samseong', 'san', 'sanchome', 'sanctuary', 'sand', 'sandal', 'sandbank', 'sandton', 'sandy', 'sanford', 'sangster', 'sanibel', 'sant', 'santa', 'santander', 'santiago', 'santo', 'santorini', 'santrian', 'sanur', 'sao', 'saona', 'sap', 'sapa', 'sapienza', 'saquarema', 'sardine', 'sardinia', 'sarova', 'saturday', 'saturno', 'saudi', 'sausage', 'savai', 'savaii', 'savannah', 'save', 'savusavu', 'say', 'sayulita', 'scam', 'scandic', 'scandinavia', 'scandinavian', 'scary', 'scenaries', 'scenery', 'scenic', 'schedule', 'scheduled', 'schengen', 'schiphol', 'schmittenhohe', 'schoenfeld', 'school', 'science', 'scilly', 'scl', 'scooter', 'scotch', 'scotland', 'scottdale', 'scottish', 'scottsdale', 'scrapbooking', 'screen', 'scuba', 'sculpture', 'sea', 'seafood', 'seajet', 'seal', 'seaplane', 'searching', 'seashell', 'seaside', 'season', 'seat', 'seaway', 'seaweed', 'seaworld', 'seaworthy', 'secluded', 'second', 'secret', 'section', 'secure', 'secured', 'securely', 'security', 'seddon', 'sedona', 'see', 'seefeld', 'seeing', 'seek', 'seen', 'segesta', 'segovia', 'seine', 'select', 'self', 'selfcatering', 'sell', 'selling', 'selous', 'semi', 'seminyak', 'senator', 'senior', 'senja', 'sense', 'sensible', 'sentosa', 'sentral', 'seogwipo', 'seoraksan', 'seoul', 'separate', 'separately', 'sepik', 'sept', 'september', 'sequoia', 'serengeti', 'serf', 'serifos', 'serve', 'served', 'service', 'serving', 'session', 'set', 'seven', 'several', 'sevice', 'seville', 'sewing', 'seychelles', 'sez', 'sf', 'sha', 'shabbat', 'shabolovskaya', 'shakespeare', 'shamwari', 'shanghai', 'shangri', 'shannon', 'share', 'shared', 'sharia', 'sharing', 'sharjah', 'shark', 'sharm', 'shaybah', 'sheba', 'sheet', 'sheik', 'sheikh', 'shelling', 'shelter', 'sherborne', 'shifen', 'shinagawa', 'shinjuku', 'ship', 'shipping', 'shirt', 'shisha', 'shoal', 'shock', 'shoe', 'shop', 'shopping', 'shore', 'short', 'shorter', 'shot', 'should', 'show', 'shower', 'showing', 'shown', 'showroom', 'shui', 'shuttle', 'siam', 'sicily', 'sick', 'sickness', 'sidari', 'side', 'sidecar', 'siem', 'siena', 'siesta', 'sigatoka', 'sight', 'sightseeing', 'sign', 'signal', 'signature', 'sihanoukville', 'silencio', 'silk', 'sim', 'simbel', 'similar', 'simple', 'sims', 'sin', 'sinai', 'sinalei', 'singapore', 'singaporean', 'single', 'siracusa', 'sit', 'site', 'situated', 'situation', 'six', 'sixt', 'size', 'sized', 'ska', 'skala', 'skardu', 'skeleton', 'ski', 'skier', 'skiing', 'skip', 'skipper', 'skirt', 'skp', 'skt', 'skukuza', 'skutvik', 'sky', 'skybus', 'skydiving', 'skye', 'skyline', 'skyliner', 'skyscraper', 'skywalk', 'sl', 'sleep', 'sleeper', 'sleepervans', 'sleeping', 'sleeveless', 'slide', 'slope', 'slot', 'sloterdijk', 'sm', 'small', 'smaller', 'smeralda', 'smoke', 'smokeless', 'smoking', 'smoothy', 'snack', 'snake', 'sneaker', 'snorkel', 'snorkeling', 'snorkelling', 'snow', 'snowdon', 'snowing', 'snus', 'soaking', 'soccer', 'sochi', 'social', 'socialist', 'soda', 'sofa', 'sofitel', 'soft', 'sohar', 'sokcho', 'sol', 'sold', 'soller', 'solmar', 'solo', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhere', 'son', 'song', 'songthaew', 'soon', 'sopot', 'sorak', 'soraya', 'sorrento', 'sort', 'sostanza', 'sosua', 'souk', 'soull', 'sound', 'souq', 'south', 'southend', 'southern', 'southport', 'southwest', 'souvenir', 'soviet', 'spa', 'space', 'spain', 'spanish', 'sparkling', 'spda', 'speakeasy', 'speaking', 'spean', 'special', 'specially', 'specialty', 'specific', 'specifically', 'spectacular', 'speed', 'spencer', 'spend', 'spending', 'sperlonga', 'spezia', 'spice', 'spider', 'spilled', 'splash', 'split', 'sponsored', 'sport', 'spot', 'spotter', 'spotting', 'spring', 'springhill', 'square', 'srei', 'sri', 'st', 'stadium', 'staff', 'stair', 'stamp', 'stanbic', 'stand', 'standard', 'star', 'stargazing', 'start', 'starting', 'state', 'station', 'stationery', 'status', 'stavropol', 'stay', 'stayed', 'staying', 'stazione', 'steak', 'steamed', 'stena', 'step', 'stephansplatz', 'sterile', 'sterling', 'stew', 'stick', 'still', 'stockholm', 'stocking', 'stockport', 'stone', 'stonefish', 'stonehenge', 'stop', 'stopover', 'stopping', 'storage', 'store', 'story', 'stove', 'straight', 'straw', 'street', 'stressful', 'stretch', 'strict', 'strip', 'stroke', 'stroller', 'student', 'studio', 'style', 'stylist', 'styrian', 'submarine', 'suburb', 'subway', 'success', 'suckling', 'sufficient', 'sugar', 'suggest', 'suggested', 'suggestion', 'sugo', 'suica', 'suit', 'suitable', 'suitcase', 'suite', 'suiting', 'sukhumvit', 'sum', 'summer', 'sumo', 'sun', 'sunbathing', 'sunburn', 'sunday', 'sunny', 'sunrise', 'sunset', 'super', 'supercar', 'superior', 'supermarket', 'supersonic', 'supplied', 'supply', 'support', 'suppose', 'supposed', 'sur', 'sure', 'surf', 'surfboard', 'surfer', 'surfing', 'surprise', 'surrounding', 'surroundings', 'sushi', 'suv', 'suvarnabhumi', 'suzhou', 'swan', 'swanage', 'swat', 'swedish', 'sweet', 'swim', 'swimming', 'switzerland', 'sydney', 'synagogue', 'syracuse', 'system', 'taba', 'tabacon', 'tabatinga', 'tabgha', 'table', 'tablet', 'tag', 'tahaa', 'tahiti', 'taichung', 'taipei', 'taisha', 'taita', 'taiwan', 'taj', 'take', 'taken', 'taking', 'tala', 'talacre', 'tallinn', 'tamarindo', 'tamaya', 'tambo', 'tampa', 'tampico', 'tan', 'tana', 'tanger', 'tangerine', 'tangier', 'tanjung', 'tank', 'tanna', 'tanzania', 'tao', 'taormina', 'taoyuan', 'tap', 'taqsa', 'target', 'taroko', 'tasman', 'tasmania', 'tasmanian', 'tasting', 'tattoo', 'tattooing', 'tauber', 'taupo', 'taverna', 'taveuni', 'tax', 'taxi', 'tbilisi', 'te', 'tea', 'team', 'teatro', 'teen', 'teenage', 'teenager', 'teguise', 'tei', 'tekapo', 'tel', 'telephoto', 'tell', 'telling', 'telmo', 'tempe', 'temperature', 'temple', 'temporary', 'temuco', 'tend', 'tenerife', 'tenerifesunvacations', 'tennis', 'tenorios', 'tent', 'tented', 'teresa', 'term', 'terminal', 'terminus', 'terrace', 'terrain', 'terre', 'terrenas', 'territory', 'tex', 'texas', 'tgunei', 'tgv', 'thai', 'thailand', 'thamel', 'thanksgiving', 'theater', 'theatre', 'theft', 'theme', 'theotokopoulou', 'theresa', 'thermal', 'thessaloniki', 'thiet', 'thing', 'think', 'tho', 'thon', 'thoresby', 'thought', 'thousand', 'threat', 'three', 'throughout', 'thru', 'thulusdhoo', 'thumrait', 'thunder', 'thunderstorm', 'tiare', 'tiberia', 'tiberias', 'tiberium', 'tibet', 'ticket', 'tide', 'tien', 'tiffany', 'tigaki', 'tiger', 'tihar', 'tiji', 'tikida', 'till', 'tim', 'timanfaya', 'timbavati', 'time', 'timer', 'timetable', 'timing', 'timor', 'tin', 'tip', 'tipping', 'tissue', 'titanic', 'tivoli', 'tlv', 'tobacco', 'tobermory', 'toboggan', 'today', 'toddler', 'todhills', 'together', 'toilet', 'tokyo', 'toll', 'tone', 'tongan', 'tonic', 'tonight', 'top', 'topcars', 'topless', 'toronto', 'torquay', 'torreblanca', 'torremolinos', 'torrent', 'torres', 'tortuguero', 'total', 'toulouse', 'tour', 'touring', 'tourism', 'tourist', 'touristy', 'tournefort', 'tovar', 'towards', 'towel', 'tower', 'town', 'township', 'towong', 'toy', 'toyota', 'track', 'tradition', 'traditional', 'trafalgar', 'traffic', 'trail', 'train', 'tram', 'trang', 'trans', 'transfer', 'transferring', 'transit', 'translation', 'translator', 'transport', 'transportation', 'transportes', 'trapani', 'trastevere', 'trattoria', 'travel', 'traveler', 'traveling', 'travelled', 'traveller', 'travelling', 'travelodge', 'treasure', 'treating', 'treatment', 'tree', 'trek', 'trekking', 'trelew', 'treman', 'trendy', 'trenitalia', 'treviso', 'trezza', 'tri', 'triangle', 'tribe', 'trick', 'trickeye', 'trinco', 'trincomalee', 'trinidad', 'triomphe', 'trip', 'tripitaka', 'tripod', 'troll', 'trolley', 'trolltunga', 'tromso', 'trondheim', 'tropical', 'trouble', 'troublesome', 'trout', 'truck', 'true', 'truly', 'trumbo', 'trust', 'trusted', 'trustworthy', 'trvel', 'try', 'trying', 'tsarabanjina', 'tsavo', 'tsilivi', 'tsim', 'tsui', 'tsukiji', 'tuan', 'tub', 'tucson', 'tuk', 'tulip', 'tullamarine', 'tulum', 'tuna', 'tunis', 'tunnel', 'turin', 'turn', 'turned', 'turtle', 'tusayan', 'tuscan', 'tuscany', 'tv', 'twiga', 'twisted', 'two', 'tyler', 'type', 'typical', 'typically', 'tzfat', 'uae', 'ubar', 'uber', 'ubud', 'udawalawe', 'udo', 'uk', 'ukulele', 'uluru', 'ulvik', 'umbria', 'unable', 'unbleached', 'uncomfortable', 'underwater', 'unheated', 'uni', 'union', 'unique', 'unit', 'united', 'universal', 'unlawful', 'unmarried', 'unpredictable', 'unwind', 'unwinding', 'upcoming', 'upgrade', 'upolu', 'upon', 'upper', 'uppuveli', 'upstairs', 'uptown', 'urojo', 'us', 'usa', 'usage', 'usd', 'use', 'used', 'user', 'ushuaia', 'using', 'usually', 'uyuni', 'v39a', 'vaadhoo', 'vacation', 'vaccination', 'vaccine', 'val', 'valais', 'valentine', 'valid', 'validate', 'validity', 'vallarta', 'valley', 'vallon', 'valparaiso', 'value', 'vamos', 'van', 'vancouver', 'vanderbilt', 'vanimo', 'vanrell', 'vanuatu', 'vape', 'vaporetto', 'vara', 'varadero', 'varanasi', 'various', 'varna', 'vat', 'vatican', 'vault', 'vega', 'vegan', 'vegetable', 'vegetarian', 'vegeterian', 'vehicle', 'vendor', 'venice', 'ventana', 'venue', 'verde', 'verdon', 'verizon', 'vermont', 'vernazza', 'verona', 'versa', 'versailles', 'version', 'versus', 'vessel', 'vezza', 'vh', 'via', 'viaduct', 'viagra', 'viazul', 'vibe', 'vice', 'vicinity', 'victoria', 'victory', 'video', 'videographer', 'videographers', 'videography', 'viejo', 'vienna', 'vieques', 'vietnam', 'view', 'viewing', 'vihar', 'viking', 'viktualienmarkt', 'vila', 'villa', 'village', 'villasimius', 'villefranche', 'vina', 'vineyard', 'vintage', 'vip', 'virginia', 'virus', 'visa', 'visalia', 'visible', 'visit', 'visitation', 'visited', 'visiting', 'visitor', 'visitorlando', 'vista', 'vladikavkaz', 'vodafone', 'vodka', 'volcano', 'volga', 'voltage', 'volume', 'volunteer', 'vorgartenstraße', 'voss', 'voucher', 'vredendal', 'vuitton', 'wa', 'wadi', 'waffle', 'waiheke', 'wait', 'waited', 'waiter', 'waiting', 'waitomo', 'wale', 'walindi', 'walk', 'walkable', 'walker', 'walking', 'wall', 'wan', 'wanaka', 'wananavu', 'wander', 'want', 'wanted', 'wanting', 'waqif', 'ward', 'warm', 'warmest', 'warsaw', 'warwick', 'wash', 'washed', 'washing', 'washington', 'waste', 'wat', 'watch', 'watching', 'water', 'waterfall', 'waterpark', 'waterparks', 'waterslides', 'watersports', 'waterworld', 'way', 'wb', 'wdw', 'we', 'wear', 'weather', 'web', 'website', 'wedding', 'wednesday', 'week', 'weekday', 'weekend', 'weekly', 'weh', 'weigh', 'welcoming', 'well', 'wellington', 'wengen', 'went', 'west', 'western', 'westhampton', 'westin', 'weston', 'wet', 'wewak', 'whale', 'wharf', 'what', 'whats', 'wheel', 'wheelchair', 'wheeler', 'when', 'where', 'whether', 'which', 'whilst', 'whis', 'whiskey', 'whistler', 'white', 'whitehaven', 'who', 'whole', 'wholesale', 'why', 'wi', 'wicklow', 'widely', 'width', 'wifi', 'wight', 'wild', 'wildlife', 'will', 'williams', 'willing', 'willis', 'wilson', 'wind', 'windhoek', 'windy', 'wine', 'winery', 'winston', 'winter', 'wiring', 'wise', 'withdraw', 'withdrawal', 'withdrawing', 'within', 'without', 'witness', 'wolfville', 'woman', 'wonder', 'wondered', 'wonderful', 'wondering', 'wonderland', 'wong', 'woodland', 'wool', 'woolworth', 'work', 'worker', 'working', 'world', 'worldwide', 'worth', 'worthwhile', 'worthy', 'would', 'wow', 'wrangler', 'wrestling', 'wristband', 'written', 'wusta', 'wv', 'ww', 'www', 'wyndham', 'xanterra', 'xiamen', 'ximending', 'xin', 'xincheng', 'xmas', 'xygia', 'y3', 'yacht', 'yala', 'yamanote', 'yangshuo', 'yant', 'yaramar', 'yardenit', 'yas', 'yasawa', 'yau', 'year', 'yellow', 'yellowknife', 'yellowstone', 'yeouido', 'yeovil', 'yet', 'yoga', 'yom', 'york', 'yorker', 'yorkie', 'yosemite', 'young', 'younger', 'yourowncuba', 'yr', 'ystad', 'yummy', 'yunque', 'yuzawa', 'zaandam', 'zahora', 'zakopane', 'zakynthos', 'zambia', 'zante', 'zanzibar', 'zealand', 'zell', 'zermatt', 'zero', 'zhangjiajie', 'zihua', 'zihuatanejo', 'zika', 'zilair', 'zimbabwe', 'zip', 'zone', 'zoo', 'zorbing', 'zurich', 'πετάνκ']\n",
            "(5000, 4947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZBPonz4Ad8y"
      },
      "source": [
        "# Create the Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8FnILd2AiCf",
        "outputId": "f8ff2903-d302-4b58-cab0-2572d4ba9a42"
      },
      "source": [
        "def createBigram(data):\n",
        "  #  data = nltk.word_tokenize(data)\n",
        "  #  listOfBigrams = []\n",
        "  #  new_words = \"\"\n",
        "  #  bigramCounts = {}\n",
        "  #  for i in range(len(data)-1):\n",
        "  #     if i < len(data) - 1 and data[i+1].islower():\n",
        "\n",
        "  #        listOfBigrams.append((data[i], data[i + 1]))\n",
        "\n",
        "  #        if (data[i], data[i+1]) in bigramCounts:\n",
        "  #           bigramCounts[(data[i], data[i + 1])] += 1\n",
        "  #        else:\n",
        "  #           bigramCounts[(data[i], data[i + 1])] = 1\n",
        "\n",
        "  #  return listOfBigrams\n",
        "  newWords = \"\"\n",
        "  previousWord = None\n",
        "  for word in data.strip().split(' '):\n",
        "      \n",
        "      if previousWord is not None:\n",
        "          newWords += \"{}{} \".format(previousWord, word)\n",
        "      previousWord = word\n",
        "  return newWords[:-1]\n",
        "\n",
        "\n",
        "dataset['questions_bigram'] = [createBigram(question) for question in dataset['stopwordsRemoved_questions']]\n",
        "print('Sample question before Bigram - ',dataset['questions'][1])\n",
        "print('Bigram - ',dataset['questions_bigram'][1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample question before Bigram -  What are the companies which organize shark feeding events for scuba divers?\n",
            "Bigram -  whatcompanies companiesorganize organizeshark sharkfeeding feedingevents eventsscuba scubadivers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOIyr4pgoRSx"
      },
      "source": [
        "**Select Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4PzZttmoQ95"
      },
      "source": [
        "x_named_entities = dataset['named_entity_questions']\n",
        "\n",
        "x_pos_tags = dataset['posTagged_questions']\n",
        "\n",
        "x_bigram = dataset['questions_bigram']\n",
        "\n",
        "x_pos_tags = dataset['posTagged_questions']\n",
        "\n",
        "x_lemma = dataset['lemmatized_questions']\n",
        "\n",
        "x_tfigf = tfidf\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-y6nfGhDLC5",
        "outputId": "3caf9afe-06a7-4e17-8050-9f7040ffca23"
      },
      "source": [
        "print(\"x_named_entities shape\",x_named_entities.shape)\n",
        "print(\"x_pos_tags shape\",x_pos_tags.shape)\n",
        "print(\"x_bigram shape\",x_bigram.shape)\n",
        "print(\"x_pos_tags shape\",x_pos_tags.shape)\n",
        "print(\"x_lemma shape \",x_lemma.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_named_entities shape (5000,)\n",
            "x_pos_tags shape (5000,)\n",
            "x_bigram shape (5000,)\n",
            "x_pos_tags shape (5000,)\n",
            "x_lemma shape  (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw-82Mo8gGNK"
      },
      "source": [
        "# Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhp5WfwSeu7T"
      },
      "source": [
        "# Convert a collection of text documents to a matrix of token counts\n",
        "def getCountVector(document):\n",
        "\n",
        "  # Get top 1500 max_features ordered by term frequency across the corpus.\n",
        "  # Ignore terms that have a document frequency strictly higher than 0.7\n",
        "  # Ignore terms that have a document frequency strictly lower than 0.5\n",
        "  vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7) \n",
        "  vector = vectorizer.fit_transform(document).toarray()\n",
        "  return vector\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBQxhnDxLwmL"
      },
      "source": [
        "# Prepare final features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfVSdZrkLxSw",
        "outputId": "44f51c4f-0fc1-43c5-b39f-a8d4f1d999c5"
      },
      "source": [
        "# Stack sparse matrices horizontally\n",
        "allFeatures = csr_matrix(hstack([\n",
        "                        x_tfigf,\n",
        "                        head_words_vector,\n",
        "                        getCountVector(x_bigram),\n",
        "                        getCountVector(x_pos_tags), \n",
        "                        getCountVector(x_named_entities)]))\n",
        "\n",
        "\n",
        "\n",
        "#Encode given classes with value between 0 and (num of classes - 1)\n",
        "def getEncodedClasses(className='coarse_classes'):\n",
        "    encoder = LabelEncoder()\n",
        "    encodedClasses = encoder.fit_transform(dataset[className])\n",
        "    return encodedClasses\n",
        "\n",
        "\n",
        "#Select features according to the k highest scores.\n",
        "finalFeatures = SelectKBest(chi2, k=3000).fit_transform(allFeatures,getEncodedClasses('coarse_classes'))\n",
        "\n",
        "print(\"Shape of the all features\", allFeatures.shape)\n",
        "\n",
        "print(\"Shape of the final features\",finalFeatures.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the all features (5000, 5905)\n",
            "Shape of the final features (5000, 3000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52nS1SHvXCti"
      },
      "source": [
        "# **Part 1** -  Create SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZYwMnFwXBiA"
      },
      "source": [
        "def trainSVMModel(features, classes):\n",
        "    best_prediction = None\n",
        "    best_test = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # Split dataset into 10 consecutive folds by controlling the randomness of each fold with shuffling\n",
        "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "    fold = 0\n",
        "    accuracies = []\n",
        "    for train_index, test_index in cv.split(features):\n",
        "      fold += 1\n",
        "      X_train, X_test = features[train_index], features[test_index]\n",
        "      y_train, y_test = classes[train_index], classes[test_index]\n",
        "\n",
        "      #Support Vector Classification \n",
        "      SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "\n",
        "      #Fit the SVM model according to the given training data\n",
        "      SVM.fit(X_train,y_train)\n",
        "\n",
        "      #Perform classification on X_test\n",
        "      predictions_SVM1 = SVM.predict(X_test)\n",
        "\n",
        "      #computes subset accuracy\n",
        "      current_accuracy = accuracy_score(predictions_SVM1, y_test)*100\n",
        "      if best_accuracy < current_accuracy:\n",
        "          best_accuracy = current_accuracy\n",
        "          best_prediction = predictions_SVM1\n",
        "          best_test = y_test\n",
        "          best_model = SVM\n",
        "      accuracies.append(current_accuracy)\n",
        "      print(\"Fold {}  {}  {:.2f}\".format(fold, \"SVM Accuracy -> \",current_accuracy))\n",
        "    \n",
        "    print(\"Best SVM accuracy : {}\".format(best_accuracy))\n",
        "\n",
        "    #evalaute the results\n",
        "    evaluateAccuracy(best_test, best_prediction)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhTlRdSecYVB"
      },
      "source": [
        "# Evaluate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Z5GoHWNrZQ"
      },
      "source": [
        "\n",
        "def evaluateAccuracy(y_test, y_pred):\n",
        "  #Calculate metrics globally by counting the total true positives, false negatives and false positives by setting average as'micro'.\n",
        "\n",
        "  #Calculate the ratio tp / (tp + fp)\n",
        "  precision = precision_score(y_test, y_pred,labels=np.unique(y_pred), average='micro') \n",
        "  print('Precision: %.2f' % (precision*100))\n",
        "\n",
        "  #Calculate the ratio tp / (tp + fn)\n",
        "  recall = recall_score(y_test, y_pred, labels=np.unique(y_pred), average='micro') \n",
        "  print('Recall: %.2f' % (recall*100))\n",
        "\n",
        "  #Calculate the ratio 2 * (precision * recall) / (precision + recall)\n",
        "  f1Score = f1_score(y_test, y_pred,labels=np.unique(y_pred), average='micro')\n",
        "  print('F1 Score: %.2f' % (f1Score*100))\n",
        "\n",
        "  #Get the confusion matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(\"\\nConfustion matrix: \\n{}\".format(cm))\n",
        "\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuoUdEqAccpB"
      },
      "source": [
        "# Train the SVM Model and get accuracy for Coarse Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvIQ2-TGbZm9",
        "outputId": "fff9de73-3bd7-48f3-a047-c9218bc4a8b3"
      },
      "source": [
        "trainSVMModel(finalFeatures,getEncodedClasses('coarse_classes'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1  SVM Accuracy ->   78.40\n",
            "Fold 2  SVM Accuracy ->   79.00\n",
            "Fold 3  SVM Accuracy ->   79.00\n",
            "Fold 4  SVM Accuracy ->   78.60\n",
            "Fold 5  SVM Accuracy ->   81.60\n",
            "Fold 6  SVM Accuracy ->   82.00\n",
            "Fold 7  SVM Accuracy ->   81.80\n",
            "Fold 8  SVM Accuracy ->   78.60\n",
            "Fold 9  SVM Accuracy ->   79.80\n",
            "Fold 10  SVM Accuracy ->   77.20\n",
            "Best SVM accuracy : 82.0\n",
            "Precision: 82.00\n",
            "Recall: 82.00\n",
            "F1 Score: 82.00\n",
            "\n",
            "Confustion matrix: \n",
            "[[ 50   0   1   3   1   4   0]\n",
            " [  1   8   0   0   0   5   0]\n",
            " [  3   0  42   1   1   3   0]\n",
            " [  4   0   3 110   9  22   0]\n",
            " [  2   1   2   3  98   4   0]\n",
            " [  3   2   3   7   1  85   0]\n",
            " [  0   0   0   1   0   0  17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMvexVFw8kae"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRuJNCAY8k06"
      },
      "source": [
        "# Train the SVM Model and get accuracy for Fine Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbYCzzie8p9O",
        "outputId": "afe1dce5-e049-4dc7-80ec-d22ccb55224d"
      },
      "source": [
        "trainSVMModel(finalFeatures,getEncodedClasses('fine_classes'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1  SVM Accuracy ->   52.20\n",
            "Fold 2  SVM Accuracy ->   53.20\n",
            "Fold 3  SVM Accuracy ->   50.40\n",
            "Fold 4  SVM Accuracy ->   50.60\n",
            "Fold 5  SVM Accuracy ->   52.40\n",
            "Fold 6  SVM Accuracy ->   53.60\n",
            "Fold 7  SVM Accuracy ->   48.80\n",
            "Fold 8  SVM Accuracy ->   50.40\n",
            "Fold 9  SVM Accuracy ->   54.20\n",
            "Fold 10  SVM Accuracy ->   48.60\n",
            "Best SVM accuracy : 54.2\n",
            "Precision: 54.20\n",
            "Recall: 56.93\n",
            "F1 Score: 55.53\n",
            "\n",
            "Confustion matrix: \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 4 0 1]\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 2 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXq6lqmVCA4h"
      },
      "source": [
        "# **Part 2** - Word embeddings using fastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5EMPySmDfbi"
      },
      "source": [
        "**Word Embedding for Course classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQzXqpi7OaY"
      },
      "source": [
        "def get_embedding_sentence(sentence):\n",
        "    \n",
        "    embedding=ft.get_sentence_vector(sentence)\n",
        "    return embedding"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgpYjGYzxiKb",
        "outputId": "3f871553-cf88-4891-91d3-cc6c5c1acd12"
      },
      "source": [
        "X_doc2fast = np.array([get_embedding_sentence(question) for question in dataset['preprocessed_questions'].values])\n",
        "le = LabelEncoder()\n",
        "y_doc2fast = le.fit_transform(dataset['coarse_classes'])\n",
        "\n",
        "trainSVMModel(X_doc2fast, getEncodedClasses('coarse_classes'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1  SVM Accuracy ->   72.80\n",
            "Fold 2  SVM Accuracy ->   75.60\n",
            "Fold 3  SVM Accuracy ->   73.00\n",
            "Fold 4  SVM Accuracy ->   72.40\n",
            "Fold 5  SVM Accuracy ->   74.60\n",
            "Fold 6  SVM Accuracy ->   75.20\n",
            "Fold 7  SVM Accuracy ->   71.40\n",
            "Fold 8  SVM Accuracy ->   71.60\n",
            "Fold 9  SVM Accuracy ->   76.20\n",
            "Fold 10  SVM Accuracy ->   75.20\n",
            "Best SVM accuracy : 76.2\n",
            "Precision: 76.20\n",
            "Recall: 76.51\n",
            "F1 Score: 76.35\n",
            "\n",
            "Confustion matrix: \n",
            "[[  0   0   0   0   0   0   0   1   0]\n",
            " [  0  49   0   0  12   0   1   8   1]\n",
            " [  0   1   3   2   1   0   1   9   0]\n",
            " [  0   2   1  42   4   0   0   5   0]\n",
            " [  0   3   0   1 100   0   3  19   0]\n",
            " [  0   0   0   0   1   0   0   0   0]\n",
            " [  0   1   0   0  11   0  77   6   0]\n",
            " [  0   2   0   1  14   0   5  99   0]\n",
            " [  0   0   0   0   1   0   1   1  11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7d5cP-DnHK"
      },
      "source": [
        "**Word Embedding for Fine classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE1TL7jGDqsZ",
        "outputId": "313f92cc-28bf-48ad-cde0-d483d9a8174c"
      },
      "source": [
        "trainSVMModel(X_doc2fast, getEncodedClasses('fine_classes'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1  SVM Accuracy ->   38.80\n",
            "Fold 2  SVM Accuracy ->   41.20\n",
            "Fold 3  SVM Accuracy ->   41.40\n",
            "Fold 4  SVM Accuracy ->   36.80\n",
            "Fold 5  SVM Accuracy ->   35.60\n",
            "Fold 6  SVM Accuracy ->   39.00\n",
            "Fold 7  SVM Accuracy ->   34.80\n",
            "Fold 8  SVM Accuracy ->   35.60\n",
            "Fold 9  SVM Accuracy ->   37.60\n",
            "Fold 10  SVM Accuracy ->   38.20\n",
            "Best SVM accuracy : 41.4\n",
            "Precision: 41.40\n",
            "Recall: 58.31\n",
            "F1 Score: 48.42\n",
            "\n",
            "Confustion matrix: \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 8 0]\n",
            " [0 0 0 ... 0 5 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjlI_RkwFepR"
      },
      "source": [
        "# **Part 3** - A NN classifier s.a. an LSTM for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIS3-sgFdA6",
        "outputId": "c27c5042-fe9d-40d1-b129-7c3f2f2e4242"
      },
      "source": [
        "\n",
        "# Text tokenization\n",
        "tokenizer = Tokenizer(num_words=5000, split=' ')\n",
        "\n",
        "#Updates internal vocabulary based on a list of texts.\n",
        "tokenizer.fit_on_texts(dataset['questions'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Number of unique tokens - %s ' % len(word_index))\n",
        "\n",
        "\n",
        "# Transforms each text in questions to a sequence of integers.\n",
        "initial_x = tokenizer.texts_to_sequences(dataset['questions'].values)\n",
        "print('Sample equence after padding ->',initial_x[0])\n",
        "\n",
        "# Pads sequences to the same length which equals to 25.\n",
        "final_x_lstm = pad_sequences(initial_x, maxlen=25)\n",
        "\n",
        "print('Sample sequence before padding ->',final_x_lstm[0])\n",
        "print('Shape of LSTM input data tensor:', final_x_lstm.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens - 5582 \n",
            "Sample equence after padding -> [4, 7, 2, 321, 105, 31, 1837, 17, 68, 9, 20, 71, 6, 194, 48, 32, 22, 376, 111]\n",
            "Sample sequence before padding -> [   0    0    0    0    0    0    4    7    2  321  105   31 1837   17\n",
            "   68    9   20   71    6  194   48   32   22  376  111]\n",
            "Shape of LSTM input data tensor: (5000, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwCviVMgy4UM"
      },
      "source": [
        "#Convert categorical variable into dummy/indicator variables.\n",
        "\n",
        "def convertVariables(className='coarse_classes'):\n",
        "    variables = pd.get_dummies(dataset[className]).values\n",
        "    return variables"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z7gb4OZ0Lgb"
      },
      "source": [
        "**Create LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jF8obI8z-ev"
      },
      "source": [
        "def createLSTMModel(features, classes, verbose=0):\n",
        "\n",
        "  # Create a Sequential model\n",
        "  model = Sequential()\n",
        "\n",
        "  # Turns positive integers (indexes) into dense vectors of fixed size\n",
        "  model.add(Embedding(5000, 160, input_length=features.shape[1]))\n",
        "\n",
        "  # drops entire 1D feature maps with 0.2 rate\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "  #create LSTM layer\n",
        "  # dimensionality of the output space is 196\n",
        "  # dropout: raction of the units to drop for the linear transformation of the inputs\n",
        "  # recurrent_dropout:  Fraction of the units to drop for the linear transformation of the recurrent state.\n",
        "  model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(classes.shape[1], activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  if verbose == 1:\n",
        "      print(model.summary())\n",
        "  return model\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miv_ygxx88x1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeViDgfh89QE",
        "outputId": "05172c50-d11b-4dbc-fd08-3d9dc186527c"
      },
      "source": [
        "createLSTMModel(final_x_lstm, convertVariables('fine_classes'), 1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 25, 160)           800000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 25, 160)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 196)               279888    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 79)                15563     \n",
            "=================================================================\n",
            "Total params: 1,095,451\n",
            "Trainable params: 1,095,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7fc84eb4b410>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}